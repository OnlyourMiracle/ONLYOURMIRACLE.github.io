{"pages":[{"title":"about","text":"Hello, World​ Welcome to my blog. ​ I’m ONLYOURMIRACLE, a college student. ​ Character: “Advance can attack, retreat can defend.” ​ Send email to me ​ Here are some of my profiles. My footprint 浙江 北京 山东 上海 绍兴 三里屯 青岛 外滩 鲁迅故居 圆明园· 八大关 豫园 五泄风景区 王府井 青岛市美术馆 杭州 北京大学· 信号山公园 西湖 清华大学· 金沙滩 西溪湿地 故宫· 崂山风景区（全线） 浙江大学 国家图书馆· 栈桥 阿里巴巴园区 奥林匹克塔 石老人海浴场 杭州乐园 颐和园 青岛啤酒城 义乌 天安门 情人坝 中山公园 金华 青岛大学 邢村 动物园（中山公园内） 鞋塘镇 青岛市图书馆 青岛天主教堂 德国总督宫邸 五四广场 奥帆中心 山东科技大学 青岛国际园艺博览园 青岛融创乐园 青岛大珠山景区 浮山 崂山书院 青岛森林野生动物园 青岛海底世界 青岛啤酒博物馆 Movies I watched(Cinema) 日期 电影名称 2022.06.19 《人生大事》 2022.06.18 《一周的朋友》 2022.06.11 《暗恋·橘生淮南》 2022.06.10 《侏罗纪世界3》 2022.05.28 《哆啦A梦：大雄的宇宙小战争》 2022.05.26 《神秘海域》 2022.05.19 《坏蛋联盟》 2022.02.11 《奇迹·笨小孩》 2021.12.31 《反贪风暴5》 2021.12.24 《穿越寒冬拥抱你》 2021.12.15 《误杀2》 2021.12.02 《古董局中局》 2021.11.21 《扬名立万》 2021.11.13 《梅艳芳》 2021.10.29 《007：无暇赴死》（IMAX） 2021.10.23 《沙丘》 2021.10.02 《长津湖》（IMAX） 2021.09.30 《我和我的父辈》 2021.08.29 《失控玩家》（IMAX） 2021.08.21 《怒火-重案》（IMAX） 2021.08.13 《盛夏未来》 2021.07.23 《白蛇2：青蛇劫起》 2021.07.11 《俑之城》 2021.07.10 《济公之降龙降世》 2021.07.10 《大学》 2021.07.09 《中国医生》 2021.06.20 《了不起的老爸》 2021.06.14 《超越》 2021.06.12 《你好世界》 2021.06.11 《比得兔2：逃跑计划》 2021.06.05 《人之怒》 2021.06.04 《我要我们在一起》 2021.05.29 《哆啦A梦：伴我同行2》 2021.05.22 《速度与激情9》 2021.05.15 《扫黑·决战》 2021.04.24 《指环王：双塔奇兵》 2021.04.18 《指环王：护戒使者》 2021.04.10 《哥斯拉大战金刚》 2021.03.14 《阿凡达》 2021.02.24 《新神榜：哪吒重生》 2021.02.23 《你好，李焕英》 2021.02.17 《唐人街探案3》 2021.01.09 《送你一朵小红花》 2021.01.03 《拆弹专家2》 2021.01.01 《心灵奇旅》 Books I’ve read Date Name 2022 《剑来》 2021 《我的治愈系游戏》（novel） 2021 《我在未来等你》 2021 《谁的青春不迷茫》 2021 《平凡的世界》（上中下） 2021 《刺杀骑士团长》（两部） 2021 《时间简史》 2020 《全球通史:从史前史到21世纪》 2020 《岁月静好 现世安稳》 2020 《人生的智慧：叔本华修心课》 2020 《滚雪球:巴菲特和他的财富人生》 2020 《个人形成论:我的心理治疗观》 - 《资治通鉴故事》 - 《瓦尔登湖》 - 《局外人》 - 《追风筝的人》 - 《摆渡人》（三部） - 《人间失格》 - 《老人与海》 - 《傅雷家书》 - 《昆虫记》 - 《论语》 - 《文化苦旅》 - 《局外人》 - 、、、、、、","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"ML Notes","text":"This article records my process of learning machine learning. 数据预处理标准化 Reason: 对于大多数数据挖掘算法来说，数据集的标准化是基本要求。这是因为，如果特征不服从或者近似服从标准正态分布（即，零均值、单位标准差的正态分布）的话，算法的表现会大打折扣。实际上，我们经常忽略数据的分布形状，而仅仅做零均值、单位标准差的处理。在一个机器学习算法的目标函数里的很多元素所有特征都近似零均值，方差具有相同的阶。如果某个特征的方差的数量级大于其它的特征，那么，这个特征可能在目标函数中占主导地位，这使得模型不能从其它特征有效地学习。 Z-score标准化这种方法基于原始数据的均值mean和标准差standard deviation进行数据的标准化。将特征A的原始值x使用z-score标准化到x’。z-score标准化方法适用于特征A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。将数据按其特征(按列进行)减去其均值，然后除以其方差。最后得到的结果是，对每个特征/每列来说所有数据都聚集在0附近，方差值为1。数学公式如下： 函数scale为数组形状的数据集的标准化提供了一个快捷实现： 1234567from sklearn import preprocessingimport numpy as npX_train = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]])X_train = preprocessing.scale(X_train) Min-Max 标准化Min-max标准化方法是对原始数据进行线性变换。设minA和maxA分别为特征A的最小值和最大值，将A的一个原始值x通过min-max标准化映射成在区间[0,1]中的值x'，其公式为： 可以使用MinMaxScaler实现，以下是一个将简单的数据矩阵缩放到[0, 1]的例子: 12345678from sklearn import preprocessing import numpy as np X_train = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]])min_max_scaler = preprocessing.MinMaxScaler()X_train_minmax = min_max_scaler.fit_transform(X_train) MaxAbs标准化MaxAbs的工作原理与Min-max非常相似，但是它只通过除以每个特征的最大值将训练数据特征缩放至 [-1, 1] 范围内，这就意味着，训练数据应该是已经零中心化或者是稀疏数据。公式如下： 可以使用MaxAbsScale实现，以下是使用上例中数据运用这个缩放器的例子: 12345678from sklearn import preprocessing import numpy as np X_train = np.array([[ 1., -1., 2.], [ 2., 0., 0.], [ 0., 1., -1.]])max_abs_scaler = preprocessing.MaxAbsScaler()X_train_maxabs = max_abs_scaler.fit_transform(X_train) 非线性转换 Reason: 对于大多数数据挖掘算法来说，如果特征不服从或者近似服从标准正态分布（即，零均值、单位标准差的正态分布）的话，算法的表现会大打折扣。非线性转换就是将我们的特征映射到均匀分布或者高斯分布(即正态分布)。 映射到均匀分布相比线性缩放，该方法不受异常值影响，它将数据映射到了零到一的均匀分布上，将最大的数映射为1，最小的数映射为0。其它的数按从小到大的顺序均匀分布在0到1之间，如有相同的数则取平均值，如数据为np.array([[1],[2],[3],[4],[5]])则经过转换为：np.array([[0],[0.25],[0.5],[0.75],[1]])，数据为np.array([[1],[2],[9],[10],[2]])则经过转换为：np.array([[0],[0.375],[0.75],[1.0],[0.375]])。第二个例子具体过程如下图： 在sklearn中使用QuantileTransformer方法实现，用法如下： 123456from sklearn.preprocessing import QuantileTransformerimport numpy as npdata = np.array([[1],[2],[3],[4],[5]])quantile_transformer = QuantileTransformer(random_state=666)data = quantile_transformer.fit_transform(data) 映射到高斯分布映射到高斯分布是为了稳定方差，并最小化偏差。在最新版sklearn 0.20.x中PowerTransformer现在有两种映射方法，Yeo-Johnson映射，公式如下： Box-Cox映射，公式如下： 在sklearn 0.20.x中使用PowerTransformer方法实现，用法如下： 123456from sklearn.preprocessing import PowerTransformerimport numpy as npdata = np.array([[1],[2],[3],[4],[5]]) pt = PowerTransformer(method='box-cox', standardize=False)data = pt.fit_transform(data) 归一化 Reason: 归一化是缩放单个样本以具有单位范数的过程。归一化实质是一种线性变换，线性变换有很多良好的性质，这些性质决定了对数据改变后不会造成“失效”，反而能提高数据的表现，这些性质是归一化的前提。归一化能够加快模型训练速度，统一特征量纲，避免数值太大。值得注意的是，归一化是对每一个样本做转换，所以是对数据的每一行进行变换。而之前我们讲过的方法是对数据的每一列做变换。 L1范式归一化L1范式定义如下： 表示向量x中每个元素的绝对值之和。L1范式归一化就是将样本中每个特征除以特征的L1范式。 在sklearn中使用normalize方法实现，用法如下： 1234567from sklearn.preprocessing import normalizedata = np.array([[-1,0,1], [1,0,1], [1,2,3]])data = normalize(data, 'l1') L2范式归一化L2范式定义如下： 表示向量元素的平方和再开平方根。L2范式归一化就是将样本中每个特征除以特征的L2范式。 在sklearn中使用normalize方法实现，用法如下： 1234567from sklearn.preprocessing import normalizedata = np.array([[-1,0,1], [1,0,1], [1,2,3]])data = normalize(data, 'l2') ​ 离散值编码LabelEncoder在数据挖掘中，特征经常不是数值型的而是分类型的。举个例子，一个人可能有[&quot;male&quot;, &quot;female&quot;]，[&quot;from Europe&quot;, &quot;from US&quot;, &quot;from Asia&quot;]，[&quot;uses Firefox&quot;, &quot;uses Chrome&quot;, &quot;uses Safari&quot;, &quot;uses Internet Explorer&quot;]等分类的特征。这些特征能够被有效地编码成整数，比如[&quot;male&quot;, &quot;from US&quot;, &quot;uses Internet Explorer&quot;]可以被表示为[0, 1, 3]，[&quot;female&quot;, &quot;from Asia&quot;, &quot;uses Chrome&quot;]表示为[1, 2, 1]。 在sklearn中，通过LabelEncoder来实现： 12345from sklearn.preprocessing import LabelEncoderlabel = label = ['male','female'] int_label = LabelEncoder()label = int_label.fit_transform(label) OneHotEncoder这种整数特征表示并不能在sklearn的估计器中直接使用，因为这样的连续输入，估计器会认为类别之间是有序的，但实际却是无序的。如将male,female，转换为1,0。1比0要大，机器就会把这个关系考虑进去，而male,female之间是没有这样的关系的。所以我们需要使用另外一种编码方式，OneHot编码。 在sklearn中通过OneHotEncoder来实现，使用方法如下： 12345678910import numpy as npfrom sklearn.preprocessing import OneHotEncoderfrom sklearn.preprocessing import LabelEncoderlabel = label = ['male','female'] int_label = LabelEncoder()label = int_label.fit_transform(label)label = np.array(label).reshape(len(label), 1)onehot_label = OneHotEncoder()label = onehot_label.fit_transform(label).toarray() 生成多项式特征 Reason: 在数据挖掘中，获取数据的代价经常是非常高昂的。所以有时就需要人为的制造一些特征，并且有的特征之间是有关联的。生成多项式特征可以轻松的为我们获取更多的数据，并获得特征的更高维度和互相间关系的项且引入了特征之间的非线性关系，可以有效的增加模型的复杂度。 PolynomialFeatures在sklearn中通过PolynomialFeatures方法来生成多项式特征，使用方法如下： 123456import numpy as npfrom sklearn.preprocessing import PolynomialFeaturesdata = np.arange(6).reshape(3, 2)poly = PolynomialFeatures(2)data = poly.fit_transform(data) 特征转换情况如下： 在一些情况下，只需要特征间的交互项，这可以通过设置 interaction_only=True来得到: 123456import numpy as npfrom sklearn.preprocessing import PolynomialFeaturesdata = np.arange(6).reshape(3, 2)poly = PolynomialFeatures(degree=2, interaction_only=True)data = poly.fit_transform(data) 特征转换情况如下： 估算缺失值 Reason: 由于各种原因，真实世界中的许多数据集都包含缺失数据，这类数据经常被编码成空格、NaNs，或者是其他的占位符。但是这样的数据集并不能被sklearn学习算法兼容，因为大多的学习算法都默认假设数组中的元素都是数值，因而所有的元素都有自己的意义。 使用不完整的数据集的一个基本策略就是舍弃掉整行或整列包含缺失值的数据。但是这样就付出了舍弃可能有价值数据（即使是不完整的 ）的代价。 处理缺失数值的一个更好的策略就是从已有的数据推断出缺失的数值。 Imputersklearn中使用Imputer方法估算缺失值，使用方法如下： 12345from sklearn.impute import SimpleImputerdata = [[np.nan, 2], [6, np.nan], [7, 4],[np.nan,4]] imp = SimpleImputer(missing_values=np.nan, strategy='mean')data = imp.fit_transform(data) 其中strategy参数用来选择代替缺失值方法： 123`mean`表示使用平均值代替缺失值 `median`表示使用中位数代替缺失值 `most_frequent`表示使用出现频率最多的值代替缺失值 missing_values参数表示何为缺失值： 12`NaN`表示`np.nan`为缺失值 `0`表示`0`为缺失值 回归算法（此处涉及线性回归）线性回归模型LinearRegressionLinearRegression的构造函数中有两个常用的参数可以设置： fit_intercept：是否有截据，如果没有则直线过原点，默认为Ture。 normalize：是否将数据归一化,默认为False。 LinearRegression类中的fit函数用于训练模型，fit函数有两个向量输入： X：大小为**[样本数量,特征数量]**的ndarray，存放训练样本 Y：值为整型，大小为**[样本数量]**的ndarray，存放训练样本的标签值 LinearRegression类中的predict函数用于预测，返回预测值，predict函数有一个向量输入： X：大小为**[样本数量,特征数量]**的ndarray，存放预测样本 LinearRegression的使用代码如下： 123456import pandas as pdfrom sklearn.linear_model import LinearRegressionlr = LinearRegression()lr.fit(X_train, Y_train)predict = lr.predict(X_test) 衡量线性回归的性能指标R-Squared公式如下： 其中ymean表示所有测试样本标签值的均值。为什么这个指标会有刚刚我们提到的性能呢？我们分析下公式： 其实分子表示的是模型预测时产生的误差，分母表示的是对任意样本都预测为所有标签均值时产生的误差，由此可知： R2leq1,当我们的模型不犯任何错误时，取最大值1； 当我们的模型性能跟基模型性能相同时，取0； 如果为负数，则说明我们训练出来的模型还不如基准模型，此时，很有可能我们的数据不存在任何线性关系。 R2使用代码 12from sklearn.metrics import r2_scorer2_score(y_true, y_pred)","link":"/2022/05/31/ML_Notes/"},{"title":"《剑来》","text":"知行合一思无邪，历尽千帆仍少年. 人生总是冷不丁的，来上那么一拳，不轻不重的，只是让人无力招架，大概这就是所谓的无力之感了。 春风知我意，送梦到当年。世间多有不妥之人，世道多有不平之事，却休想打杀我心中之美好。 过去已过去，未来还未来。时时是过去，刻刻有未来。过去曾未来，未来会过去。 小石碑，一块块块，竖在门口分两排。（牙齿 劝人笑口常开） 风儿在跟竹叶打架，枝头鸟儿在劝架。 大雪给青山盖了一层又一层的被子，溪水吃掉了一颗又一颗的石头，一天天在长大。 白云不招呼就走，月色不敲门就来。 师父觉得最远的路程，都不是什么去远方，不是去大隋书院，甚至都不是去剑气长城，是师父的小时候，在山上遇到了一场暴雨，然后隔着一条发洪水的溪涧，师父在一边，回家的路，在另外一边。 很多的自欺欺人，在外人看来是可悲可笑的。” “但是对当局者而言，是幸运美好且是必须的。 我是一把镜子，不信的话你瞧瞧，我眼中有没有你？ 只要遇见对的人，双方眼中便会看见最好看的景色，如天各一方，日月遥对，目光却亘古不变。 是一棵参天大树，便力所能及，庇护一方人心荫凉，是尚未成长起来的花草儿，就无忧无虑，慢慢长大，天暖花开，一样是春。 肩挑着美好的世界。 余了好多，余着好久。 看得真切，记得牢靠，才能真正记得打念得好。 一句顶美好的言语，只要被人在耳边唠叨千百遍，就要变得俗不可耐，面目可憎。 强扭的瓜，蘸了蜂蜜糖水，吃到最后，还是苦的，先甜后苦最麻烦。 明天永远属于少年。 少年年年有，我始终在其一。 不付出，就不会珍惜。付出越多越在意。跟好人坏人没什么关系。同样一壶酒，不管原因为何，涨价了还是降价了，喝出来的滋味，喝酒的快慢，都是不一样的。 一个经历越多、攒下故事越多的人，心狠起来最心狠。 人心不是水中月，月会常来水常在。人容易老心易变，人心再难是少年。 以前我总喜欢听好话，听不得半句不好听的。后来遇到了老爷，他就跟我说，好话坏话都会听着的，都别太当真，何况十句好话，往往给一句坏话就打死了。所以每听人一句好话，让我就先余着九成，到时候攒够了好话，就可以等那一句坏话登门做客了，半点不伤心。 世上人，少有这么算账精明、晓得自补心路的，都喜欢只拣好听的听。不然就是富贵得闲了，吃饱了撑着只挑难看的看。 青山绿水千万重，翩翩少年思无邪。 天变未必变天。 许多时候，看见了一部分的真相，最让人自以为是。 只不过寻常人越自以为是，活得越轻松就是了 转过头，笑眯起眼，蓦然灿烂而笑，双脚轻轻跺地，双手飞快晃动。 有心者有所累，无心者无所谓。 所有久别重逢的开怀，都将是未来离别之际的伤心。 搏一搏，厕纸变丝帛，押一押，秃子长头发。 所有能够言说之苦，终究可以缓缓消受。唯有偷偷隐藏起来的伤感，只会细细碎碎，聚少成多，年复一年，像个孤僻的小哑巴，躲在心房的角落，蜷缩起来，那个孩子只是一抬头，便与长大后的每一个自己，默默对视，不言不语。 十岁之前，光阴是一条小溪的缓缓流淌，慢得好像一辈子都长不大，看不到远处的风光。 二十岁之后，根本不在意光阴的流逝，快慢随意，多看一眼都算闲得慌。 三十岁之后，时间开始撒腿狂奔，拽得行人措手不及。 四十岁之后，像那即将入海的滚滚江河。 六十岁以后，又是骤然一变，静谧的湖泊，静止不动。 临终之际，宛如一条瀑布骤然跌落深潭。 心湛静，笑白云多事，等闲为雨出山来 人生路上，许多人都愿意自己朋友过得好，只是却未必愿意朋友过得比自己更好，尤其是好太多。 独处容易让人生出孤单之感，孤独却往往生起于熙熙攘攘的人群中。 今日事之果，看似已经了解昨日之因，却往往又是明日事之因。 人的肚子，便是世间最好的酒缸，故人故事，就是最好的原浆，加上那颗苦胆，再勾兑了悲欢离合，就能酿造出最好的酒水，滋味无穷。 君子之心，天青日白，秋水澄镜。君子之交，合则同道，散无恶语。君子之行，野草朝露，来也可人，去也可爱 心田之间，田垄分明，行走有路，仿佛每一步都不逾越规矩，每天都能够守着庄稼收成，如此约束人心，好事自然是好事，却会让一个人显得无趣 与人坦诚相待，不是时时刻刻掏心掏肺，一方掏出去了，对方一个不小心没接好，伤人伤己。 言念君子，温其如玉。 礼轻情意重，关系没好到那个份上，才会在礼物礼节上过多客气，真是朋友了，反而随意。 心神越小，念头越小，步子越小，我们反而走得快些。 少年喜欢少女，是饮糯米酒酿，酒味其实不重，可是初次喝酒，也能醉人。长大之后，男子喜欢女子，如饮烧酒，一个不小心就要烧断肝肠。上了岁数，老人思念女子，是大冬天，温了一壶黄酒。 明则有王法，幽则有鬼神，幽明皆浑浊，良知还在心。天地乾坤，日月光明 些许忧愁似那小鱼儿，一个甩尾，窜入水草中，再不与人相见。 年轻时记性好，每逢思乡，人事历历在目，心之所动，身临其境，宛如返乡。 上了岁数，记忆模糊，每逢思乡，反而感觉离乡更远。人生无奈，大概在此。 桃花开时，若是花上还有黄鹂，尤为动人，眼不敢动，心魄动也 一年逢好夜，万里见月明。 自知者不怨人，知命者不怨天。 碎碎平碎碎安，碎碎平安，岁岁平安…… 初念浅，转念深，再转念头深见底。此念渐深，见得人心，未必见得本心。 心中积郁，随雪落地。 晓不晓得天底下哪个家伙的忧愁最多。是那个名叫“长大”的家伙 等人是一件很开心的事情啊，然后等着人又能马上见着面就更幸福嘞。 很多时候，过分的聪明，本身就是一把无鞘无柄的长剑，出剑伤人，握剑伤己。 天边泛起鱼肚白，先是米粒之光，然后大放光明。 别伤心别伤心，我把乐呵借你乐呵啊。 山上传闻，真真假假，山水邸报之上，一些个大义凛然言之凿凿的言语，反而就那么回事，一部分真相，只会远离真相，倒是某些三言两语一笔带过的，反而藏着余味无穷的浩然正气。 人心如水，所以与人交心，就是涉水而行，或小河溪涧，清澈见底，或江河滚滚，浑浊不堪，或古井深渊，深不见底，一着不慎，就会淹死人。 什么叫赤子之心？是与所做之事壮举与否，与一个人年纪大小，其实都关系不大，无非是有人过河拆桥，有人偏要铺路修桥，有人端碗吃饭放筷骂娘，有人偏要默默收拾碗筷，还要关心桌凳是否稳当。有人觉得长大是世故圆滑，有人偏觉得成长，是可以为己为人承受更多的苦难。有人觉得强者是无所拘束，是一种唯我独存的纯粹自由，有人偏觉得我要成为强者，是因为我要为这个世界做点什么！ 当一个人的见识过高，往往容易生出惫懒之心，反而不如一知半解、懵懂之人那么拼搏奋进。 花有再开日，年年如此，人无再少年，人人这般。唯有桃李春风一杯酒，总也喝不够。 其实中土读书人，不全是这样意气用事的。只不过很多时候，能够让咱们瞧见的，往往会是些龌龊人糟心事。 人生路上，会遇到很多一别过后再无重逢的匆匆过客。可是人心间，过客却可能是别人的久住之人。还会笑颜，还会高声言语，还会同桌饮酒醉醺醺。还会让人一想起谁，谁就好像在与自己对视，不言不语得让人无话可说。 小错早犯早知道，长辈早说孩子早记住。 修行是每天的脚下事，多年以后站在何处是将来事，既然注定是一桩当下多想无益的事情，不如以后忧愁来了再忧愁，反正到时候还可以喝酒嘛 谁说做了件好事，就不会伤人心了？很多时候反而让人更伤心。 一个人手上的本事不大，嘴边的道理太大，会惹人烦，所以不用着急，先余着。 青山不改绿水长流 时时在法中，处处法无碍。 良辰美景十六事，都比不上个‘今日无事’。 人心炎炎酷暑中，可得一剂清凉散。 昨日对未必是今日对，今日错未必是明日错。 与虎谋皮，是火中取栗之举。但是君子之交，才是天高月白。 老百姓过日子，有些撕心裂肺的伤心事，摔落在世道的坑洼里，人跌到了，还得爬起来继续往前走，伤心事掉下去就起不来了，甚至人熬过去，就是事过去了。 有些事情来了，不会等我做好准备，好像不打个商量就劈头盖脸冲到了眼前，让人只能受着。同时有些事情要走，又怎么拦也拦不住，一样只能让人熬着，都没法跟人说什么好，不说心里憋屈，多说了矫情，所以就想找个长辈，诉几句苦 有些道理，其实姚仙之是真懂，只不过懂了，不太愿意懂。好像不懂事，好歹还能做点什么。懂事了，就什么都做不成了。 莫道君行早，更有早行人。莫道君行高，早有山巅路。 热闹处守口，僻静时守心。 下雨是乡愁的声音。 冬天的积雪，是落在夏天的贫家子身上的一件狐裘，好看是好看，就是穿着难熬。 多年以来，她始终在一处山中，修道幽居，不来见我。 哪处山头？ 我心中。 很多裴钱个儿矮矮时候的有趣事情，就像兜里的瓜子，一磕就没了。 小米粒好像从裴钱袖子上双指捻住了一粒瓜子，往自己嘴里一丢，“小小忧愁，一吃就没。” 老道人一手托瓜，一手轻敲几下，侧耳聆听，自言自语道：“天地氤氲，万物化醇。大音希声美矣，大中至正粹然……肯定甜！” 一个人的气清气浊，其实就看有无一颗平恕心。 只能活在别人心中，活成另外一个自己，一定很辛苦。 孩子吃疼，哇哇大叫。成年人呢…… 书上将道理说破了，好像很简单。只可惜人生各有症结，太难知道一个自己不知道了。 一家和乐，即是大年。 有些远远的喜欢，总是忍不住要让人知道，才能甘心。 一个人的学问多寡，很其次，做人其实最怕拎不清。 心地就是福田，言行就是风水。所以要懂得惜福，要能够藏风聚水。 看了她一眼，人间颜色如尘土。 黄花黄，白云白，青山青，少年年少。 桌上灯半黑，窗外月半明，有人觉得不够亮，有人觉得不算黑。 很多时候，一个人的眼睛里，脸上的细微处，那些未说之话，反而比开口所说言语，更接近真相。 肚子坏水晃荡来晃荡去，归根结底，得有一颗坏胆撑起那份胆识。 当一颗坏胆给彻底碾碎了，变成满是苦胆苦水，坏人就会老实很多。 各有因缘不羡人，各有付出无愧人。 何谓心仪，大概是人海熙攘，惊鸿一瞥，再难忘记。 其实陈平安是想要先与包袱斋欠个人情。 唯有如此，才会有人情往来。 聚散都随缘。 女子不是真的全然不讲道理，只是往往男子所讲的道理，与她们想要听的道理，往往不在一条脉络上。 女子的道理，其实更多在心情。如果男子连她为何不讲理，都整不明白，那就没辙了，自然只会说多错多。 人生如梦，灵犀一动，不觉惊跃，如魇得醒。 有人日丽中天，云霞四护。 有人一味蝇营狗苟。 有人随日开眼界，随月息心。 有人只顾着低头刨食。 有人只恨读书写字，不到古人佳处。 有人在辛苦过活，不奢谈安心之所，只求立锥之地。 有好人某天在做错事，有坏人某天在做好事。 可能学塾里读书最好的少年，飞黄腾达，当了大官，再不返乡。 可能学塾里的顽劣少年，混迹市井，横行乡野，某天在陋巷遇见了教书先生，恭敬让路。 人生有很多的必然，却有一样多的偶然，都是一个个的可能，大大小小的，就像悬在天上的星辰，明亮昏暗不定。 那日丽中天之人，有天骤然跌落泥泞，身上都是过客的鞋印。 那蝇营狗苟之辈，也能为身边人庇护出一方荫凉。 那眼界大开之人，突然有一天对世界充满了失望，人生开始下山。 那些低头刨食之辈，偶然一抬头，便对生活生出希望，走向了远方和高处。 有人觉得人生没意义，没劲，只需要有意思。 有人觉得人生没意思，很苦，但是得有意义。 有些少年暮气沉沉，有些老人少年意气。 有人大梦一场，不曾醒过。有人痛苦万分，难求一醉。 有人觉得只有书上的圣贤才能说道理，有人觉得庄稼汉辛勤劳作就是道理，一位孤苦无依的老妪也能把生活过得很从容。 有人觉得自己什么道理都懂，过不好，怪道理。 如果一辈子都过不好了，咬牙切齿，怨天尤人。白走一遭。 有人觉得自己什么都不懂，过不好，是道理还懂得太少。 如果一辈子还是过不好，对自己说，那就这样吧。到底走过。 有人自己从不曾杨柳依依，草场莺飞。人生道路上，却一直在铺路搭桥，一路栽种杨柳。 有人瞪大眼睛，费劲气力，寻找着这个世界的阴影。等到夜幕沉沉就酣睡，等到日上三竿，就再起床。 明月山头，荆棘林中，绿水池塘，春浪桃花。一样米养百样人，不同的人生不同的道路上，可能都曾昨夜梦魂中，花月正春风。 世道如此，你想如何，你能如何，你该如何。 自律，自省，自求，自由。 多读古书开眼界，少管闲事养精神。 那些人生意外，就像一场突如其来的磅礴大雨，强者手中有伞，弱者两手空空。 强者撑伞而行。要为这个世界遮风挡雨，片刻也好。 摊开手掌，陈平安开着玩笑，说手中有阳光，月光，秋风，春风。 还说人情世故事上练，破我心中犹豫贼。 “一棵山中幽兰。 它从不曾见过世人，世人也不曾见过它。 便不开花吗？” 何谓失望，无非就是万般努力过后，不得不求，求了没用，好像与天地与人求遍都无用。 良辰美景满树花，故人重逢俩无恙 世间不是所有男女情思，都会是那春种一粒粟，秋收万颗子，可能没有什么春种秋收，一个不小心就会心田荒芜，就是野草蔓延，却又总能野火烧不尽，春风吹又生。 但愿。 跋山涉水，风景秀丽。久别重逢，故人无恙。 入庙烧香，有求有应。异乡游子，又逢佳节。 明月皎皎，一样的月光，照过历代圣贤，文人名士，剑仙豪客，照过窗边书生凭栏美人，水上艄公山中樵子，照过夜不能寐的帝王将相，一样也照过鼾声如雷的贩夫走卒，照过高高的华宅飞檐，低低的田埂坟茔，照过元宵的灯市清明的黄纸中秋的月饼年关的春联，照过无人处千百年的白云青山绿水黄花…… 脚步再匆匆，人生需从容。 新一辈江湖人的为人处世，往往劝酒只是为了看人醉后的丑态。 老江湖，是自己酒不够喝，才会劝酒不停，让朋友喝够。或是不缺酒水的时候，劝酒是为多听几句心里话。 可能每个老江湖，都像个酒缸，装满了一种酒水，名为“曾经”。 他说有意思的事，有意义的事，都不容易做到。 有意思的难事，做成了，未必有什么意义。但是一件有意义的事情，做成了，一定很有意思。 忽然春天，蓦然夏天，突然秋天，已然冬天。 大事心静，小事心稳，有事心平，无事心清。 人生路上，真正的过失，错过和失去的，不是什么擦肩而过的机缘，不是失之交臂的贵人，而是那些原本有机会改正的错误。然后错过就失去。 “先生希望落魄山永远是今天的落魄山，我希望先生永远是明天的先生。” “学生相信每个明天的先生，一定会比每个今天更好吧。” 宁姚在不在乎，是一回事，自己在不在乎，绝对是另外一回事。她之所以会不在乎，可不就是自己次次很在乎？ 为人处世，安身立命，其中一个大不容易，就是让身边人不误会。 亲近之人，若想久处无厌，就得靠这个“明明明白”，不会因为诸多意外，或是种种琐碎事情，某天突然让人觉得“你原来是这样一个人”。其实许多误会，往往来自自身的捣浆糊。 人生不能总是处处事事迁就他人，不然老好人一辈子都只能是个老好人。往往老好人的问心无愧，就会让亲近之人吃亏吃苦。 天下姻缘，世间情动，也多有那蓦然回首的悄然生发。 可能是因为离着远了，喜欢的人会更喜欢，讨厌的人也就没那么讨厌了。 真正的书生意气，不是什么都不懂，就偏要与所有老规矩、风俗为敌。 而是很多都懂了，我再来无所谓，单凭自己喜好，说话做事，来跟这个世道，毫不圆滑地打交道。 玉在山而草木润，渊生珠而崖不枯。 许多偶然，实则必然。但是一连串的必然，又会出现万一和偶然。 每一个生性乐观的人，都是主观世界里的王。 那么一个天生悲观的人，就更需要在心境的小天地之内，构建屋舍，行亭渡口，遮风挡雨，停步休歇。 良心在夜气清明之候。 君子立业，贫不足羞。 富贵门户，常有穷苦亲戚来往，不曾空手而返，便是忠厚之家。 路过高门，百姓不会如避灾殃，刻意快步走过，正是积善之门。 世事若飞尘，向纷纭境上勘遍人心。日月如惊丸，于云烟影里破尽桎梏。 无心为之，最是有心。 与亲近之人，不要说气话，不可说反话，尤其不要不说话。 一个大老爷们，记仇确实不好，不大气。多半是修心不够。 很多时候咱们不得不承认，不是所有事情，都可以未雨绸缪的，还真就只能事情来了，再去解决，才能解决。 我怜梅花月，终宵不忍眠。” 陈平安附和道：“终宵不忍眠，月花梅怜我。” 以前国师崔瀺就在此年复一年，日复一日，独来独往，却从无半点寂寥之感。 心之忧危，若蹈虎尾，涉于春冰。 如今多了个师弟，一样行走巷中。 昭昭若日月之明，离离如星辰之行。 真正过去的事情，就两种，完全记不得了，再就是那种可以随便言说的往事。 坏事不怕早，好事不怕晚，早点与之面对，才好早做准备。 大概所谓成长，就是有个谁都不知道好坏的自己，在远处等着今天的我们走过去见面。 只有偷着乐的乐呵，最值得乐呵。 男女情爱，何谓风流薄情，就是一个人明明只有一坛真心酒，偏要逢人便饮。 何谓深情，就是一坛酒深埋心底，然后某天独饮到底，喝光为止，如何不醉。 安稳日子过久了，难免乏味，这是人之常情。人间乐事如饮醇酒，往往醒来就无，极难留住，唯有失落，倒是苦事如茶，往往有机会苦尽甘来，让人倍感珍惜。平淡事就是喝水了，没什么滋味，可就是每天都得喝，不喝还不行。 世上人事无穷，我辈光阴有限，可能正因为如此，所以我们才会更珍惜人间这趟逆旅远游。 每个结局伤感的故事，都有个温暖的开头，每年的大雪隆冬，都是从春暖花开中走来。 何谓自由，就是我们下雨天出门，手里边有把伞，唯一的不自由，就是得撑着伞，别走出伞之外。 很多好道理为何会空，因为说理之人，其实未曾感同身受，与听理之人并未悲欢相通，无法真的将心比心。 就像男女情爱之间的磕磕碰碰，其实女子那些让男子摸不着头脑的情绪，本身就是道理，认可她的这份情绪，再帮忙疏解情绪，等女子渐渐不在气头上了，然后再来与她心平气和说些自己道理，才是正途。这就叫退一步思量，先后顺序的学以致用，一旦跳过前边的那个环节，万事休矣。 一张白纸最易下笔，稚子都可以随便涂抹，一幅画卷题跋钤印无数，好似布满牛皮癣，还让人如何落笔，两者各有好坏吧。 那么读书识字，图什么呢。为人少点戾气，处世多点耐心，渐渐的把脚下道路越走越宽，在世道中，走得稳当些，从容些。 了亏就长点记性，不然就白吃顿苦头了。下了山出门在外，不是爹不是娘的，谁也不会惯着谁。 生活不是处处屠狗场，没那么多狗血。 世道又处处是屠狗场，遍地洒落狗血。 粗粮养胃，糙话活人。 力能胜贫，谨能胜祸，年年有余，每年年关就能年年好过一年，不用苦熬。 约莫是每个人都会有自己的主心骨，行走在复杂的世道上边，帮助我们用来对抗整个世界。输了，就是苦难。赢了，就是安稳。 每一个昨天的自己，才是我们今天最大的靠山。 道不远人，苦别白吃。 每个人都是各自生活的写书人，与此同时，看别人就是翻书。 可能世界把我们看得很轻，但是我们又把自己看得太重。 一个人的理性，是后天积累的学问汇总，是我们自己开辟出来的条条道路。我们的感性，则是天生的，发乎心，心者君主之官也，神明出焉。可惜人为物累，心为形役。故而修行，说一千道一万，终究绕不过一个心字。 人与人两心不契，稍有间隙，便如隔山川，不可逾越。阿良曾经说过，世间言语，皆是桥梁。此言不虚。 人生一传舍，无处是吾乡。世间万物各有归属，哪来的什么主人，我们都只是个当铺伙计。 长大的孩子会把心里话放在嘴边，长大了就是会把心里话好好放在心里。 老人从不觉得一个人的朝气勃勃，只是那种一年到头的言语欢快，行事跳脱。 而是在人生的每一个关隘那边，独独在苦难之际，年轻人反而能够眉眼飞扬，意气风发。 做出最意外的事，递出最快的剑，与这方天地说出最有分量的言语。 平时一贯寡言者，偶尔放声，要教旁人不听也得听。 那就什么都别多想，过日子嘛，还真就有很多事情，只能是船到桥头自然直。 富贵可不用尽，余点就是积福。贫贱不可自欺，敬己就是敬天。 谁终将声震人间，必长久独自缄默。 谁终将点燃闪电，必永恒如云漂泊。 好看的风景，值钱的草药，往往都在险峻处。 大概这就是喜欢。 让一个人能够不像自己。能让乐观者悲观，能让悲观者乐观。能从绝境中看到希望，有胆子去憧憬未来。 能让一个贫寒困苦的陋巷少年，突然觉得自己就是天底下最有钱的人。 能让一个连剑字都不会写的草鞋少年，跨越山与海，默默练拳百万，还要默默告诉自己，一定要成为大剑仙。 好像人生总有些坎坷，是怎么熬也熬不过去的。就算熬过去了，过去的只是人，而不是事。 人间事，其实好坏之别，往往就只差那么一两句话，就可以好坏颠倒。 气头上，多了一两句不该有的重话反话，平日里，少了一两句宽慰人心的废话好话。 因为越是亲近之人，越容易觉得对方做什么事都是天经地义的，都觉得一切只需要在不言中。 结果越是觉得对方应该什么都懂的时候，往往就是对方什么都不懂的时候。 粒善因，只要能够真的开花结果，是有可能花开一片的。 其实不是我在行善事，施舍钱财给他人，而是他人施舍善缘与我。 世态翻覆雨，吾心分外明。 我们称赞一个人，有分寸感，其实就是保持一种妥当的、得体的距离，远了，就是疏离，过近了，就容易苛求他人。所以得给所有亲近之人，一点余地，甚至是犯错的余地，只要不涉及大是大非，就不用太过揪着不放。心细之人，往往会不小心就会去求全责备，问题在于我们浑然不觉，但是身边人，早已受伤颇多。 有些时候，与不讲理之人不讲理，就是讲理。 善解人意，人解善意，善人解意，人意善解 道之大原出于天，天不变，道亦不变，披星戴月，人间大美，此行走好，平平安安 天降之福，先开其慧。最不起眼，也最重要。 十八般武艺傍身，绝不会闲置，总有用到时。 这就叫屋大人少，多生精怪作祟。屋小人多，易生口舌是非。 总得有些人，得比坏人更聪明些，才能有更多的好人有好报，就可以让更多好人做好事，能够可以完全不计后果。 就算注定人力有穷尽时，也要先竭尽人事，再来听天命。无非是能够做成眼前一事是一事，能够手边出力一分是一分。 有些不堪言说的苦难，当一个人好不容易熬过去了，自己默默消受着就是了，别与正在吃苦的旁人说什么轻巧话了，那是作妖作怪。 不惮以最大恶意揣测他人，与愿意对他人给予最大善意，两者只是看似矛盾，其实双方并不冲突。 天底下最难学问在努力，天底下最简单学问在结果。” 师长，同窗，好友，故人好似庭中树，一日秋风一日疏。 有些为难，并非全是当局者迷旁观者清，也可能是当局者想得太透彻。 事事难上难，时时人下人。 骂人先骂己，立于不败之地。多说了一句气话，往往节外生枝，功亏一篑，之前苦口婆心的百般道理，悉数阵亡。少说了一句废话，便起误会，人心处处，杂草丛生，猜忌，失望，怨怼，此起彼伏。唯独老江湖，只在不言中。相逢投缘，下马饮君酒，遇见不平事，杀人都市中。 望之俨然，即之也温，恭而安。 在家乡在异乡，在远游在归途，在山中在山外，在人间在人心，在山河锦绣里，在日月乾坤中，在人间大美处，在世道泥泞上，在剑修如云处，在希望失望重新希望后，先生皆在独自练拳，与天地问拳，与自己问拳。 我们与人讲理，不是为了否定他人。此外，给予他人善意，除了我们自身的问心无愧，也需要讲究一个分寸感。这就是道术之别了，大道唯一，术却有千百种，因人而异，因地而异，所以说当好人，很难嘛。” 什么都知道，跟什么都不知道，一向没什么两样。 做事千百件，还是做一个人 老人一老，就会说些翻来覆去的车轱辘话，三十岁之前的年轻人，听着往往倍感厌烦，来一句“说过了”，便让老人陷入沉默。只是等到年轻人自己变成了中年人，尤其是等到有妻有子了，在面对自家老人唠叨的时候，耐心又往往会变得越来越好。 行路窄处留一步与人行，便是行大道，滋味浓时减三分让人尝，便是真滋味。 没有说出口的特别喜欢，就像一场无声无息的鲸落。 薄暮远岫茫茫山，细雨微风淡淡云。自家数峰清瘦出云来。 曾几何时，溪水渐浅，井水愈寒，槐树更老，铁锁生锈，大云低垂。今年桃叶见不到桃花。如今却是，积雪消融，青山解冻，冰下水声，叶底黄莺，又一年桃花开，报今年春色最好。 真正的强者，不独有令人侧目的壮举事迹，还有坚持不懈的细微付出。 别人愿意说几句心里话，就得好好记住，不能听过就忘，因为天底下好听的心里话，其实不在嘴边，在眼睛里边呢。所以听在耳朵里的心里话，往往就不那么好听了，一来二去，要是总记不住对方说什么，脾气再好的人也要当哑巴了，同时还要让自己不往心里去，不然以后就没人愿意跟我们说心里话喽。 愁绪如山，都攒在眉头，情思似水，都流到心头。 心里多知道，嘴上少说道。 着手处不多，用心处不少，还是很辛苦的，相信掌律长命都看在眼里了。 小有早慧，老有晚福，是两大人生幸事。一个靠上辈子积德，一个靠这辈子行善。 知其不可奈何而安之若命，德之至也。 当某一人太过瞩目，其余人等，难免黯淡失色，旁人要么生出惰性，躺在大树底下好乘凉，要么容易提不起心气。 可能是因为人间所有的悲欢相通，都只会来自感同身受 得众动天，美意延年。 何谓遗憾，不可再得之物，不可再遇之人，就是遗憾。 我们的言语，既会千山万水，迷障横生，也能铺路搭桥，柳暗花明。故而与亲近之人朝夕久处，不可说气话，不可说反话，不可不说话。 世间聚散苦匆匆，一回相见一回老。历史就像一只火盆，装着一堆有余温的灰烬。所有的灰烬，都是已经被彻底遗忘的逝去之人，而那些火星，就是已逝之人却依然留在天地间的痕迹。 得之我幸，失之我命。墙外花开，也是开花。 是一个“书本上不说，老话都不提”的狗屁道理。有些自愿去做的好事，那么行事之人，最好别把好事当做一件好事去做，就可以为自己省去许多麻烦。既符合书上道理所谓的君子施恩不图报，关键是可以保证未来不管发生了什么，都不会有任何失望，再有他人之回报，就都是意外之喜了。 天底下不少好人做好事，好人是真，好事也是真，唯一问题，在于他们兴许可以不求利字之上的丝毫回报，却难免会索求他人人心之上的某种回响，一旦如此，那么在某些被施恩之人眼中，甚至还不如前者来得清爽、轻松。 穷人的钱财，就是手心汗，不累就无，累过也无。 攒钱给子孙，未必是福，接不住还是接不住，唯独行善积德，留给子孙的福报，他们想不接住都不行，最重要的，是老话说，家家户户都有一块田叫福田，福田里边容易生出慧根，所以余给子孙一块福田，比什么都强，比钱财，甚至是比书籍都要好。 天上月，人间月，负笈求学肩上月，登高凭栏眼中月，竹篮打水碎又圆。山间风，水边风，御剑远游脚下风，圣贤书斋翻书风，风吹浮萍有相逢。 万事尽量从最坏处打算，未雨绸缪，思虑周全，之后一切，就都可以视为往好处好一点点转变之事了。 苦中作乐，只是处世法，苦不自知，才是立身道。 此生此身在此时此地见此景，心不可得。 一生痴绝处，无梦到龙州。青山立眼前，初逢两少年。 学者立身希圣希贤，释者发心成佛成祖。取法乎上，仅得乎中，总是先有一等心思才能有二等人三等事 人生路上，有时接纳一个极有分量的道理，哪怕这个道理再好，就是一个登山之人的背篓里增添了一块大石头。会让人步履蹒跚，不堪重负，苦不堪言。 人难无过错，人生多错过。事错过，错过人，反复思量，都是过错，过去的错。 放开眼界看，世上几百年旧家无非积德行善，头顶三尺有神明。理当如此说，天下第一件好事还是立志读书，功夫不负苦心人。 “吃年夜饭，再跟人一起守夜，无法想象的事情。”——“等到新鲜事不新鲜了，还能照旧，才算是件无法想象的事情。” “有那人间美事之一，却最不赏心悦目，你猜猜看，是什么事情？”——“睡个回笼觉。” 但是被人喜欢，是一件很难得、需要很珍惜的事情唉，比不被讨厌还要难嘛，所以可不是一件可以拿来炫耀的事情，就应该只是一件偷藏在心里的高兴事啊，然后偶尔心情不好的时候，一开门，就会高兴嘞，一开门就心情好，所以就叫‘开心’嘛。” 秋霁，眉妩，赚煞，山渐青，水龙吟，眼儿媚，更漏子，水调歌头，卜算子慢，千秋万岁，花雪满堆山，荷叶铺水面，春从天上来，入梦来，风波定，好事近…… 畏强者凌弱，媚上者欺下，很难有例外之人事。你要是没有与强者心平气和说道理的心气，就定然会对弱者容易失去耐心。 何谓豪杰，总有那么几件事，天下人都做不到，我做得。何谓圣贤，总有那么几件事，天下人都可做，我做不得。 思君如弦月，一夜一夜圆。 始终被他人寄予希望，会让自己觉得不孤单。 人辛苦活着，骗过自己，就是希望。 可能我们都曾对这个世界感到失望，但是我们都愿意对这个世界寄予希望。 我们都是一盏灯火，在天地间忽明忽暗。言行互为卯榫，人心共作灯火。搭建屋舍，抱团取暖。 爱欲之人，犹如执炬逆风而行，必有烧手之患。 心怀远望又谨慎之人，能成大功。秉性忠良敦厚之人，可托大事。 想要省心省力，就得花大价钱，用足够的钱填平人心大坑。 挣钱小心，花钱大方，自家钱财不管多寡，都从正门出入，就是一家门风所在。钱要挣，积德也别耽误。不然夜路走多了，偏门财攒得越多，就越容易出事情，还会祸及子孙。世间钱难挣，祖荫福报更难积攒。 一穷二白的时候，挣点偏门钱，以此发家，无可厚非，等到有钱了，就得挣正门钱了。否则德不配位，坐拥金山银山，福祸转换只在一夕之间，钱算什么，前人田地后人收。 大钱是上辈子带来的，书是给下辈子读的。 总会有些人，会让我们想要成为那样的人。 光阴总是最不讲道理的，就像一个跟人打架从没输过的，偷东西从没落空过的蟊贼。 一个心里边装着很多人的人，就容易心肠软，看待世界的目光太温柔。 一个人知识上的充沛，会给自身带来一个巨大陷阱，计算力和智力上的优越感，那种习惯性居高临下看待所有人的眼光，迟早要出问题，大问题！ 老人一年一年老，少年却难再年少。 天上皎皎明月光，人间匆匆少年郎，脚步最匆匆。 对一个还是孩子的人来说，早早懂得哪怕明明是某个极好的道理，所谓的更早懂事，就是一种残忍。 多少人来看明月，谁知倒被明月看。 眉是聚愁峰，眼是折柳渡。 心仪女子之美，总是这般动人，美得教人装得下日月的双眼都装不下她，得搬去心扉，余在心头。 初见时，她姗姗然从我心头路过，荒芜之地就开满了花。惨绿少年春游遍，罗绮百花成丛，就中堪人属意，最是XX，还是XX，只是XX。九岁与卿初相识，再见卿时吾九十。少年骑竹马，转身白头翁。 天底下单相思的痴情，好像便都是这般一文不值的。可若是值钱，又何必相思呢。 既然已经想了那么多，还想那么多做什么。 梦回少年丛中，吾亦是少年。 书上说，天下没有不散的筵席，但是不要怕，书上还说了，人生何处不相逢。 过尽千帆皆不是，当时只道是寻常。 男女情事，谁先动心谁吃亏，越吃亏越难难忘，到最后，到底是喜欢对方呢，还是喜欢自己，都搞不清楚了，答案偏偏在对方身上，所以才说，由爱故生忧。 此身原本不知愁，最怕万一见温柔。 飒飒松风，一天天的，就这么撞罢晨钟又暮鼓，每天做完课业吃完饭，睡觉醒来又是一天，光阴如水悠悠过。 夕阳西下，就像有人在天边放了一把大火，烧得云海鲜红。 湖光山色有无中，人生行乐须年少。 有心为善与无心为恶两事。 违心的事情，不要做。发自本心的事情，但是有违江湖道义的事情，也不要做。今日做不成，未来有望做成的事情，切不可为达目的不择手段，不要着急去做。 雪月两相宜，少年更清绝。 好像这个世道，越相信好人有好报的人，总是过不上好日子，不是烂好人，就是穷好人。就像把阳关大道让出来，只能自个儿走独木桥，辛苦攒下点钱，都还给了日子，最后只攒了一肚子苦水，又不愿意说给身边亲人，朋友，晚辈，说给他们听。 没力气去自怨自艾的可怜，才可怜，无可奈何，没法子，还能如何，就这样。 有些人，历经坎坷，总能峰回路转，柳暗花明。但是有些人生如船搁浅，水道提纲如一线，进不得，退也不得，原地鬼打墙。好像做多错多，就只能破罐子破摔。 一个每天把无所谓摆在脸上的人，可能才是真正有所谓的。 一个人，如果连做梦都不敢了，得多苦啊。昔去花如雪，今来雪如花，良辰美景总不虚设，如何安顿无限心。可能我们都与这个世界，有过情人一般的缱绻，互为仇寇一般的怒目相向，聋子与瞎子一般的自说自话，无话可说之人与不可言说之人，相对而视，哑口无言。 既冬日可爱，又如沐春风。 孩子就是孩子，所以有些事情，成人不能奢望孩子们去理解，有些道理，就真的只能孩子们在各自成长过程中，去慢慢体会。如果说梦想是堆雪人，大概成长就像吃冷饭。 做事情，要急缓得当，松弛有度 攒点小钱钱，可难可难。 你想说什么，我当然知道，可那只是我们想的，我真正在意的，是XXX自己怎么想的。 做买卖的人，有自己的生财之道，自古而然，只是生意人，归根结底还是做人，还是要讲一讲底线的。”“买卖想长久，跟着大势走。”“可要是亏心事做多了，人不收天收。 雾失楼台，月迷津渡，往事已空，如一梦中。一身犹在，乱山深处。枯木犹能逢春，老树尚可着花。故人呢？ 春者天之本怀，秋者天之别调，花开花落又开。 就像端着小碗，春暖花开，天清气朗，今日无事，平平安安。于事，不问收获问耕耘，莫向外求。于心，勤勉修行戒定慧，与天祈福。 卯时，天微亮，山中多雾，气象清新，朝露凝结在花叶，团团圆圆，摇摇晃晃，欲语还羞。 一棵参天大树，有些原本粗壮的树枝会在风雨中腐朽剥落，有些纤细枝条，却会逐渐成长为粗壮的枝干，再生长延伸出更多的枝丫，绿叶葱郁，供后世子孙乘凉者，就是祖荫福报 我们如何看待这个世界，这个世界就如何看待我们。 人生可能没有真正的同悲共喜，大概就像两个人，就是两座天地。各有所思，你情我愿，此消彼长，教人间没个安排处。 只是年岁渐长，就会越来越明白一个道理，哪怕是与人给予善意这种事，我之心无愧疚，对待某事不曾多想，与他人之心思百转，反复思量，同一件事会是两种心思。懂得这个道理，不叫无奈，而是成长。照顾他人内心，本来就不是什么简单的事情。 我与我之外，即是天地之别。有人与这个世界有过情人一般的旖旎和争执，也有人与这个世界有过仇人一般的怨怼与和解。 说话，得留点余地。说话结实，跟个糯米团似的，好吃是好吃，就是容易撑到，不如一碗白米粥，养胃。 男女情爱一事，心心念念，求之不得的，其实都只是心目中的那份儋州云霞钱江潮，牵肠挂肚，百般恨千种怨，怎一个愁字了得，可等到真正得手了，儋州云霞钱江潮还是儋州云霞钱江潮，心却变了，风动耶旛动耶，心动而已。 狗掀帘子全凭嘴呗 对孩子来说，什么叫长大，大概就是能够爹娘不管，想吃什么就吃什么。对成人而言，什么叫有钱，也许就是可以不看价格，想买什么就买什么。 人生美好风景如初见，风景得是多美好。 混账话，糊涂话，玩笑话，轻巧话重话，打开天窗的亮话，盖棺定论的明白话 有事直接说，不管是大事小事，宁肯当场吵架，惹来对方的不高兴，也绝对不给“误会”留出丝毫余地。","link":"/2022/06/04/Reading_Notes_%E3%80%8A%E5%89%91%E6%9D%A5%E3%80%8B/"},{"title":"C exercises","text":"This article records my process of learning C. 不使用第三个变量达到交换两个变量值的目的123456789101112131415161718192021#include&lt;stdio.h&gt; int main(void) { int a,b; //Enter a and b: scanf(&quot;%d %d&quot;,&amp;a,&amp;b); printf(&quot;a=%d b=%d\\n&quot;,a,b); /*********Begin*********/ a = a+b; b = a - b; a = a - b; /*********End**********/ printf(&quot;a=%d b=%d\\n&quot;,a,b); return 0; }result：input: 1 2a=1 b=2a=2 b=1 数字分离输入一个三位数，分别求出x的各位数字，十位数字，百位数字的值。 思路：可以用数组！！！ 1234567891011121314#include&lt;stdio.h&gt; int main(void) { /*********Begin*********/ int i; char str[3]; scanf(&quot;%s&quot;, &amp;str); for (i = 0; i &lt; 3; i++) { printf(&quot;%c &quot;, str[i]); } /*********End**********/ return 0; } 实数除法保留小数部分12double aveave = (float)23/2 求最大公约数123456while (m % min != 0 || n % min != 0){ min--;}printf(&quot;最大公约数是:%d\\n&quot;, min); 字符串中各类字符数的统计12345678910111213141516171819202122232425262728293031323334353637#include&lt;stdio.h&gt;#include&lt;string.h&gt;#include&lt;ctype.h&gt; int main(void) { /*********Begin*********/ char test[100], c; int ea = 0, num = 0, sp = 0, oth = 0, len, i; scanf(&quot;%[^\\n]&quot;, &amp;test); //注意:scanf里面用%s不能接收字符窜中的空格，空格以后会被截断，用正则即可解决。 len = strlen(test); for (i = 0; i &lt; len; i++) { if (isdigit(test[i])) { num++; } else if (isalpha(test[i])) { ea++; } else if (test[i] == 32) { sp++; } else { oth++; } } printf(&quot;%d %d %d %d&quot;, ea, num, sp, oth); /*********End**********/ return 0; } 求各位数字之积123456789101112131415161718#include&lt;stdio.h&gt; int main(void) { /*********Begin*********/ int num, sum = 1; scanf(&quot;%d&quot;, &amp;num); while (num) { sum *= num % 10; num /= 10; } printf(&quot;%d&quot;, sum); /*********End**********/ return 0; } 将十个数进行从大到小的顺序进行排列。123456789101112131415161718192021222324252627282930#include&lt;stdio.h&gt;int main(void){ /*********Begin*********/ int i, j, a[10]; for (i = 0; i &lt; 10; i++) { scanf(&quot;%d&quot;, &amp;a[i]); } for (i = 9; i &gt; 0 ; i--) { for (j = 0; j &lt; i; j++) { if (a[j] &lt; a[j+1]) { a[j] = a[j] + a[j+1]; a[j+1] = a[j] - a[j+1]; a[j] = a[j] - a[j+1]; } } } for (i = 0; i &lt; 10; i++) { printf(&quot;%d &quot;, a[i]); } /*********End**********/ return 0;} 打印杨辉三角（以空格分隔整数）123456789101112131415161718192021222324252627282930313233343536#include&lt;stdio.h&gt;int main(void){ /*********Begin*********/ int a[10][10], i, j; for (i = 0; i &lt; 10; i++) { a[i][i] = 1; a[i][0] = 1; } for (i = 2; i &lt; 10; i++) { for (j = 1; j &lt; i; j++) { a[i][j] = a[i-1][j-1] + a[i-1][j]; } } for (i = 0; i &lt; 10; i++) { for (j = 0; j &lt;= i; j++) { printf(&quot;%d&quot;, a[i][j]); if (i != j) { printf(&quot; &quot;); //保证每行最后一个数字后面没有空格 } } printf(&quot;\\n&quot;); } /*********End**********/ return 0;} 字符逆序1234567891011121314151617181920212223#include&lt;stdio.h&gt;int main(void){ /*********Begin*********/char a[200];int i=0,j=0,t=0,k;scanf(&quot;%s&quot;,a);while(a[i]!='\\0'){ //important i++; j++;}for(i=0,k=j-1;i&lt;k;i++,k--){ t=a[i]; a[i]=a[k]; a[k]=t;}for(i=0;i&lt;=j;i++) printf(&quot;%c&quot;,a[i]); /*********End**********/ return 0;} 字符串统计 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include&lt;stdio.h&gt;#include &lt;string.h&gt;int main(void){ /*********Begin*********/ char a[100]; int i, j, pos = 0; int str_len, word_len, max_word_len; while(1) { str_len = word_len = max_word_len = 0; fgets(a, 100, stdin);//fgets函数的用法 if (strlen(a) &lt;= 1)//输入的字符只有一个的情况 continue; if (strlen(a) &lt; 99) //remove '\\n' a[strlen(a)-1] = 0; if(strncmp(a,&quot;stop&quot;, strlen(&quot;stop&quot;))==0) break; for(i = 0; a[i] !='\\0'; i++) { if(a[i] != ' ') { word_len++; str_len++; continue; } if (word_len &gt; max_word_len) { max_word_len = word_len; pos = i - word_len; } word_len = 0; } if (word_len &gt; max_word_len) { max_word_len = word_len; pos = i - word_len; } printf(&quot;%d &quot;, str_len); for (i = pos; i &lt; pos + max_word_len; i++) printf(&quot;%c&quot;, a[i]); putchar(10); } /*********End**********/ return 0;} 输出若干个学生成绩中的最高分.要求用指针函数实现输入 第一行为整数n，代表学生的数量。 第二行为n个学生的成绩，n个整数之间用一个空格隔开。 123456789101112131415161718192021222324252627282930313233343536373839#include&lt;stdio.h&gt;/*********Begin*********/int *maxn(int, int array[]);int *maxn(int n, int array[]){ int max, i; int *p; max = array[0]; for (i = 1; i &lt; n; i++) { if (array[i] &gt; max) { max = array[i]; } } p = &amp;max; return p;}/*********End**********/int main(void){ int n,s[110]; int *p; scanf(&quot;%d&quot;,&amp;n); for(int i=0;i&lt;n;i++) scanf(&quot;%d&quot;,&amp;s[i]); int ans; /*********Begin*********/ p = maxn(n, s); ans = *p; /*********End**********/ printf(&quot;%d&quot;,ans ); return 0;} 最小公倍数求法1a / 最大公约数 * b 使用 long long int 注意点12long long int a;printf(&quot;%lld&quot;, a); 用递归求Sn=1!+2!+3!+4!+5!+…+n!之值，其中n是一个数字。12345678910111213141516171819202122232425#include&lt;stdio.h&gt;long long solve(long long n){ /*********Begin*********/ if(n ==1) { return 1; } else { return n * solve(n-1); } /*********End**********/}int main(void){ long long n; scanf(&quot;%lld&quot;,&amp;n); long long ans=0; for(long long i=1;i&lt;=n;i++) ans+=solve(i); printf(&quot;%lld&quot;, ans); return 0;} 读取a.txt中文本，统计文本中字母数量。123456789101112131415161718192021222324252627282930313233343536#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;int main(void){ int n, k; char a[100]; k = 0; FILE *fp; fp = fopen(&quot;a.txt&quot;,&quot;r&quot;); for (n = 0; n &lt; 100; n++) { if (feof(fp)) { break; } fscanf(fp, &quot;%c&quot;, &amp;a[n]); if (a[n] == '\\0') { break; } if (a[n]&gt;='A' &amp;&amp; a[n] &lt;= 'Z' || a[n]&gt;='a' &amp;&amp; a[n] &lt;= 'z') { k++; } } printf(&quot;%d&quot;, k); fclose(fp); return 0;} 读取文件中指定学生信息12345678910111213141516171819202122232425262728293031323334353637383940#include&lt;stdio.h&gt;#include&lt;string.h&gt;extern void solve();int main(void){ FILE *f=fopen(&quot;a.txt&quot;,&quot;w&quot;); fprintf(f, &quot;%s\\n&quot;, &quot;11405200101 zhangsan 70 80 90 2140 80&quot;); fprintf(f, &quot;%s\\n&quot;, &quot;11405200102 lisi 80 60 70 210 70&quot;); fprintf(f, &quot;%s\\n&quot;, &quot;11405200103 lisasdsi 80 10 70 220 70&quot;); fprintf(f, &quot;%s\\n&quot;, &quot;11405200104 liassi 80 60 70 240 70&quot;); fprintf(f, &quot;%s\\n&quot;, &quot;11405200105 lzissssi 80 30 76 210 70&quot;); fprintf(f, &quot;%s\\n&quot;, &quot;11405200106 lisgsi 80 50 70 210 70&quot;); fprintf(f, &quot;%s\\n&quot;, &quot;11405200107 lizssi 80 67 70 280 76&quot;); fprintf(f, &quot;%s\\n&quot;, &quot;11405200108 lisqsi 80 60 70 210 70&quot;); fprintf(f, &quot;%s&quot;, &quot;11405200109 lisdssi 80 67 70 210 70&quot;); fclose(f); char s[100]; scanf(&quot;%s&quot;,&amp;s); solve(s); return 0;}void solve(char s[]) { FILE *fq = fopen(&quot;a.txt&quot;, &quot;r&quot;); int n; char line[20], a[100]; for (n = 0; n &lt; 9; n++) { fgets(line, 12, fq); fgets(a, 100, fq); if (strcmp(s, line) == 0) { printf(&quot;%s %s&quot;, line, a); fclose(fq); break; } if (n == 8) { printf(&quot;Not Found!&quot;); } }}","link":"/2022/05/30/C%20language%20exercises/"},{"title":"Life experience","text":"Some life experiences. 在法国的一些米其林三星就餐时，服务员通常会拿上两份菜单，标有价钱的一份是男士的，没标价钱的是女士的。这样做主要是为了女士能够不受价格影响选择自己喜欢的菜肴，而买单的事情则交给男士就好。 在知乎上，有一个关注度很高的提问：“初次跟女生吃牛排时，女生对服务员说要八分熟，应该说些什么来化解尴尬？”题主之所以说尴尬是因为牛排一般没有偶数熟的，他大概觉得在服务员面前，女生这么说很丢面子。下面的回复几乎没有一条是认真帮助题主解答问题，多是在讽刺题主在此时表现出来的好面子和低情商。一条获得了一万四千个赞同的回答是：“我也要八分熟的。”高情商和低情商的对比如此清晰。 5 mistakes ruining your focus The quality of my life is directly proportional to my ability to focus. 5 Mistakes: Not having a plan need to make a plan. Ignoring how you feel. You’re switching between too many tasks Do one thing at a time. Turn off notifications. You’re not taking breaks Set time intentionally Block out time for a specific task and do not deviate from it during that time. Work until focus drops Work on a specific task with full focus until you start getting distracted. The thing that you do during the break should not create new obligations. You’re not hitting the golden trio(Diet, Exercise and sleep) There is no such thing as a loser who wakes up at 5 AM and does a workout. Question that you can ask yourself: Do you have a plan? What emotion am I avoiding? Boredom Anxiety(fear) Self-Doubt(fear) What would this look like if it were fun? Play Power People The opposite of distraction is traction. You have to have a plan. What would this look like if it were fun? Play Power People 5 Small changes to supercharge your focus The ready to resume plan. The 40 Hz tactic(music). The 10-minute Declutter. The 90-20 Rule. For every 90 minutes of work, consider taking a 15-30 minute break. The color coding hack. The opposite of distraction is traction. 7 Life lessons I wash I knew sooner The value of headspace The power of mornings Define your constraints The last man standing Avoid optimising away the fun Dream moments Journey over destination 1. The value of headspace Focusing on one thing Having a balanced liffe Not burning out You can have anything you want. But not everthing you want. 2. The power of mornings Eat the frog first. 3. Define your constraints Draw constraints around their life and optimise their work within that. 4. The last man standing The last man standing is often the winner. The only thing that will make you fail is quitting. Just keep on going. Keep playing the game. Being in the game. 5. Avoid optimising away the fun Left unckecked, players will optimise the fun out of the game they are playing. Be wary of optimising the fun out of what you are doing. 7. Journey over destination There is no achievement you can achieve, that will make you any happier than you are right now. Focus on enjoying the journey. Change your life by journalling - 10 powerful questions What would you do if money were no object? What would you like people to say at your funeral? If I repeat this week’s actions for 10 years, where does it lead, and is that where I want to be? What activities in the last 2 weeks have energized and drained me? How is your wheel of life? What is your Odyssey Plan? What is the goal and what is the bottleneck? Which goal will have the greatest positive impact on your life? Do you work for your business or does it work for you? If you know you’d die in 2 years, how would you spend your time? What would you do if money were no object?The talent service prompt: Imagine a world where you have all the money and all the time. How would you use your talents and skills to serve others? What would you like people to say at your funeral? Family Friends Coworkers People impacted by your work No one cares about your performance. They care about how you make them feel. What activities in the last 2 weeks have energized and drained me?Maximising Energy How do I do more of the things that energise me and less of the things that drain me. Convert Drainers Find a way to convert things that are currently draining to a source of energy. How is your wheel of life? What is your Odyssey Plan?The Current Path Write out, in detail, what your life would look like 5 years from now if you continued down your current path. The Alternative Path Write out, in detail, what your life would look like 5 years from now if you took a completely different path. The Radical Path Write out, in detail, what your life would look like 5 years from now if money and social obligations, and what people would think, were completely irrelevant. You have got way more control and options available than you might think. What is the goal and what is the bottleneck?Example: 5 Actionable ways to become more self-disciplined**Discipline: ** Getting yourself to do something that you don’t feel like doing. Make the work feel good. When something feels good and enjoyable, you don’t need to rely on discipline. Figure out what your main thing is. Focus on progress not immediate success. Appreciate the tension between higher and lower self. Balence structure with flexibility. Stick don’t switch. Figure out what your main thing is If a man knows not to which port he sails, no wind is favourable. Do fewer things and obsess over quality. Figure out your essentials and eliminate the rest. What are the 3-4 goals that will most positively impact my life this year? If I only had 2 hours a week to do all of my work, what would I do in that time? Focus on progress not immediate successFogg Behaviour model: B=MAP Model What would the smallest and easiest version of this look like? Appreciate the tension between higher and lower self Get these characters to talk to each other. Hear and accept what they have to say. How to 10x your income - The 4 ladders of wealth Ladder 1: Time for money. Ladder 2 : Your own service business. Ladder 3 : Productized services. Ladder 4 : Selling products. Ladder 1: Time for money Hourly job working for a company. Salary working for a company. Ladder 2 : Your own service business Hourly work for clients. Finding clients who will actually buy your service. Creating proposals for those clients and pitching for work. Knowing how to price your service. Charging by the project. Managing a team to do service work. Ladder 3 : Productized services Fixed scope for a fixed price. Selling consulting packages. Recurring services provided by employees. Recurring productized services. Know how to write sales copy that can make a sale without direct customer interaction. Learn to design sales pages or hire experts for sales and landing pages. Understand online payment processing and develop standardised systems for consistent service quality. Ladder 4 : Selling products Digital products(ebooks, courses, downloadables). Products sold in an existing ecosystem. Physical products &amp; ecommerce. Subscription software launched with consulting services. SaaS(Software as a Service). Marketplaces &amp; Social Networks.","link":"/2022/05/30/Life%20experience/"},{"title":"《Rich Dad Poor Dad》","text":"穷人和中产阶级为钱而工作。富人让钱为他工作。 第一课：富人不为钱工作1、 穷人和中产阶级为钱而工作。富人让钱为他工作。 2、 机会总是转瞬即逝，知道什么时候要迅速做出决定是一项非常重要的技能。 决断性：你做出决定的速度越快，抓住机会的可能性就越大。 3、 学习金字塔：我们通过行动——事件或模拟，才能学得最好。（体验式学习） （阅读和讲座是学习效果最差的方式） 4、 生活就是教师： 从尝试和错误中学习变得越来越重要。 5、 改变你能改变的： 生活中的很多事情是我们无法控制的，我们需要专注于我们所能控制的。如果事情必须改变，首先要改变的就是我们自己。改变自己比改变他人更容易。 6、 资产大于收入： 购买或创造提供现金流的资产能使你的钱为你工作。 7、 真正的学习需要精力、激情（愤怒和热爱的结合体）和热切的愿望。 8、 对大多数人而言，给他们的钱越多，他们欠的债也就越多。 9、 正是出于恐惧的心理，人们才想找一份安稳的工作。 10、 人们的生活永远被这两种感觉所控制： 恐惧和贪婪 11、 对钱的无知导致了恐惧和贪婪。 12、 第一步：讲真话——弄清真相——你真正的感觉 13、 第一恐惧： 与金钱密切相关 14、 用钱买来的快乐往往是短暂的。 15、 很多人说“我对钱没兴趣”，可他们却每天工作8小时。 16、 忠实于你的感情，以你喜欢的方式用你的头脑和感情，不要让他们控制你。 17、 感情只是感情，你还必须学会抛开感情来思考。（感情常常过多地代替了思考） 18、 支配钱，而不是害怕它。 19、 支配金钱： 在利率走低和股市不确定的情况下，储蓄和长期投资的古老格言已毫无意义。 20、 造成贫困和财务问题的主要原因是恐惧和无知，而不是经济环境、政府或者富人。 21、 不幸的是，对许多人来说，离开学校是学习的重点而不是起点。 22、 以工资的多少来决定过什么样的生活不是真正的生活。认为工作会给你带来安全感其实是在欺骗自己。 23、 收入不平等导致富人和穷人之间产生一条可怕的鸿沟，当其大到极点时一个社会就会崩溃。 24、 学会让感情跟随你的思想。 25、 直面我们的贪婪、弱点和缺陷是唯一的出路，这条路需要你用心去确认你的思想，确认我们该怎样思考而不只是对感情做出反应。 26、 情感和理智： 当情感占据上风时，理智就会下降。 27、 想象的力量。 为什么要教授财务知识1、 你挣了多少钱并不重要，重要的是你留下了多少钱。 2、 资产是能把钱放进我口袋里的东西。负债是把钱从我口袋里取走的东西。 3、 现金流说明了问题，即一个人怎样处理他的钱。 4、 如果你发现你已在深渊，那你自己就别再挖了。 5、 比钱更重要的是什么？ 企业团队！ 谁属于你的团队？ 6、 聪明人总是雇用比他更聪明的人。 7、 对于钱的热衷会降低财商，钱能使决策变得情绪化。 8、 为什么富人越来越富? 因为他们资产项产生的收入足以弥补支出，还可以用剩余的收入对资产项进行再投资。 9、 为什么中产阶级无法摆脱财务问题？ 当他们的工资增加时，指支出也增加，税收也增加，并且没有把钱投在那些能带来收入的资产上。 10、 克服恐惧： 不害怕失败，“我志在将每一次灾难转化成机会。” 11、 克服愤世嫉俗： 看起来最糟糕的时期实际上正是赚钱的最佳时期。 12、 克服懒惰（忙碌之下，没时间管理他们的财富，健康或者是社会关系）： “贪婪一点儿” 13、 克服坏习惯： 需要先支付自己，考虑如何获取额外的收入来支付债权人。 14、 克服骄傲： 15、 积累资产需要胆量、耐心和对待失败的良好态度。 16、 剧烈的变化就在眼前 17、 投资股票方式： a) 证券集中化（集中于几种投资）而不是实行多元化投资 18、 巨大的经济变革正在来临 19、 一个有准备的人，无论经济走向何方、何时发生变动，都能获得成功 20、 关心自己的长期财务安全 21、 学习成为一名投资者，而不是把钱交给别人替你投资 22、 不要躲避风险，要学会如何驾驭风险 23、 敢于冒险：财务自由的价格是用我们的梦想、渴望和战胜一路上不停袭来的失意的能力来计量的 24、 财商不是指你赚了多少钱，而是指你有多少钱、这些钱为你工作的努力程度，以及你的钱能维持几代 25、 同时在B和I两个象限工作会让你在财务自由的世界里站得更稳 26、 真正的投资者在不利的市场中挣到的钱更多 27、 富人和穷人之间唯一的差别就是他们在闲暇时间所做的事情 28、 如果你一定要忙碌，那么就在两侧的象限同时忙碌，以便有更好的机会去获得更多的自由时间和更大的财务自由 29、 当你工作的时候，请努力工作；下班后，你用你的薪水和闲暇时间所做的事情将决定你的未来 30、 推荐道路：先从E/S到B象限，再到I象限 a) 理由：经验和教育：如果你首先在B象限获得成功，那么你将更有可能成为一个有影响力的I，I投资于B（真正的投资者只在拥有稳定企业系统的成功的B身上投资） b) 现金流：如果你拥有一家企业并且运作良好，那么，你因该有多余的时间和现金流支持你在I象限的活动 31、 进入B象限，目标是：拥有一个系统，让人们通过这个系统为你工作 32、 在建立一个成功的公司之前，你可能会先赔掉两三个公司 33、 成功是一位贫乏的教师，他能教给你的东西很少。我们总是在失败的时候学到最多的东西 34、 一个真正的B不只是有一个系统就行，还需要有优秀的人去操作这个系统 35、 如何迅速成为B： a) 找一位导师（已做过并且成功地做了你想做的事情的人） b) 特许经营权： 通过购买特许经营权系统，而不是努力创办自己的系统，能够让你集中精力开发自己的员工队伍 排除一个重要不确定性因素：许多银行愿意贷款给特许经营企业，而不愿意贷款给新成立的小企业 使用好的系统创业将会降低经营风险 c) 网络营销： 1、想要成功，你需要克服害怕被拒绝的心理，不必过多地考虑他人对你的评价（你怎么看我我管不着，最重要的是我怎么看我自己） 2、学会领导别人 36、 小心接受别人的建议。虽然你必须思想开放，但是你一定要先搞清楚这条建议来自哪个象限 37、 管理者通常把他们的下属看成不如他们的人，而领导者必须指挥那些通常更为聪明的人 38、 创办自己的系统需要多次的尝试和失败，，付出大量的前期法律成本以及大量的文字工作。但在完成这些工作的同时你也在形成了自己的员工队伍 39、 银行不会把钱借给没有系统的人 40、 一个企业想要生存和发展，需要多种系统共同发挥作用（例如：营销、财会、销售、人力资源、法律等系统） 41、 注意时间的价值，时间是我们最宝贵的财产之一 42、 投资者的七个等级： 第0级：一无所有的投资者（大约50%地成年人都属于这个等级） 第1级：借钱者（他们拥有的东西都跟负债有关） 第2级：储蓄者（适当储蓄，建议在银行存入可支付半年到一年的生活开销的现金，其余可用来投资） 第3级：“聪明的”投资者：可分为三级： A级： 他们确信自己弄不懂钱是怎么回事而且永远不会懂 B级：“愤世嫉俗者”（他们装成智者并用自己内心的恐惧影响身边的人） C级：“赌徒”（不靠长期的勤勉、学习和领悟，而靠所谓的“内幕消息”或者“捷径”，在投资问题上过于懒惰） 第4级：长期投资者（十分清楚地列出长期计划，并通过该计划达到财务目标） 第5级：成熟投资者（“财力充足”，能够制定出更积极地或者更有风险的投资战略。） 第6级：资本家（通过使用别人的智慧和财富，为自己和他人创造投资） 43、 如何成为一个长期投资者： 坐下来，制定一个计划，控制你的花钱习惯，把你的各种债务最小化；用你的钱生活，并增加你的财富；弄清楚你每月要投资多少钱，按实际回报率用多长时间能收回成本，以最终实现你的目标（目标：我计划在多少岁时停止工作？我每月将需要多少钱？） 简化你的投资，不要频繁地改变花样。 忘掉那些复杂地投资，只做绩效好的股票和共同基金 学会购买封闭式共同基金 不要试图超越市场 将保险工具作为保障措施而不是积累财富的措施 没有“百分之百保险的投资”，指数基金同样有其固有的悲剧性缺点 别再等待“大额交易”，试着通过小额交易进入“游戏”（先不要担心是对是错，只要开始做了就行了） 小额交易通常能够导向大额交易——但是你必须先开始 钱可以迅速提升你的才智 44、 你不能光用眼睛看钱，钱是用你的头脑来看的 45、 实际上任何重要的东西都无法在教室里学到，必须要通过采取行动、犯错误然后改正错误来学习。这是智慧才有可能产生 46、 当某人对你说“你不能这样做”时，他可能正用一只手指着你——却又三只手指反过来指着他自己 47、 训练你用大脑看钱——掌握金融学 48、 学会识别真正的风险是什么（投资没有风险，没文化才有风险） 49、 坏建议是有风险的 50、 你的顾问，只能和你一样聪明 51、 你的利润是在你购买时——而不是卖出时产生的 52、 让别人欠你的债，并小心你欠别人的债 53、 如果你准备负债并承担风险，就要确信你能为此得到支付 54、 大多数人在财务方面努力挣扎，因为他们一生都在靠接受建议而不是根据事实来做出财务决策 55、 适度的勤勉：弄清楚什么是建议，什么是事实，然后做出决策 56、 如果你在玩捉人游戏，但20分钟后你还不知道替罪羊是谁，那么你就是替罪羊了 57、 钱是一种毒品，一旦开始沉溺于金钱，就很难再摆脱它 58、 永远不要为钱工作 59、 记住你最初的梦想，让它常驻心中，让热情的火焰继续燃烧。你任何时候都可以停下来，可为什么要现在停呢 60、 小规模地开始并利用好自己的时间，记住，经验比金钱更重要 61、 要想在右侧象限获得成功，需要5%用眼睛、95%用大脑来观察事物 62、 了解法律和市场对于财务成功至关重要，财富的巨大转移通常发生在法律和市场变化时 63、 不要总想迈出“一大步”，而要先学会迈出一小步。长期的财务成功不是以你步幅的大小来衡量的，而是用你前进的步数、方向和年数来衡量的。 64、 你如何吃掉一头大象?——每次吃一口 65、 行动胜于不行动 66、 要有长远的眼光和计划 67、 相信延迟的回报 68、 以有利于自己的方式运用复利的力量 69、 设定一个留有余地的目标，然后迫使自己坚持它 70、 设定可以完成的每日目标，这样，当你实现这个目标后，就会有一种积极的强化力量帮助你沿着通向远大目标的的道路不断前进 71、 如果想变富，你必须改变你的规则 72、 设定财务目标：（1）未来十二个月内（2）五年目标 73、 在你试图增加收入之前，最好量入为出 74、 人们在身处逆境时，喜欢责备别人 75、 把失望转化成资产而不是负债 76、 期待失望：在思想上准备好来面对你并不希望发生的意外；如果你保持平静，你就能更好地进行思考 77、 有导师帮忙 78、 善待你自己：如果你总为失望惩罚自己或者责备别人，那么你就很难学到任何新东西 79、 讲述事实：成功的大小是用渴望的强烈程度、梦想的大小以及处理失望的方式来衡量的 80、 能够发挥上帝赋予你的天赋的东西就是你的梦想、决心和相信自己拥有独一无二的天赋和才能的坚定信念 81、 人在情绪激动时会说出自己的真心话 82、 所有的话语都是镜子，是你有机会看见自己的灵魂 83、 心有多大舞台就有多大 84、 致富的关键在于，认识到金钱系统是不公平的，学习金钱系统的规则，并运用这些规则。这需要理财智慧，而理财智慧只能通过解决财务问题才能获得 85、 不轻言失败而放弃努力。想解决问题的专家寻求帮助 86、 不断学习，并且通过不断学习变得更加富有 87、 充满激情地生活 88、 五种基本的财商： a) 第一财商：赚更多的钱 b) 第二财商：守住你的钱 c) 第三财商：预算你的钱 d) 第四财商：撬起金钱的杠杆 e) 第五财商：改善你的财务信息 89、 答案关乎过去，而能力面向未来 90、 金钱影响了我们的生活水平、健康状况和受教育程度 91、 三种重要的智慧： a) 学术智慧（是我们读、写、做数学题以及处理计算机数据的能力） b) 专业智慧（用这种智慧来学会一项技能以及赚钱） c) 健康智慧（健康和财富密切相关） 92、 每个目标都有一个过程 93、 选择你的目标，然后选择你的过程。请永远记住，过程比目标更为重要 94、 理财智慧也是情感智慧。如果你无法控制你的感情，你就无法控制你的金钱 95、 意识到延迟满足感的重要性 96、 获胜时，你可以退出，但是千万不要因为失利就马上放弃 97、 真正的智慧是学习如何解决问题，目的是为了解决更大的问题 98、 企业家通过解决问题而不是创造问题来得到报酬 99、 在签署协议前考虑退出策略 100、 预算：为平衡资源和支出所做的计划 101、 提高收入而不是削减开支，是解决预算赤字的更好方法 102、 投资公司，关心公司的内在价值： a) 这个公司能赚更多的钱吗？ b) 这个公司有守住资产的制胜法宝吗？ c) 这个公司对金钱和资源做好预算了吗？ d) 这个公司有升值和发展的空间吗？ e) 这个公司的管理团队是聪明且知识渊博的吗？ 103、 内在价值是指： a) 法宝。即企业需要有一个核心竞争力——无论世道好坏都可以赚钱的东西。 b) 杠杆。 c) 可持续发展。 d) 可预测性。 104、 要改变你的生活先改变你的环境 105、 在现实生活中，执着和干劲比优秀的成绩更有价值 106、 财富是一个人思考能力的产品 107、 关于反馈的三件事情： a) 有勇气面对反馈 b) 只有在被询问时，才提供反馈或建议 c) 骗子会说你想听的，而不是你需要听的 108、 密切注意你自己的反馈，它正在为你提供世界上最重要的信息 109、 任何获得巨大财富的方法都需要付出代价，而且这个代价不一定能用金钱衡量 110、 大多数人想变得富有，但他们不想付出努力 111、 相比节俭的人，人们更喜欢慷慨的人 112、 安稳是有代价的：自由 没有了自由，生活中就没有梦想，许多人只是为了挣钱而生活 113、 安稳和自由可以兼得，只不过大多数人终其一生只肯为其中之一付出代价，而非两者兼顾 114、 我们甚至会为不付出代价而付出代价 115、 牛顿第三运动定律：力的作用是相互的，适用于任何事情，任何事情 116、 不要怕犯错，但要善于从错误中吸取经验","link":"/2022/05/30/Rich%20Dad%20Poor%20Dad/"},{"title":"《The Wisdom of Life》","text":"财富犹如海水，喝得越多越口渴。 -财富犹如海水，喝得越多越口渴。 -他人的看法确实与我们本身没有关系，完全可以置之不理。 -一个人首先是并确实是寄居在他自身的皮囊中，而不是存在于他人的看法中。 -美貌是一封公开的推荐信，让人更受青睐。 -幸福不是一件容易的事，它不会自发产生，也很难从他人那里获得。 -决定人命运的根本差别取决于以下三个内容： 1、人是什么：可以用“个性”一词来概括。 2、人有什么：即外在财产和一切占有物。 3、人在他人眼中是怎么样的。","link":"/2022/05/30/The%20Wisdom%20of%20Life/"},{"title":"《Walden; or, Life in the Woods》","text":"不容置疑的是，太阳升起时你正好在场，这才是最重要的。 Henry David Thoreau​ 1、不容置疑的是，太阳升起时你正好在场，这才是最重要的。 ​ 2、你寻找它就像一个梦，而一找到它，你就成了它的俘虏。 ​ 3、所谓听天由命无非就是一种习以为常的绝望。 ​ 4、太阳升，万物明。抛弃我们的偏见永远都不会太迟。 ​ 5、单独上路的人，今天想出发就可以出发；可是和别人结伴旅行的人，却必须等到另一个准备好，说不定要等很久才能成行。","link":"/2022/05/30/Walden;%20or,%20Life%20in%20the%20Woods/"},{"title":"《人间告白》","text":"我看万物像你，我看你像万物。 我看万物像你，我看你像万物。 我们每个人都害怕死亡，然而，比起害怕死亡，会不会觉得没有好好珍惜时光更为可怕？ 其实照顾癌症病人的人同样也是病人，他们也需要爱和拥抱，而不是日复一日地加油。 谁能比谁更坚强？谁又比谁更勇敢？只要爱的人还在，家属就有一直拼下去的勇气和动力。","link":"/2022/05/30/%E4%BA%BA%E9%97%B4%E5%91%8A%E7%99%BD%20%E3%80%90%E9%87%91%E9%B1%BC%E9%85%B1%E3%80%91/"},{"title":"What I learned today","text":"This is what I have learned today! Python网络爬虫思维导图Python网络爬虫技术 Python爬虫常用模块 Python正则表达式 数据持久化 2021.07.31检查文件是否存在，不存在则创建12345678import os if os.path.exists(path):​ print('exist')else: os.mkdir(path) ​ 2021.07.15比特币​ 一种数字货币，虚拟货币，有网名为松本聪的人提出。 ​ 优点：去中心化、交易过程双方个人信息保密 ​ 缺点：为不良交易行为提供方法途径，滋生犯罪。 ​ 产生：通过计算打包信息（区块）的头部来获取比特币及手续费奖励。规定每十分钟打包一次，前四年每次奖励50个比特币，第二个四年25个，以此类推每四年减少一半，计算下来大概一共有2100万个比特币。大概可以防止“通货膨胀”。 ​ 应用：每个人都有一个私有密匙和公开密匙加一个地址，当别人转给你钱的时候你只需要提供你的地址，当你需要转给别人钱的时候，你需要提供你的公开密匙和地址。当你发出交易信息，你需要将你的这条信息用hash算法（正着计算容易，倒着计算很难，需要一个一个代）转化为256位数字，再用你的私人密匙加密这条信息，同时将加密后的信息连带着你的公开密匙和地址一同发送给全网，接收到的人首先用你的公开密匙解密，再和hash计算你的交易信息后得出的256位数字进行比较，若一样则可证明是你本人发出的交易信息，这样就可以防伪。 ​ 最长链原则（新的区块优先连接到长的那一条区块链）防止双重支付（同时发送两条交易信息） ​ 区块链记录了所有交易信息，可以防止有人虚假交易。 AI与机器人​ AI是大脑，机器人是body。两者老实讲同样重要。 ​ 数据的重要性：目前机器学习需要大量的数据去训练模型。 ​ 讲座上提到智能康复机器人、智能建筑机器人。 ​ 智能康复机器人能协助病人做术后康复，发展要点是要做到结构柔软、能提供反馈（比如病人发生痉挛时能够及时地检测到）、趣味性更强，满足功能的前提下同时兼顾轻便的特点。 ​ 智能建筑机器人需要增加自重比，未来建房可以结构化、分块，最后运到指定地点组装成房屋。 2021.07.23​ 互联网能够做大做强的关键是四大法则：规模效应、协同效应、梅特卡夫效应和双边市场效应。 ​ 规模效应：通过一定的经济规模形成的产业链的完整性、资源配置与再生效率的提高带来的企业边际效益的增加 ​ 协同效应：协同效应就是指企业生产，营销，管理的不同环节，不同阶段，不同方面共同利用同一资源而产生的整体效应。或者是指并购后竞争力增强，导致净现金流量超过两家公司预期现金流量之和，又或合并后公司业绩比两个公司独立存在时的预期业绩高。 ​ 梅特卡夫效应：网络价值以用户数量平方速度增长 ​ 双边市场效应：指的就是淘宝、美团、滴滴那种，消费者之间互动很少，但供需双方互动强，供给者越多，产品成本越低，越能吸引消费者，从而正向循环。 2021.07.29沉没成本 ：以往发生的，但与当前决策无关的费用。 2021.08.28Pythonwordcloud review1234567import wordcloudc =wordcloud.WordCloud(font_path='simkai.ttf',width=1000,height=800,background_color='white')with open('公开信.txt', 'r', encoding='utf-8') as f: text = f.read() c.generate(text) c.to_file('letter.png') 思维轮元学习人为什么会讨厌学习？ 答案是：反馈回路的缺失 学习是本能，元学习，学习是本能，能够帮助识别万物，建模世界，做出理性而客观的选择。 算法分为生产算法和消费算法","link":"/2022/05/30/What%20I%20learned%20today/"},{"title":"《大学四年：我多想在入校前就知道的事》","text":"大学的可能性很多，如果早一点明确自己的目标，如果早一点进行规划，那么，你的大学绝对可以更精彩，而不是时时刻刻都处在迷茫状态。 第一章 认清自己，没人能给你标准答案在可预见的未来，人类都不可能参透生死和永恒， 但即便须臾，我还是想多知道一点，多看一点， 把世事里灰色区域的面积，尽量多挤压一点， 使之变得黑白分明，是非可见。 一、暂时明确你读大学的目的​ 大学的可能性很多，如果早一点明确自己的目标，如果早一点进行规划，那么，你的大学绝对可以更精彩，而不是时时刻刻都处在迷茫状态。 二、认清自己​ 过去的无法更改，未来的你还想要将就吗？ ​ 客观认识、认清自己，不妄自菲薄，也不妄自尊大。 三、停止抱怨四、尊重自己与他人的时间五、养成良好的作息规律六、寻找自己感兴趣的领域​ 多去尝试，一定要找到自己的能力和兴趣所在，你的兴趣决定了你以后做什么事情会开心。 七、多与优秀的人交往​ 你在光明中待久了，自然而然就不想再回到混沌的黑暗之中了。 努力学习究竟有什么意义​ 不负韶华 如何克服没有考上名牌大学的心理落差​ “物以类聚，人以群分”。 大学教会你的最重要的事是什么不要急着把时间用在将劳动力变现上不要盲目羡慕别人，适合自己的才是最好的千万不要自己感动自己，唯有时刻保持清醒，才能看清真正的价值在哪里在看一个人的成就之前，需要先看看他的起点有多高学会保持独立思考，保持学者的沉默与谦恭跳出所在的圈子，去认识更有意思、更酷的人培养执行力： ​ 早睡早起、去Coursera等网站学习世界名校的课程、多出去走走、 有哪些是读书学不来却很重要的素质一、期望值管理能力​ 快乐 = 现实（能力） - 期望 二、阈值自控意识​ 阈值太高就会很难得到满足，简单来说就是会“笑点太高” 三、应对主观时空扭曲的能力​ 防止感觉到“时间过得越来越快”。 ​ 我们必须走出熟悉的区域，熟悉的区域是时空黑洞，会不断加速消耗，吞噬你的时间。 注意沉没成本给自己营造良好的学习环境有哪些免费的学习网站和资源一、综合型在线学习网站 Coursera edX Udacity（IT类，趋向收费） 网易公开课 网易云课堂 学堂在线 中国大学MOOC iTunes U Khan Academy（适合入门） TED 均一教育平台（台湾） FutureLearn（英国） iversity（德国） School（日本最大MOOC网站） Open2Study(无明确开课和结束时间) Udemy(大部分不免费) OpenupEd(欧盟) MIT OpenCourseare 好大学在线（上交） 万门大学 ​","link":"/2022/05/30/%E5%A4%A7%E5%AD%A6%E5%9B%9B%E5%B9%B4%EF%BC%9A%E6%88%91%E5%A4%9A%E6%83%B3%E5%9C%A8%E5%85%A5%E6%A0%A1%E5%89%8D%E5%B0%B1%E7%9F%A5%E9%81%93%E7%9A%84%E4%BA%8B/"},{"title":"《我在未来等你》","text":"人们常常不知道，哪一次分别是最后一次相见。 ​ 人们常常不知道，哪一次分别是最后一次相见。 很多人就像浑浊的水，要经过一段时间的沉淀才能分清楚他们对自己的意义，而成长就是把所有的人生细节沉淀出不一样的味道。 最美不过少年时，不是少年美，而是回忆美。 很多事情你接受不了，只是暂时没想通而已。 如果你知道自己为了什么，任何一点因自己而起的改变都值得庆祝。 喜欢的人就跟喜欢的东西一样，看中了就要买，你一定要相信自己的眼光。你喜欢的东西，肯定是好东西，别人也一定会喜欢。你货比三家，肯定就被人买走了。 “虽然我知道我们现在走的路不是你想去的地方，但能和你一起走，我去哪儿都可以。” “凡是没有经历挫折，就不算真正努力过，只能算是顺流而下。凡是经历挫折，还能存在，挺立的样子就是努力后的风骨。” “不就是道歉嘛，你爸最不怕的就是道歉了。既然道歉能解决问题，那就别怕自己犯错误。你使劲儿犯，我使劲儿道。” 少年就是好，容易开心，也容易忘怀。但凡走了心，一切都有意义。 “人生总是奇妙的，一旦你努力去做一件事，如果结果不是你想象的那样，那么老天一定会给你一个更好的结果。” “我知道你爱我，我也知道你太爱我，所以很多话都堵在心口不知如何说。” “你以为的就真的是你以为的吗，多少人一直活在‘以为’当中。” 尽情地告别就像拿着一台相机，“咔嚓”一声，留下最好的回忆。 原来，很多事情当你彻底放弃、重新面对的时候，才有可能是新的转机和开始。 有时，当你说某个人变了，也许不是他变了，而是你们的关系本就没有到真正了解彼此的内心而已。 你知道异地恋为什么会容易分手吗？”“寂寞就容易劈腿呗。”“为什么会寂寞呢？”“因为没人陪呗，所以就想找个离自己近的人陪呗。”“你看我，每天忙到死，从来不会有寂寞的感觉，所以感到寂寞的人本身就是很空虚的。再说了，如果她在外地寂寞了，我就立刻坐火车过去。火车不行，我就坐飞机。” 如果我们一直在正确的路上行走，那不是我们的人生，那只是看起来正确的人生。很多时候，我们心怀遗憾，并不是当时我们做错了什么，而是我们没做什么。一个是为了朋友可以做一切，一个是不想为青春留下遗憾。这个世界上正确的事太多，但如果没有错，正确也就毫无意义。","link":"/2022/05/30/%E6%88%91%E5%9C%A8%E6%9C%AA%E6%9D%A5%E7%AD%89%E4%BD%A0/"},{"title":"《我曾走在崩溃的边缘》","text":"人要活到老学到老，人生任何阶段都是由进步空间的，你不努力，就没有未来。 1、 一个机构要给新人“三个台”：一个是舞台，一个是平台，一个是后台。 舞台：能够给他一片发挥自己才华的天地。 平台：平台就是舞台的叠加，也就是说他已经不再是一个舞台了，而是一个更加广阔的概念。 后台：事情可以变得更加简单。 2、 人要活到老学到老，人生任何阶段都是由进步空间的，你不努力，就没有未来。 3、 既要有竞争对手，又要有学习榜样。 4、 在艰难的环境中，你的成长速度反而会更快。 5、 人在关键时刻，是要知道后退的（退一步海阔天空）。 6、 人生要做好两种打算：一种是长远打算，你活到100岁会是怎样的，这样的岁月你怎么去安排；另一种是每一天的打算，这就意味着即使明天已经不再属于你，你也不会为今天所做的事情而后悔。其中两个最重要的因素：时间、身体。 7、 每个人个性不同，这种个性在潜移默化中塑造了一个人的状态，但也会阻碍一个人的发展。所以我们要做的就是努力发扬个性中好的一面，努力克服个性中坏的一面。 8、 队伍的年轻化永远是正确的选择。 9、 做人做事的核心要素：诚信、坦诚和信任. 10、 要排除琐碎的事情，最好的方法就是有一个博大、坦诚、诚恳的胸怀。 11、 表达的方式要讲究技巧，要让人感到非常舒服。 12、 与政府官员打交道：坚持原则（友情相交，而不是利益相交；要做没有后遗症（合理合法，清白）的事情。）做事情。 13、 中国从古代到今天就是一个人情社会，人情常常会大于规矩。在中国的不少场合，如果人情没到，关系就不会到。","link":"/2022/05/30/%E6%88%91%E6%9B%BE%E8%B5%B0%E5%9C%A8%E5%B4%A9%E6%BA%83%E7%9A%84%E8%BE%B9%E7%BC%98%EF%BC%88%E4%BF%9E%E6%95%8F%E6%B4%AA%EF%BC%89/"},{"title":"第一、二次创客说心得","text":"时刻关注热点，在创业中可战略性地调整方向、路线。 1、 学会主动请教在你所创业的领域中已经有所成就的前辈；2、 学会主动创造（抓住）机会：多参加一些活动；3、 时刻关注热点，在创业中可战略性地调整方向、路线；4、 核心竞争力不仅仅包括技术、资源、市场、人脉、流量，还可以是人；5、 考虑将公域流量转化为私域流量；6、 内容创作方向考虑变现方式；7、 考虑增加用户粘性；8、 培养（学习）企业家精神；9、 注意经验沉淀；10、 注意专注；11、 注意创新性（不可替代性） 第二次“创客说”心得笔记 遇到行业内卷，抢占市场，之后维护市场； 视频创作者，观众首先注意到视频的清晰度，然后是内容； Never Stop; 不怕失败，从失败中成长； 想了解某一行业、职业，最好的办法就是亲身体验一遍； 创业者较难平衡工作与生活； 初创，学会抱团取暖； 三条不同的路：先沉淀自己，后实践；先实现，后沉淀（总结经验）；实践的同时不断总结； 开始之前，明确市场（优先选择未饱和市场）、受众； 细心，防骗；","link":"/2022/05/30/%E7%AC%AC%E4%B8%80%E3%80%81%E4%BA%8C%E6%AC%A1%E2%80%9C%E5%88%9B%E5%AE%A2%E8%AF%B4%E2%80%9D%E5%BF%83%E5%BE%97/"},{"title":"《谁的青春不迷茫》","text":"人需要有一段好的感情、一份好的工作、一个能自得其乐的爱好。三个占一个，生活就能有盼头。 如果你在乎一个人，他活在你的生命里全是细节，而非感受；他活在你的世界里全是故事，而非评价。 人需要有一段好的感情、一份好的工作、一个能自得其乐的爱好。三个占一个，生活就能有盼头。 不是花钱买的，而是靠心做的，让人明白被懂得和理解的意义。 一句感谢不足以表达最真挚的内心。唯有大家都越来越好，才能连成一片星空。 其实害怕陌生人，不过是保护自己的一种方式。对于陌生人，多一点点善意，多一点点勇气，可能很多事情就会变得不一样吧。 当你决定很认真做任何一件事的时候，整个人的心情和状态都会因为那一件事而变得安静。人一旦变得安静，就能发现很多以往不易察觉的问题，能解决很多以往常常会忽略的事。 你不成为敢解决问题的人，你就会变成别人的问题。 不是因为我可以，而是因为这个人值得。 你此刻的贫穷限制了你对未来的想象，这里的贫穷不仅仅指金钱，也指精神。你对自身的自卑、眼界的狭隘，让你看不到更远。 既然仰头看不到未来，那就低头走好眼前的路。 敢用人格为另外一个人担保时，这两个人都是可以完全信任的。 当一个人很真诚的为其他人推荐某件东西时，被推荐的东西一定是值得花时间的，哪怕也许最后你说了一句“有点儿无聊”。 每个人都要看得起自己，不要以为自己无足轻重就放任自己做一件事、说一些话，其实你所做的任何事情都可能对你周围的人造成一生不可弥补的破坏。 有时候，你没那么重要，轻一点儿，或许活得更好。 生活是由细节组成的，而不是时间。 所有同甘的人都被认为理所当然，偶尔共苦就被认为是命中注定。 一切美好的，地下都堆积着大量残骸。一切矮小的，踩着的都是巨人的肩膀。 没有什么事情是非得别人和你一起做不可的，总有身陷绝境的时候，能鼓励你的人只有你自己。 正因为是你，我才会这样要求你。 了解与否并不重要，重要的是你是否认真观察过一些东西。 如果我爱谁，我们一定会乘火车去很远的地方，一路都是风景，包括思考时呈现出来的风景。 爱情本来并不复杂，来来去去不过三个字，不是“我爱你”“我恨你”，便是“算了吧”“你好吗”“对不起”。 生命的意义不在于人健壮时有多么辉煌，而是在他逐渐凋落时，有明白她的人在一旁静静地陪着她待着，不言，不语，屏息中交换生命的本真，任凭四周的嘈杂与纠纷。 时间是我们正在服下的毒药，也是未来我们的解药。时间可以改变一切你认为改变不了的。 记录，是一个拯救生命的决定。 美丽最少年，美丽了年华，颓废了脸颊。 相似的人可以一同欢愉，互补的人才能适合相伴到老。 丢了任何东西，都不能丢了记忆，坏的好的，都是财富。 没有人会一直正确，他们只会越来越正确。 我理性地知道自己什么时候该感性，而在感性的时候理性地知道自己的底线在哪儿。 如果你想找到改变你一生的人，看一眼镜子就能找到。 苦不是一件坏事，它会让你未来的甜更甜。 我们之所以战斗，不是为了改变世界，而是为了不让世界改变我们。 所有少年相约的承诺，在未知命运前都只是当下的安慰。 有些人，有些事，一时错过，就是一世。 苦等的幸福，就在于你说一句：我没事。 人生如寄，不过如此。 人生如戏，你的一生中，若要精彩，总得靠自己去碰几个配戏的好演员。 爱上一个认真的消遣，不过用了一朵花开的时间；遇见一场烟火的表演，只不过用了一场轮回的时间。 感情不能假手于人，中间一旦掺杂了等价交换物，也许最后记得的只是等价物了。 看上了就追，相中了就买，宁肯做一个草率的决定，也不要一直后悔地回忆。 今天永远对明天充满幻想，才有坚定的信念活到后天。 被否定和被质疑，目的不是让你放弃一件事，而是让你更明白自己要做的事情究竟是什么。 人心中都有自己的早晨，时候到了，人会自己醒来。 风刮到头是一场风的空。 放弃一个自己不理解的，就能有更多的时间遇见那些适合自己的。 被人理解是幸运的，但不被理解未必不幸。 成功是一出太过诱惑的大戏，他踮起脚尖奋力触摸，掩耳盗铃般却全心全意。 把彼此安置于安全位置，小心保护，无论外在撞击几何，时光凝成最坚硬的核，野火烧不尽，春风吹又生。 从出生那一刻我们就还像是一粒沙，随风飘散，相聚又离开，只是为了看看这世间的一些真美好，希望你越走越远。 每个想要飞起来的人都应该爱惜自己的羽毛。 当你得到越来越多的时候，你就开始担心失去的也会越来越多。 尽力而为，则万事无憾。 日记，无论再矫情再幼稚再做作，那都是一个真真实实的我们，没有什么好鄙视的，更不用一个激动就按了删除键。要知道，你删除的并不是幼稚，而是一段青春好看的风景。这些风景或许你现在随处可见，但这些风景在未来你的世界很难寻。 回忆是什么？丈量一段旧时光，裁成合适的摸样，你看不见我过往的不堪，我看得见自己内心的坦荡。 了解是什么？是知道对方心底最深的痛处，痛在哪里。 以前的迷茫是大雾弥漫，今日的迷茫虽仍有雾气，却有阳光穿透，满是暖意。","link":"/2022/05/30/%E8%B0%81%E7%9A%84%E9%9D%92%E6%98%A5%E4%B8%8D%E8%BF%B7%E8%8C%AB/"},{"title":"《Make It Stick The Science of Successful Learning》","text":"万千智慧始于记忆。 导语：万千智慧始于记忆。 CONTENTS1、 学习是挑战天性的必修课 2、 学习的本质：知识链和记忆结 3、 “后刻意练习”时代的到来 4、 知识的“滚雪球”效应（你知道的却多，就越有可能为新知识建立联系。） 5、 打造适合自己的心智模型（效率的本质取决于我们领悟世界的能力，以及衡量自己表现的能力。） 6、 选择适合自己的学习风格 7、 终生学习者基本的基本（你做的事情越多，你能做的事情也就越多。） 8、 写给大家的学习策略 推荐序一： 轻松的学习是无效的 推荐序二： 学习不止技巧 人们在学习一个概念的时候，花费越多的心思，尝试用自己的话语去演绎它，或者是尝试理解这个概念在不同语境下的不同意义，就能越牢固地掌握这个概念。 举例：如何背单词？ ​ 应用“生成性学习”方式：在生活中学习单词，通过互联网查找相关内容，用新单词造句以加深理解，写博客，记录有意思的单词（生活中的每一个细节也可以成为你的单词书） 前言： 如果学习者能把研究一个问题的时间分散开，并阶段性的回顾这个问题，那么他们就能记得更牢； 如果他们可以穿插研究不同的问题，那么效果就比一次研究一个主题要好 学习是挑战天性的必修课1、要让学到的知识与技能在脑子里随时待命，这样你才能在以后遇到问题时，思路清晰，并抓住解决问题的机会。 2、 想要学以致用，就必须记忆。 3、 坚持不懈地学习并记忆，终生不怠。（擅长学习的人会终生受益） 天性懒惰孕育了认知规律和心智模型 1、 耗费心血的学习才是深层次的，效果也更持久。 2、 与反复阅读这种复习方法相比，回想事实、概念或事件会更有效。（检索式练习）：检索会强化记忆，并阻止遗忘。（在学习新知识后用一道简单的小问题考考自己，就可以巩固所学，强化记忆）。 3、 定期练习可以防止遗忘，强化检索路径。 4、 在别人教给你答案前，先尝试自己解决问题。 5、 运用全部才能和智谋学习时，效果要超过只用最熟悉的风格学习。 6、 穿插练习和多样化练习可以让你更好地掌握提炼技能，更有希望在陌生的情境中找到正确答案。 7、 测验可以帮助我们判断自己学到了什么，发现并巩固自己的薄弱环节，从而更好地掌握知识。 8、 不管学习什么新知识，都需要有已知作为基础。 9、 牢固的掌握新知识方法：细化知识：用自己的语言把新知识表达出来，把它和已知联系起来。 10、 把新知识放到更广泛的情景中有助于学习。 11、 更好地掌握复杂的知识：从新知识中提取关键概念，并把这些概念组织成一个心智模型，同时把这种心智模型与已知联系起来。 12、 影响智力水平的因素在很大程度上是由你本人控制的。 科学“照妖镜”下的学习方法 1、 正是感觉到学习更吃力时，记忆才更为长久，牢固。 2、 反复阅读有三大不足：浪费时间、无法产生持久的记忆、产生一种自己已经掌握了内容的错觉。 3、 主动参与，可以产生持久记忆。 4、 在初次阅读后，隔一段时间再次阅读是有意义的；但是连续多次阅读只是空耗时间。 元认知（我们对知识掌握情况的理解）带来的学习假象——自以为掌握了所学 1、 有些事是已知的已知：有些事情我们知道自己知道。 2、 有些事情是已知的未知：有些事情我们知道自己不知道。 3、 但还有些事情是未知的未知：有些事情我们不知道自己不知道。 4、 那些不给自己出题的学生，容易过高地估计自己对学习资料的掌握程度。 知识多不等于学习能力强1、 对要用到的东西一无所知，是不配谈实用的。 2、 只有在练习新技能的同时付出努力、展开思考，并在心里演练，成果才会显现。 考试是最有效的学习策略之一1、 把考试当成一种学习工具 2、 主动检索——考试——可以强化记忆，有效地终止遗忘。而且检索花费的心思越多，受益就越多。 第一节小结1、 进行有规律的自测，重新校准自己知道什么，不知道什么。 2、 各种形式的检索练习。 二、学习的本质：知识链和记忆结知识最终将变成条件反射1、 人们可以通过反思自己的经历获得一种必不可少的知识。（反思会涉及多种认知活动，这些活动可以带来更好的学习效果） 自我检测：给知识链打上记忆结1、 改善我们学习方法的一大挑战就在于找到办法中断遗忘的过程。 2、 重复多次检索，并且检索之间要有间隔。 只需一次自测，一周后回忆率从28%跃迁为39%1、 填字母完善单词就能让实验对象更好地记住这个词。 1、 与单单进行测验相比，给出反馈更能增强记忆。稍微把反馈延迟一段时间会产生比立刻。反馈更好的长期学习效果（反馈会频繁打断学习者建立稳定的表现模式）。 2、 有时间间隔的练习会改善记忆。 3、 反复检索可以加强建议。 4、 测验比重复阅读更能将知识学以致用。 5、 在一次测验后，学生会花更多时间重新学习那些生疏的资料。 6、 低权重的课上测验好处： 1） 强化学习与记忆效果 2） 改善学生出勤情况 3） 能让学生预习 4） 增加学生们在课上的注意力 5） 让学生们更好地了解自己知道什么，哪里需要加把力气温习 6） 可以解决误把重复阅读产生的流利感当成精通知识的问题 第二节小结1、 练习从记忆中检索新知识或新技能是有效的学习工具，也是保持长久记忆的有力武器。 2、 努力检索有助于人们获得更好的学习效果，产生更持久的记忆。 3、 反复检索不仅能让记忆更持久，还能让知识在多变的环境中更容易被检索，而且可以解决更多的问题。 4、 和检索练习相比，反复阅读会遗忘更多东西。检索练习的好处是长期的。 5、 只需在一堂课上加入一次测验（检索练习），就能极大地改善学生期末考试的分数。 6、 测验不是非要授课者发起。学生可以随时随地练习检索，并不是非要在课堂上做小测验。 7、 和那些只是重复阅读资料的学生相比，参与测验的学生更了解自己的学习进展。 8、 在测验后向学生提供纠正反馈，可以避免它们记住错误的东西，让他们更好的学习正确的答案。 9、 在课上引入无关紧要的小测验会让学生们接受这种练习。 三、“后刻意练习”时代的到来频繁的集中练习只会产生短期记忆1、 只有当练习被分散安排在有间隔的培训里的时候，才更为有效。 2、 获得“更为有效”的代价：当练习有间隔、与其他内容有穿插且多样化的时候，你花费的努力也就越多。但正是花费了更多的心血，学习成果才变得更牢固。 间隔练习使知识存储的更加牢固向长期记忆中存放新知识需要有一个巩固的过程。 穿插练习有助于长期记忆在练习中插入两个以上的主题或技能，胜过集中练习。 多样化练习促进知识的活学活用1、 进行不同种类的练习会使用大脑的不同区域。 2、 通过难度较低、集中式的练习学到的东西，被变成了一个更简单、相对来说更直白的心理表征。 3、 多样化、难度更高的练习需要耗费更多脑力，通过这种方式学到的东西会被大脑编成更灵活的表征，适用范围也会更广。、即使是与学习运动技能相对的认知性学习，也可以从多样化练习中受益。 善用练习组合，带来成长性思维穿插练习和多样化练习显著优点：有助于我们更好地学习如何评估背景、辨识问题间的差异、从一系列可选的答案中选择并应用正确的解决方案。 知识是平面的，复合型知识是立体的1、 最有效的检索练习，是那些可以反映出你今后如何运用自己知识的检索练习。 2、 “把训练当成比赛，才能把比赛当成训练。” 3、 实践经验的重要性。 关于练习的几条普适性原则检索、有间隔、有穿插的练习、反思，以及细化。 第三节小结1、 有间隔的练习、有穿插内容的练习，以及多样化练习 2、 把学习与练习间隔开来分期进行，能让学习成果更加显著、记忆更加牢固，能有效地形成习惯优势。 3、 两次练习间至少间隔一天。 4、 掌握得越好的东西，就越不用经常练习。 5、 多样化练习有助于学习者树立更开阔的心理模式。 6、 培养反思的习惯：发生了什么？我是怎么做的？怎样才能有用？下次我要采取别的什么方法？ 四：知识的“滚雪球”效应学习的三个关键步骤：1） 编码：大脑把感官感知到的东西转化为有意义的心理表征，是短期记忆。 2） 巩固：把这类心理表征强化为长期记忆的过程。睡眠似乎有助于巩固记忆。 3） 检索： 一、 把短期记忆重新编码并巩固成长期记忆的时候，我们必须把这项工作做扎实。 二、必须把这些资料与不同种类的线索联系起来，以便我们今后回忆这些知识时能够游刃有余。 欲求新知，先忘旧事1、 你的知识在头脑中准备得是否充分，是否能为你所用，取决于环境，取决于近来是否用过，取决于关联这些知识的线索是否够多、是否形象，以及你是否能利用这些线索及时将知识调取出来。 2、 在日常生活中，你经常需要忘记一些矛盾的、与旧记忆相关的记忆线索，这样才能把记忆线索和新知识联系起来。 3、 学习新知识有时候就是要忘掉一些东西。 4、 在学新东西的时候，日常生活中已经根深蒂固的大部分事务不会从长期记忆中消失。 5、 你停止使用记忆线索，或是给他们安排新用途，只是为了不用轻易回忆起它们。 6、 背景可以激发记忆，想打开一把旧锁，就需要正确的钥匙。（所见所闻可以唤醒很多回忆，哪怕是许久不曾想起的陈年旧事） 越容易想起，却不容易记住1、 检索联系是强化所学的一种方法。 2、 在检索知识或技能上花费的努力却多，检索练习就越能深化这种记忆。 3、 让练习增加一些难度，，让人们做更多的努力，启用有间隔的、穿插安排的、多样化的练习，让表面上的成果来的来得慢一些。 4、 我们在判断什么学习方法最适合自己的时候，通常会做出错误的决定，会受到自以为精通的错觉的影响。 5、 关于一件事情你忘记的越多，重新学习就更为有效，能更好地形成永久性的知识。 学习中必须要做哪些“努力”1、 重新巩固记忆： 这种做法需要你用新的方式“重新下载”或重新构建长期记忆中的技能或资料的组成元素，而不是漫无目的地在短期记忆中重复他们。 2、 打造心智模型（被牢牢记住并熟练运用的技能或知识结构）： 1、下足了功夫练习，就会使彼此相关的复杂理论或是连续的运动技能融合为一个有意义的整体。 2、积累大量类似的心智模型，从而保证自己在特定环境下做出正确分析，立刻挑选出正确的应对方案并加以执行。 3、举一反三： ​ 在不同时机、不同环境下多次进行检索练习，其间穿插不同的学习资料，这样做有助于给这些资料建立新的练习。 4、 构建概念学习： 随即接触不同的样例 5、 学习迁移： 1、 所谓学习迁移，是指在新环境下运用所学的一种能力。 2、 需要用有间隔、有穿插、多样化的方式进行检索联系。 6、 做好学习的心理准备： 在没有答案的情况下自行寻找解决方案，你就能更好地理解答案，也能把它记得更牢。 这些“良性干扰”能提升学习效果1、 当页面文本稍有模糊，或是字体略微有些难以辨认时，人们能更好地回忆起文章的内容。 2、 当教学大纲的编排顺序不同于课本内容时，可以让学生更好地会议课程内容。 3、 当一段文字中有单词缺少字母，需要读者自行补齐时，阅读速度就会放慢，但记忆会更牢固。 4、 改变正常的表达形式会带来困难——干扰了学习的流畅性——但这种困难会让学习者更努力地构建一种合理的解读。多下的那番功夫加强了人们对资料的理解与学习效果。 （但不能难得离谱） 5、“生成”：尝试解答一道题目或是解决一个问题，而不是坐等信息或解决方案的出现。 6、在测验中，想出一个答案要比从多个选项中选择一个答案更有利于学习。强迫自己写一篇短文还会让资料被记得更牢固。 7、解决一个问题总要好过记住一个问题的答案。尝试一种解决方法但是的除了错误的答案，也是要好于不去尝试的。 8、“反思”：花几分钟，复习一下从一段经历中学到了什么，在拿一些问题考考自己。 9、在完成一节课或一次阅读作业后，问问自己：课程的核心思想是什么？哪些是相关的例子？如何把这些内容和我的已知联系起来？ 10、在练习过新知识或新技能后，问问自己：哪些是做的好的？哪些还可以做得更好？要想进一步精通，我需要做些什么，或者下次我用什么方法可以获得更好的结果？ 11、“以写促学”的反思形式：让学生写一篇作文，来反思最后一堂课的主旨。 化解因失败带来的焦虑感1、给学生们留出苦思难题的空间，这样他们会表现得更好。 2、通往精通的道路上，失败是必不可少的经历。在失败面前坚持不懈才是成功的关键。 创造性源于不设限的学习别在无法克服的困难上浪费时间第四节小结1、学习过程至少分三步： 1、 对短期工作记忆中信息的编码。 2、 巩固 3、 检索 2、学习总是建立在已知基础之上。 3、长期记忆的容量基本上是无限的。 4、阶段性地检索所学，有助于强化记忆之间的联系，也能强化回忆知识的线索，同时还能弱化连通冲突记忆的路径。练习难度越大，收效才越大。 五、打造适合自己的心智模型1、效率的本质取决于我们领悟周围世界的能力，以及衡量自己表现的能力。 2、元认知：对自身思维的审视。对那些迷惑我们的方法保持警觉。 没头脑的机制1和爱自省的机制21、 在所有你想要有所建树的领域中，想要提高自己的胜任能力，，一个重要方法就是学习何时该信任直觉，何时该质疑直觉。 学习时避免错觉和记忆扭曲1、 记忆是重构出来的，我们没办法记住一件事情的方方面面，所以我们只记住其中对自己的情感影响最大的元素，剩余的空白部分则由自己用细节来填补，但这些细节有可能是错误的。 2、 暗示的东西才会被人们记住，而明确表述的东西则不然。 3、 经过生动想象的虚构事件，可以和真实事件一样牢牢地留在记忆当中。 4、 暗示会引起记忆错觉。 5、 来自其他事件的干扰也可以歪曲记忆。 6、 那些听起来很耳熟的描述会让人产生知晓感，也会让人把它们和真实情况混淆。 7、 倾向于把流畅阅读文字误认为是充分掌握了文字的内容，就会导致流畅错觉。 8、 我们的记忆还被社会影响所左右，而且会与周围人的记忆趋同。 9、 对一段记忆充满自信，并不代表这段记忆肯定时准确的。 打造适合自己的心智模型你无法从不擅长的事情里学到知识：1、不能胜任某项工作的人会过高的估计自己的能力，而且感觉不到自己的表现与实际要求之间的差距，觉得没有必要试着改进。 2、我们容易受到错觉与误判的影响，因此做事时要缓一缓。 3、避开错误和误判的方法是，用一组自身之外的客观标准，来替代用作决策参考的主观经验。 实践和测验才能暴露学习漏洞在很多领域中，和更有经验的同伴工作，可以校准一个人的判断和学习。 六、选择适合自己的学习风格主动学习能制造掌控感评估学习风格6个方面： ​ 环境、情绪、社会性、感知、生理，以及心理。 其他维度： ​ 感知的风格是抽象的还是具体的。 ​ 处理事情的模式是主动探索还是反身观察。 ​ 安排事情的风格是随机性较强还是比较有条理。 你是分析型、创新型，还是实践型思维？学不好的领域暴露了你的能力结构动态测验三步骤： 1、 进行某种类型的测验——可以是一段经历，也可以是一次笔试——看到自己的知识或技能在哪些方面有所欠缺。 2、 我决心运用反思、练习、间隔练习或其他有效的学习方法提高自己的能力。 3、 再次测验自己，留意哪些方面已有改善，同时特别注意在哪些地方还需要下功夫。 用搭积木的方法构建知识有人喜欢看说明书，有人喜欢动手试错小结1、 想要达到精通，就需要探索。 2、 开阔眼界，别局限在自己喜欢的那套学习风格中，要运用你的资源，发挥你的全部“智力”，把你想掌握的知识或技能练得滚瓜烂熟。说出你想要知道、做到、成就的事情，然后列出需要的能力、需要学习的东西，以及从哪里可以找到这些知识和技能，再放手去做。 3、 巩固自己的长处。 4、 用测验、考验、试错等手段不断提高自己的知识水平，在不足的地方加以弥补。 5、 采取主动的学习方法，例如检索练习、有间隔的练习和穿插练习。要有进取心。 6、 不要只靠感觉做事。 7、 提取基本原则，构建结构。 8、 把你的想法或想要得到的能力分解成各个组成部分。 综合： 1、提取重要的规则，将它们整合到结构之中。 2、 成功在很大程度上取决于专注与自律。 3、 大脑并非一成不变，而是可塑、可变的，能够凭借每一次新任务重新组织自己。 4、 心智能力的高低是由神经连接发展的强弱决定的。 5、 智商是基因与环境共同作用的产物。（环境：教育、文化、营养、社会经济地位；）（教育：早期教育；营养：脂肪酸）(经常同孩子热心地聊天、提开放性的问题) 6、 脑力训练可以提升学习自信。 7、 智商分为流体智力和晶体智力。 8、 科学理论，根基就在于实证研究成果的可复现性。 9、 放大智力水平：抱有一种成长心态；像专家那样练习；建立记忆线索。 10、 想要终身成长，请像专家那样思考。 11、 只是信念，就可以在很大程度上影响学习成绩。 12、 追求成绩的人会在无意识中限制自己的潜力。 13、 医学系为目的激发的思路和行动，与以成绩为目标激发的完全不一样。 14、 强调天生的智力则是将它至于孩子的控制之外，对于应对失败来说没有好处。 15、 取得成功更多的是靠勇气、好奇心与坚持，而不是智商。 16、 失败让人获得有用的信息，也让人有机会发现自己在竭尽全力时能做到什么。 17、 自律、勇气及成长心态这些素质才让人敢想敢做，具备创造力与毅力。 18、 提高能力的动力在很大程度上是由你自己掌控的。 19、 学习执行力比学习技巧更重要。 20、 独立思考：让我们能够透过表象看本质，培养更强的洞察力，这样提出的观点才会具有说服力，不流于表面。 21、 数千小时积累的可以练习只能成为一个领域的专家。 22、 专家级的表现需要高质量的练习，而不是靠遗传因素。 掌握几个适合自己的记忆方法1、 记忆宫殿：用来组织并记忆大量资料。 方法：将心中的形象与一系列实体位置联系起来，建立记忆线索。 原理：视觉形象可以建立到记忆的联系，而这些联系是生动的、有关联的。（人类记忆图片的效果好于记忆单词） 2、助记工具：格律 字钩法就是用来记住一系列事情的格律。 3、用韵脚让形象具体化：押韵的那个形象保持不变，但每次联想的东西都可以变化，这样你就可以记忆新东西了。 4、熟悉的歌曲也是一种助记手段：把每小节的歌词和一个形象联系起来，同时这个形象又是检索有用记忆的线索，就能起到帮助记忆的作用。 5、助记手段形式：编号、线路、平面图、歌曲、诗词、谚语、缩写。 共同点：形式的结构都为人熟知且其中的元素可以轻松联系到需要记忆的目标信息上。 6、记忆宫殿是学习工具，也是一种将所学组织起来，以便在写文章的时候随时检索。 7、要达到精通，就需要独自一人努力很长一段时间。 8、细化（在新资料中找到其他层面的含义的过程）（为新资料提供一种比喻或视觉形象）会提高你对新资料的掌握程度。 1、 生成（在得到答案或解决方案之前，尝试回答疑问或解决问题）的效果是让意识更容易接受新学问。 2、 高材生学习习惯列表： a) 课前一定要阅读资料 b) 在阅读的时候预想考试会出什么题目，以及这些题目要如何作答 c) 课上在心里回答这些假设问题，从而测验阅读内容的记忆成果 d) 复习学习指南，找到那些回忆不出或不知道的术语，重新学习 e) 在阅读笔记中抄写标粗的术语及定义，确保能够理解 f) 参加教授在网上发布的模拟测验，从中发现不知道的概念，重点学习 g) 用自己的方式把课上的信息重新组织成一份学习指南 h) 写出复杂或重要的概念贴在床头，不时自测 i) 在整个学习过程中，把复习和练习间隔开 3、","link":"/2022/05/30/Make_It_Stick_The_Science_of%20Successful%20Learning/"},{"title":"Reading Notes","text":"人生如梦 灵犀一动 不觉惊跃 如魇得醒 ​ 既是一阵凉风 吹走了最后那一丝温暖 ​ 也是一阵清风 吹散了最后那一片雾霾 ​ ———黄文杰 ​ 忠于热爱 终见花开 ​ 一花一世界，一歌一物人 ​ 人生如梦 灵犀一动 不觉惊跃 如魇得醒 ​ 外貌美只能取悦一时 心灵美才能经久不衰 ​ 失去才会懂得 懂得才会珍惜 ​ ​ 岁末回眸 ​ 当一个人敢用人格为另外一个人担保时，这两个人都是可以完全信任的， ​ 当一个人很真诚地为其他人推荐某件东西时，被推若的东西一定是值得花时间的，哪怕也许最后你说了一句“有点儿无聊。每个人都要看得起自己，不要以为自己无足轻重就放任自己做一些事、说一些话，其实你所做的任向事情郡可能对你周围的人造成一生不可弥补的破坏。。 ​ 有时候，你没那么重要，轻一点儿，或许话得、生活是由细节组成的，而不是时间所有同甘的人都被认为理所当然偶尔共若就被认为是命中注定， ​ 昂首阔步，穿过黑夜，拥抱初阳！ ​ 能遇到她，是你的幸运，一定不要让幸运溜走，把幸运留在身边，好好对待她，幸运就会变成幸福。 ​ 真正懂演技的人其实都很清楚，有一种奖项叫做，如果你不把奖发给他，那不代表他不行，而是代表你这个奖项不行。 ​ 人生中有很多快乐都是后知后觉的，身在其中的时候可能觉得没什么，但过了很久很久之后，很多事情不可能再回到当时那样的模式了，就会开始怀念，觉得人生中能有那样一段日子真的很棒，普通人无法同时拥有青春和对青春的感受。 所有的事业和梦想，最终都落足于家庭幸福上。做的出色了，让很多很多人的家庭幸福。做的一般般，也要保证自家的幸福。所以，那不是小家子气。","link":"/2022/07/01/Reading-Notes/"},{"title":"大道","text":"人生如梦，灵犀一动，不觉惊跃，如魇得醒。 引子 人生如梦，灵犀一动，不觉惊跃，如魇得醒。 十岁之前，光阴是一条小溪的缓缓流淌，慢得好像一辈子都长不大，看不到远处的风光。 二十岁之后，根本不在意光阴的流逝，快慢随意，多看一眼都算闲得慌。 三十岁之后，时间开始撒腿狂奔，拽得行人措手不及。 四十岁之后，像那即将入海的滚滚江河。 六十岁以后，又是骤然一变，静谧的湖泊，静止不动。 临终之际，宛如一条瀑布骤然跌落深潭。 缘起 肩挑着美好的世界 此山月色迷人，最能勾留人心。 明月当空，月光满人间，恍如琉璃世界，夜气清新，风过衣袂凉爽，此时情绪此时天，忙里偷闲即神仙。 岸边桃花千百树，红云一片，间有白桃数株，花开如少女可爱。 黄鹂颜色已可爱，添得叶底三五声。 二月二，龙抬头。 斗指正东，角宿初露，物换春回，为万物生发之象，鸟兽生角，草木甲坼，春耕农事由此开始。 就这样，又一天，白云走上青山头，来了又走。 仙草山中，杏花桃花里，笛声悠悠喊来满天月色。 天地霜尽，春山如笑，群峰之巅，长风浩荡。 山温水软，杨柳依依，草长莺飞，春暖花开。 渡船下边，大地山川，青青河畔草，绵绵思远道。 山上层层桃李花，层层又叠叠，云下烟火是人家，家家连户户。 旧山河新气象，年年岁岁又新年，共欢同乐，嘉庆与时新。 天高地阔，云宽土厚水长，美不胜收。 故乡的青山白云，小桥流水，在等着远方的游子回家。 好像天一亮，梦醒时，就会“睁眼看到”卖花声四起。 薄暮远岫茫茫山，细雨微风淡淡云。 自家数峰清瘦出云来。 清晨时分，月落日升，气候清新。 如人夜行，披星戴月，已得天明。 天边晚霞似锦，老天爷倒是不小气，就这样送给了人间，从不要钱。 水陆草木之花，可爱者甚蕃。不蔓不枝，亭亭净植。出淤泥而不染是也。 蹇驴破帽旧衣，青山绿水老路，朝露晚霞星河，灯火花瓯佳人。 继续看风景去，天地之间有大美，等我千万年，不可辜负。 黄昏之中，江畔石崖，清风拂面。今夜应该还会是那明月在天。 头顶也星河，脚下也星河，天上天下皆有无声大美。 星月皎洁，明河在天，四无人声，声在树间。 老夫子看着这一幕，怎么说呢，就像在欣赏一幅世间最清新温馨的画卷，春风对杨柳，青山对绿水。 有句诗词写得好，金风玉露一相逢，胜却人间无数。 朝霞散彩羞衣架。 月下打瀑，一挂彩虹。 山水亭山水亭，山嶙嶙水潺潺。 河清海晏，时和岁丰。 山风和煦，旭日东升。 有层层叠叠的瀑布群，在雨后挂起小小的彩虹，少年好像伸手一搂，就能带回家珍藏起来。有千万飞鸟聚集的陡峭山崖，一粒粒串在一起，像是挂在墙壁上的雪白帘子。 某些人和事，哪怕是路边的风景，可是只要看一眼，依然会让人觉得很美好。 青青槐荫，皎皎月光。春风一披拂，百卉各争妍。 既冬日可爱，又如沐春风。 此生此身在此时此地见此景，心不可得。 薄暮远岫茫茫山，细雨微风淡淡云。自家数峰清瘦出云来。 花月正春风。 昨夜梦魂中，花月正春风。 杨柳依依，草场莺飞。 良辰美景满树花，故人重逢俩无恙。 道之大原出于天，天不变，道亦不变，披星戴月，人间大美，此行走好，平平安安。 少年 青山绿水千万重，翩翩少年思无邪 有道理，这句话，说到我心坎里去嘞。哈，这么好的道理，我要关起门来，跟它好好相处，可不能让它偷偷溜走哩。 她趴在廊道里边，双手托着腮帮，仔细数着崖外过路的白云，今儿雾大云就胖，一大坨呢，嗯，就是云海。 她蹦跳着跨上台阶，满脸喜悦，两条疏淡微黄的眉毛上边，就像两条小长凳，并排坐满了出门晒太阳的的小人儿，不是亲戚就是街坊邻居，开心，高兴，欢喜，愉快，雀跃…… 她，“哈！” 他，“哈哈。” 她，“哈哈哈！” 他，“你赢了。” 就像她经常一个人在落魄山崖畔看风景，不开心的事儿，就随云飘走吧，开心的，如鸟雀停枝头，留下做客吧。 只有少年才会一门心思想着白发显老亦无妨。 少年远游，仿佛背过烈日，总是满肩月光。 好像少年们的每个今天，一双眼睛总是望向前方，憧憬着明天，希冀着后天。 好像所有的过往，都可以全部统称为昨天。 梦回少年丛中，吾亦是少年。 金鞭美少年，去跃青骢马，当时春衫薄，杏花吹满头 她一挑眉头，看似是有些烦那人的唠叨不停，实则她那双天底下最好看的眉眼里，全是微微漾开的开心、喜悦和骄傲。 就像那春风微微吹皱的湖水涟漪。 世间苦难临头，我们敢怒敢言。 小姑娘的心情，是那天上的云。 人心中须有日月。 孩子小小的忧伤，往往如风似雾。 少年心思，清澈见底。 小姑娘听过京城上空悠扬的鸽哨声，小姑娘看过摇摇晃晃的漂亮纸鸢，小姑娘吃过觉得天底下最好吃的馄饨，小姑娘在屋檐下躲过雨，在树底下躲着大太阳，在风雪里呵气取暖而行…… 一个肩上有杨柳依依，一个肩上有草长莺飞，一个肩上有清风明月，多好，我一想到这个，我就会开心，很开心。 窗外的阳光溜进了屋子，像一群不爱说笑的稚童，累了后，然后它们便懒洋洋趴在桌上，地上，少年的肩头。 初春的山风依旧凛冽，吹拂得少年鬓角发丝肆意飞扬。 少年肩头挑着草长莺飞。 少年的肩膀，就该这样才对嘛，什么家国仇恨，浩然正气的，都不要急，先挑起清风明月、杨柳依依和草长莺飞，少年郎的肩头，本就应当满是美好的事物啊。 有些人心如花木，皆向阳而生。 棉袄小姑娘在陈平安回到树下的时候，满脸隐忧，陈平安灿烂一笑，揉了揉她的小脑袋，轻声说没事了。小姑娘脸色呼啦一下蓦然灿烂起来，如一抹令人意外的雨后彩虹，干净得让人心颤。 你托付他一事，千难万难，哪怕明知道少年到最后，拼尽全力也做不到，可是你却能实实在在笃定一件事，他只要答应了，就一定会去做，十分气力做不到，也愿意咬牙使出十二分力气。 这就是一件让人感到心安的事情。 质胜文则野，文胜质则史。文质彬彬，然后君子。 攒点小钱钱，可难可难。 雪月两相宜，少年更清绝。 梦回少年丛中，吾亦是少年。 “有那人间美事之一，却最不赏心悦目，你猜猜看，是什么事情？”——“睡个回笼觉。” 一生痴绝处，无梦到龙州。青山立眼前，初逢两少年。 心湛静，笑白云多事，等闲为雨出山来。 小石碑，一块块块，竖在门口分两排。 风儿在跟竹叶打架，枝头鸟儿在劝架。 大雪给青山盖了一层又一层的被子，溪水吃掉了一颗又一颗的石头，一天天在长大。 白云不招呼就走，月色不敲门就来。 黄花黄，白云白，青山青，少年年少。 自律，自省，自求，自由。 些许忧愁似那小鱼儿，一个甩尾，窜入水草中，再不与人相见。 明天永远属于少年。 少年年年有，我始终在其一。 春风知我意，送梦到当年。世间多有不妥之人，世道多有不平之事，却休想打杀我心中之美好。 言念君子，温其如玉。 转过头，笑眯起眼，蓦然灿烂而笑，双脚轻轻跺地，双手飞快晃动。 别伤心别伤心，我把乐呵借你乐呵啊。 什么叫赤子之心？是与所做之事壮举与否，与一个人年纪大小，其实都关系不大，无非是有人过河拆桥，有人偏要铺路修桥，有人端碗吃饭放筷骂娘，有人偏要默默收拾碗筷，还要关心桌凳是否稳当。有人觉得长大是世故圆滑，有人偏觉得成长，是可以为己为人承受更多的苦难。有人觉得强者是无所拘束，是一种唯我独存的纯粹自由，有人偏觉得我要成为强者，是因为我要为这个世界做点什么！ 当一个人的见识过高，往往容易生出惫懒之心，反而不如一知半解、懵懂之人那么拼搏奋进。 花有再开日，年年如此，人无再少年，人人这般。唯有桃李春风一杯酒，总也喝不够。 其实中土读书人，不全是这样意气用事的。只不过很多时候，能够让咱们瞧见的，往往会是些龌龊人糟心事。 人生路上，会遇到很多一别过后再无重逢的匆匆过客。可是人心间，过客却可能是别人的久住之人。还会笑颜，还会高声言语，还会同桌饮酒醉醺醺。还会让人一想起谁，谁就好像在与自己对视，不言不语得让人无话可说。 小错早犯早知道，长辈早说孩子早记住。 谁说做了件好事，就不会伤人心了？很多时候反而让人更伤心 一个人手上的本事不大，嘴边的道理太大，会惹人烦，所以不用着急，先余着。 老道人一手托瓜，一手轻敲几下，侧耳聆听，自言自语道：“天地氤氲，万物化醇。大音希声美矣，大中至正粹然……肯定甜！” 摊开手掌，陈平安开着玩笑，说手中有阳光，月光，秋风，春风。还说人情世故事上练，破我心中犹豫贼。 老人从不觉得一个人的朝气勃勃，只是那种一年到头的言语欢快，行事跳脱。 而是在人生的每一个关隘那边，独独在苦难之际，年轻人反而能够眉眼飞扬，意气风发。 做出最意外的事，递出最快的剑，与这方天地说出最有分量的言语。 平时一贯寡言者，偶尔放声，要教旁人不听也得听。 大概这就是喜欢。 让一个人能够不像自己。能让乐观者悲观，能让悲观者乐观。能从绝境中看到希望，有胆子去憧憬未来。 能让一个贫寒困苦的陋巷少年，突然觉得自己就是天底下最有钱的人。 能让一个连剑字都不会写的草鞋少年，跨越山与海，默默练拳百万，还要默默告诉自己，一定要成为大剑仙。 世态翻覆雨，吾心分外明。 望之俨然，即之也温，恭而安。 桃夭 桃之夭夭 灼灼其华 少年情思如泼墨，哗啦一下就倾泻在了纸上，满是写意，妙在层层晕染，局中人看不真切。若是一场男欢女爱，历历分明，严谨如工笔画，言行举止纤毫毕现，敢问妙在何处。 真正有嚼头的男女情爱，就是哑巴吃黄连，旁人拦不住，不吃还不行。 男女关系，屋内有屋，楼上有楼，局中人说不清道不明，如犯死罪，最难自证清白。 爱情是个叫任性、小名顽皮的孩子，一长大就改名叫责任、别名默契了。 世上有一种无知，是最美好的。比如因为年少无知，因此情丝百结。少年与少女，何必在年少时就要懂爱情，那会儿懂得的，想必就不是爱情了。一语中的，真知灼见。 青青翠翠草木，年年岁岁旧人，朝朝暮暮相思。 你觉得何谓喜欢一个人？大概是如坠贼窟，任你杀贼如麻，依旧敌不过。 少年忧愁与眷念，满地月光，流淌如水。 终究还是一个女子。 只要走在人间情路上，谁不是患得患失的胆小鬼。 听了句不顺耳的话，女子的心路上，就会愁云惨淡，阴雨绵绵，可能蓦然听见一句中听的情话，又突然是艳阳高照，晴空万里。 今年心头喜欢之人，还是去年之旧容颜。 大概所谓挂念，就是心扉当中挂起一幅心爱女子的画像，念念不忘。 这位姐姐，说话真好听，嗓音脆脆的，好似盛夏梅子白瓷汤，碎冰碰壁当啷响哩，又善解人意，真是金声玉韵、蕙心兰质的一朵解语花呢。 单相思，就像一场上吊，自缢的绳子，就是思念，头顶那根横梁，就是那个求而不得的心上人。 所有不曾遂愿的单相思，都是个阴魂不散的吊死鬼。 女子眉如春山蜿蜒，有心事时，一双秋水长眸，便似有云水雾霭绕山。 小雨淅沥，天地朦胧，英俊书生忽见一女子，撑伞而行，青罗之衣，撑伞如花开陌上，人如杨柳依依春雨中，绝美。 不是所有男人，都会意识到自己的身边人心爱人，是万万年只此一人有此姻缘的。 看见他的容颜之后，就像心头蓦然窜出一头小鹿，在她心路上，撒腿乱跑。 喜欢一个人，就是照顾她一辈子，把自己这辈子也交给她。 我先走，最后看到的是她。她先走，最后看到的是我。 所思之人，翩翩公子，便是世间第一消暑风。 抬头，是三轮天上月，低头，是一个心上人。 男子年轻时候，总是想着自己有什么，就给女子什么，这没什么不好的。不同的岁月，不同的情爱，各有千秋，没有高下之分，好坏之别。人生无遗憾，太过圆满，事事无错，反而不美，就很难让人年老之后，时时惦念了。 年轻女子言语不多，更多还是看着身边的男人。 她的眼睛在说着悄悄话。 他双手笼袖，安安静静看着这一幕，风景绝好。 天底下只要是真正好看的女子，说不说话，都是风景。 很多我原本也以为是天经地义的道理，如今都不知不觉，就变了很多，唯独这件事，从来没有变过，喜欢一个人，就只喜欢她，很够了。 少女想法，大概总是要比同龄少年更长远的，怎么说呢，两者区别，就像少年郎的想法，是走在一座山上，只看高处，少女的心思，却是一条蜿蜒小河，弯弯曲曲，流向远方。 真正喜欢一个人，是要喜欢一个人不好的地方。 有些人总是这么不一样，看了一眼，就能让人记住很多年。而有些人，哪怕看了再多年，也没在心头住下。 愿娘子春寒衣暖，愿娘子愁眉舒展，愿娘子次次推窗就是明月当空，绿水青山…… 不知为何，又想起了在家乡遇见的那位青衫读书郎，他的模样干干净净，像是夜夜笙歌、灯红酒绿的红烛镇大泥塘水面上，飘过的一片春叶。 破山中贼易，破心中贼难。 男女情爱一事，不要寄予有过高的期望，不要在自己心中全无希望。 世间喜欢好像都一般，低低在地。她喜欢他，他喜欢她，就是不知道那个她又会喜欢哪个他。 怎么个动人，能教原本打算一辈子守身如玉的忠贞汉子，一眼望去的功夫，就变了五六回心。 终究还是一个女子。只要走在人间情路上，谁不是患得患失的胆小鬼。听了句不顺耳的话，女子的心路上，就会愁云惨淡，阴雨绵绵，可能蓦然听见一句中听的情话，又突然是艳阳高照，晴空万里。 我是否喜欢谁，与谁喜不喜欢我，半颗铜钱关系都没有！就像山看水，水流山还在，喜欢之人，只管远去，我只管喜欢。 人生美好风景如初见，风景得是多美好。 男女情爱一事，心心念念，求之不得的，其实都只是心目中的那份儋州云霞钱江潮，牵肠挂肚，百般恨千种怨，怎一个愁字了得，可等到真正得手了，儋州云霞钱江潮还是儋州云霞钱江潮，心却变了，风动耶旛动耶，心动而已。 男女情事，谁先动心谁吃亏，越吃亏越难难忘，到最后，到底是喜欢对方呢，还是喜欢自己，都搞不清楚了，答案偏偏在对方身上，所以才说，由爱故生忧。 此身原本不知愁，最怕万一见温柔。 心仪女子之美，总是这般动人，美得教人装得下日月的双眼都装不下她，得搬去心扉，余在心头。 初见时，她姗姗然从我心头路过，荒芜之地就开满了花。惨绿少年春游遍，罗绮百花成丛，就中堪人属意，最是XX，还是XX，只是XX。九岁与卿初相识，再见卿时吾九十。少年骑竹马，转身白头翁。 天底下单相思的痴情，好像便都是这般一文不值的。可若是值钱，又何必相思呢。 眉是聚愁峰，眼是折柳渡。 爱欲之人，犹如执炬逆风而行，必有烧手之患。 思君如弦月，一夜一夜圆。 但是被人喜欢，是一件很难得、需要很珍惜的事情唉，比不被讨厌还要难嘛，所以可不是一件可以拿来炫耀的事情，就应该只是一件偷藏在心里的高兴事啊，然后偶尔心情不好的时候，一开门，就会高兴嘞，一开门就心情好，所以就叫‘开心’嘛。 愁绪如山，都攒在眉头，情思似水，都流到心头。 没有说出口的特别喜欢，就像一场无声无息的鲸落。 我是一把镜子，不信的话你瞧瞧，我眼中有没有你？ 只要遇见对的人，双方眼中便会看见最好看的景色，如天各一方，日月遥对，目光却亘古不变。 少年喜欢少女，是饮糯米酒酿，酒味其实不重，可是初次喝酒，也能醉人。长大之后，男子喜欢女子，如饮烧酒，一个不小心就要烧断肝肠。上了岁数，老人思念女子，是大冬天，温了一壶黄酒。 桃花开时，若是花上还有黄鹂，尤为动人，眼不敢动，心魄动也。 等人是一件很开心的事情啊，然后等着人又能马上见着面就更幸福嘞。 多年以来，她始终在一处山中，修道幽居，不来见我。 哪处山头？ 我心中。 有些远远的喜欢，总是忍不住要让人知道，才能甘心。 看了她一眼，人间颜色如尘土。 各有因缘不羡人，各有付出无愧人。 何谓心仪，大概是人海熙攘，惊鸿一瞥，再难忘记。 女子不是真的全然不讲道理，只是往往男子所讲的道理，与她们想要听的道理，往往不在一条脉络上。女子的道理，其实更多在心情。如果男子连她为何不讲理，都整不明白，那就没辙了，自然只会说多错多。 世间不是所有男女情思，都会是那春种一粒粟，秋收万颗子，可能没有什么春种秋收，一个不小心就会心田荒芜，就是野草蔓延，却又总能野火烧不尽，春风吹又生。 她在不在乎，是一回事，自己在不在乎，绝对是另外一回事。她之所以会不在乎，可不就是自己次次很在乎？ 天下姻缘，世间情动，也多有那蓦然回首的悄然生发。 可能是因为离着远了，喜欢的人会更喜欢，讨厌的人也就没那么讨厌了。 男女情爱，何谓风流薄情，就是一个人明明只有一坛真心酒，偏要逢人便饮。 何谓深情，就是一坛酒深埋心底，然后某天独饮到底，喝光为止，如何不醉。 就像男女情爱之间的磕磕碰碰，其实女子那些让男子摸不着头脑的情绪，本身就是道理，认可她的这份情绪，再帮忙疏解情绪，等女子渐渐不在气头上了，然后再来与她心平气和说些自己道理，才是正途。这就叫退一步思量，先后顺序的学以致用，一旦跳过前边的那个环节，万事休矣。 境意 桃李春风一杯酒。 出门俱是看花人，河边多有钓鱼客。 不知名的曲子，笛声空灵悠扬。 四下无人处，明月分外明。 天地寂寥时，笛声尤其清。 还蛮好听的，青天鹤唳，云外龙吟，声在庭院。 光阴匆匆最无赖，用少年白了头，朱颜亦辞镜，偷偷换取樱桃红，芭蕉绿。 山一程，水一程，风一更，雪一更。近路愁，远道愁，南一声，北一声。 思悠悠，恨悠悠，江水流，河水流。梦难成，意难平，东山青，西山青。 曾经的先生，在回乡路上，牵着一匹瘦马，随水转，转山斜，斜阳古道，道旁孤村三两家。山瘦水也瘦，马瘦人更瘦。 日月驱光阴，江湖动客心。 新年春风里，陌上又花开。 陌上又花开。 牧笛，驼铃，皆是风过声。 他远远看着那幅画卷，就像在心中，开出了一朵金色的莲花。 人从天上，载得春来。剑去山下，暑不敢至。 尤其那些篆文，极慰人心。青丝染霜雪，依旧是美人。 山上层层桃李花，云间烟火是人家。 天幕中星星点点，如同最漂亮的一幅百宝嵌，挂在人间万家灯火的上方。 月色入高楼，烦，它也来，恋，它也去。 人生初见，山野见少女婀娜，登高见山河壮阔，仰头见仙人腾云，御风见日月悬空，与以后见多了类似画面，是决然不同的风景。不一定是初见之人事一定有多美，但是那份感觉，萦绕心扉，千百年再难忘记。 过鸟一声如劝客，仙人呼我云中游。 常将半夜萦千岁，只恐一朝便百年。 愿先生心境，四季如春。 那些入秋的喜怒哀乐。 其形，神姿高彻，如瑶林琼树，自然风尘物外。其神，夜光之珠，仿佛一轮遗落人间的袖珍明月，未被月宫神人收回天庭，无数的碎片像那璀璨星光，如众星拱月。 那些心尖上摇曳的悲欢离合。 心离其形，如鸟出笼。皎然清净，譬如琉璃。内悬明月，身心快然。 唯有冬夜里冰冷刺骨的瓢泼大雨，像是老天爷睡梦里的念念不休。 我想变成一棵树，开心时，在秋天开花。伤心时，在春天落叶。 卯时，天微亮，山中多雾，气象清新，朝露凝结在花叶，团团圆圆，摇摇晃晃，欲语还羞。 夕阳西下，就像有人在天边放了一把大火，烧得云海鲜红。 秋霁，眉妩，赚煞，山渐青，水龙吟，眼儿媚，更漏子，水调歌头，卜算子慢，千秋万岁，花雪满堆山，荷叶铺水面，春从天上来，入梦来，风波定，好事近…… 天上月，人间月，负笈求学肩上月，登高凭栏眼中月，竹篮打水碎又圆。山间风，水边风，御剑远游脚下风，圣贤书斋翻书风，风吹浮萍有相逢。 下雨是乡愁的声音。 忽然春天，蓦然夏天，突然秋天，已然冬天。 世事若飞尘，向纷纭境上勘遍人心。日月如惊丸，于云烟影里破尽桎梏。 我怜梅花月，终宵不忍眠。 终宵不忍眠，月花梅怜我。 谁终将声震人间，必长久独自缄默。 谁终将点燃闪电，必永恒如云漂泊。 师长，同窗，好友，故人好似庭中树，一日秋风一日疏。 成长 一个孩子渐渐长大，尤其是等到爹娘走后，就像一家门户，少了一扇大门，门外就站着死亡，轮到这个人去与之对视。 她小时候的每一个明天，都好像有做不完的好玩事情，每天的行程，都满满当当，所以需要小姑娘一直跑得飞快，车轱辘转动似的不停歇，仿佛跑得太快，一下子把童年岁月落在了身后，人长大了，童年就会留在原地，偶尔回头望去，愈行愈远，模糊不真切。 抬头看了眼天上云海。记得小时候，随便看一眼云朵，便会觉得那些是爱妆扮的仙子们，她们换着穿的衣裳。 她在小时候，好像每天都会有这些乱七八糟的想法，成群结队的闹哄哄，就像一群调皮捣蛋的小人儿，她管都管不过来，拦也拦不住。长大以后，我好像再也没有那些念头了。好像它们不打声招呼，就一个个离家出走，再也不回来找她。 长大不是慢悠悠的岁月变迁，不是从一个地方走到另外一个地方，往往只是一瞬间的事情。心意所至，飞剑所往，身心性命皆自由。 小时候，会觉得有好多大事真忧愁。长大后，就会忘了那些忧愁是什么。 年少时，喜欢与厌恶，都在脸上写着，嘴上说着，告诉这个世界自己在想什么。长大之后，便很难如此随心所欲了。 小时候，日子好像是一天一天，掰着手指头过去的。 大一些，一个月一个月，便过了每一年。 人生往往如此，碰到了，分别了，再也不见了。 没有那些让人觉得哪怕物是人非，也有故事留心头。 人生苦难书，最能教做人。 人都会长大的，长大了之后，就会捡起一些新东西，丢掉一些旧东西，就这么丢丢捡捡，哗啦一下子，就老喽。 大概童年，就是一场无忧无虑的跳方格，方格内是自己的家，方格外是外边的世道。 如果打个比方，童年就是一场跳方格的游戏，那么爹娘、长辈们的规矩，言传与身教，就是那些条条框框的线条。 复杂的世道里，人之天真，就是一把无鞘剑，只能将其悬挂在一堵名为童年或少年的墙壁上。兴许可以偶尔返回心乡时，看它几眼，却不能一直随身携带。 对孩子来说，什么叫长大，大概就是能够爹娘不管，想吃什么就吃什么。对成人而言，什么叫有钱，也许就是可以不看价格，想买什么就买什么。 只是年岁渐长，就会越来越明白一个道理，哪怕是与人给予善意这种事，我之心无愧疚，对待某事不曾多想，与他人之心思百转，反复思量，同一件事会是两种心思。懂得这个道理，不叫无奈，而是成长。照顾他人内心，本来就不是什么简单的事情。 孩子就是孩子，所以有些事情，成人不能奢望孩子们去理解，有些道理，就真的只能孩子们在各自成长过程中，去慢慢体会。如果说梦想是堆雪人，大概成长就像吃冷饭。 对一个还是孩子的人来说，早早懂得哪怕明明是某个极好的道理，所谓的更早懂事，就是一种残忍。 老人一年一年老，少年却难再年少。 总会有些人，会让我们想要成为那样的人。 人难无过错，人生多错过。事错过，错过人，反复思量，都是过错，过去的错。 很多个儿矮矮时候的有趣事情，就像兜里的瓜子，一磕就没了。 晓不晓得天底下哪个家伙的忧愁最多。是那个名叫“长大”的家伙。 孩子吃疼，哇哇大叫。成年人呢…… 真正过去的事情，就两种，完全记不得了，再就是那种可以随便言说的往事。 大概所谓成长，就是有个谁都不知道好坏的自己，在远处等着今天的我们走过去见面。 每一个昨天的自己，才是我们今天最大的靠山。 道不远人，苦别白吃。 未长大的孩子会把心里话放在嘴边，长大了就是会把心里话好好放在心里。 老人一老，就会说些翻来覆去的车轱辘话，三十岁之前的年轻人，听着往往倍感厌烦，来一句“说过了”，便让老人陷入沉默。只是等到年轻人自己变成了中年人，尤其是等到有妻有子了，在面对自家老人唠叨的时候，耐心又往往会变得越来越好。 生思 被一个人太喜欢了，被喜欢的那个人，好像就不太懂得怎么喜欢对方。简而言之，就是被宠坏了。习惯了与人索取，不懂付出。 忘了是谁说过，犯错与遗忘，都是天公作美，是一种带着怜悯的温柔，属于法外开恩。 多少世事与人心，兜兜转转一大圈，原来还是在原地。 一本书，言语质朴，故事流畅，偶有几句妙语，就是平地起惊雷。 如果连篇累牍，皆似花团锦簇，只知一味堆砌，反而远远不如一碟咸菜佐粥的滋味。看待女子，亦然。 不用假装与这个世界如何亲近，也不用假装与这个世界如何疏远，理贵适中平常心，不可过厚与太薄，我们还是我们，我们就是我们。 老人走着走着，才发现用心看旧风景，就像是新风景。 一直听不见想听的话，时日久了，我们当然会感到失落，但是不用怀疑我们心中早早就有的那个答案。 喜欢且擅长讲求一个层层递进，环环相扣，不轻易否定，却也不会轻易认定，真正的好，往往在更高处。 许多少年朝气和雄心壮志，被世事那么一嚼，就沦为了满地甘蔗渣。 为人处世，需要跟精明人精打细算，不然他不骗你骗谁，同时还需要跟聪明人待人以诚，切记你笨一点，就是聪明两点。 人生不求十全十美，偶有美中不足，月未全圆花半开，不是很好么。 苦日子只能熬，别无学问。但是有钱以后，过上了好日子，讲究就多了，家风若好，哪怕一时不显，必定子孙晚发，不会受穷，会有晚福。不仅仅是道理如此，事实就是这样。 人间许多言语和絮叨，都是这个世界想要听见的话，不是我们自己想说的话。 若是所有心中美好，都成为了一种负担。那么美好的意义何在，如果如此，肯定是我们有哪里做得不对了。 年轻人，朝气勃勃，喜欢也敢于否定世界的诸多不合理。 某些老先生们的心胸气量，都是被历史和苦难撑开的，所以在各种各样的年轻人那边，这些老人们都愿意对年轻人的言行，说个好，给予肯定。 少年有青云志，任侠意气，作白雪文章，当然是好事，可是切记一点，为人若无器量，自己心中无容他人之地，终究只是血气之私，技能之末。恐怕只会把脚下道路越走越窄。 老实做人，安心睡觉。公道求之，自有宽路。 不堪回首的往事，与之背对而行，生活道路上每走一步，不回头看就是了，最终就可以越走越远，直到彻底释怀。 许多话，是言者无意听者有心。那么犹有一些话，是言者有心听者无意。 一个人某些棱角鲜明的性格，城府深沉如宫阙重重复重重，阳光普照的白昼时分，也有阴影无数。 锋芒毕露的才华横溢是一座文昌塔，嫉恶如仇是一座城隍庙。豁达或开朗，便如一座凉亭，四面通风。 抑郁如坠入一口无底深井，暗不见天日，我与我独处，与世隔绝，无法自拔。 大概世间有一种自讨苦吃，叫作设身处地，处处替他人着想。 世间事，一犬吠影，百犬吠声。人间人，一人道虚，千人传实。 为师者传道，求学者受业，皆须心平气和，先生治学严谨，气态安详，学生求学恭敬，彬彬有礼，且共从容。 天地间的第一等读书人，在‘礼’字上做学问，或开辟或稳固道路，让人间道路，干旱不干裂，雨季不泥泞。就像我们来时的路。第二等读书人，穷其一生，在‘理’字上钻研，力求得其醇正，承袭道统续香火。就像那边的屋舍，还有我们手中雨伞。第三等，在书斋治学，白首皓经，在‘字’上兜兜转转，也能裨益文脉。就像每隔三五里路，就有一处的路边歇脚行亭。再下一等，就是读过很多圣贤书，仍旧是半桶水，趋利避害，却也无心害人，还愿意做些力所能及的好事，天底下的读书人，这类人十占八九。又下一等，便是俗不可耐的腐儒了，道貌岸然，古板迂腐，以礼教道统和正人君子自居，行事刻薄，不通人情。最下一等，则是伪君子，真小人，他们学问越大，于世道危害越大。就像一本佛经上说的某种人，入我法中，住我寺院，坏我正法。 人之年少阶段，除了求学，增长见识，还需要讲究一个培元气养精神，强身健体，稳固体魄。 你看，每年冬去春来，新翻杨柳枝，风景旧曾谙。曾经的逝去的过往的痕迹，是有几分哀伤缅怀之意的。 当我们知道了一个个更多的‘为什么’，会让我们更有耐心和平常心，一个人能够心平气和，就是修心功夫有成，以后遇到事情，就不容易与人说气话，说重话。 很多时候，犯错了却知错，有两种可能，一种是就此习惯成自然，都懒得自欺欺人，只是学会用一个个借口铺开心路，另外一种，就像在人心中筑起一道堤坝，不会洪水泛滥，走极端。过则勿惮改。 当我们无法对自己负责，就很难有资格对别人负责。 阳寿参差，不独在天，修身养性，可以永年。 有人说，不苦人不敢不从之事，要劈开自家胸中荆棘，打破心中壁垒以便人我往来，便是天下第一快活世界。那些荆棘与壁垒，你以为是什么？是我们自身与心中的道与理，礼与法。喝水不忘挖井人。万年之前，先贤们若无舍我利他的心境和舍生忘死的气魄，人间就不可能有如今万年的‘人间’。年年春风和煦，也会吹老美人面，白了少年头。 童年是个楔子。 有些朋友，一见如故，如饮烈酒。有些交情，却是一壶需要文火慢炖之酒。 好像总是这般事与愿违。所以我们才要愈发珍惜心中的各自美好。 有点小聪明的人不栽个大跟头，结果只是吃点不痛不痒的小苦头，很容易归咎于运气，而不是承认自己的脑子不太灵光。 苦酒尚有回甘时，苦情却似无涯山海都填不满的无底洞。 人之追忆缅怀，伤感和遗憾，宛如古井深潭，深陷其中，不可自拔。 情人间的眷念，一路蔓延而去，风驰电掣，远远乡念念人，好似他与她，转瞬即相逢。 当我们越对这个世界怀揣着希望，给予越多的善意，世界是否回报以善意，还是反而还以恶意，我们就会越在意，就会越受累。如果觉得都没有关系，大概这就是一种修行。天地间只有两种强者。我向这个世界获得了什么。或雄心猛气，气概凛然，取之有道，青史留名，或巧取横夺，恶狠狠争来一场富贵名利，难将由我，我不为难，谁敢兴之。我为这个世界付出了什么。穷则独善其身，名声不显心不朽，再挑灯火看文章，达则兼济天下，欲立掀天揭地的事功，自讨苦吃，缓缓向薄冰上履过。 施恩宜由淡转浓，由浓转淡反成仇。刑罚宜从严转宽，先宽后严怨其酷。 言语既是人与人沟通的桥梁，人间多歧路，同样来自言语。 愁思飘到眉心住，老尽少年心。 我觉得他可能就只是在做一件自己真心喜欢的事情吧，别人眼中的结果如何，好像不是那么重要，又可能这个过程就是最好的结果。 你最终能够依靠的，始终是你自己。 不要跟这个世界达成和解，每一次所谓的和解，是自欺欺人，就是委屈，委屈永远是委屈，不会减少丝毫的。只说我自己的一点见解，要小心翼翼，偷偷摸摸，悄悄拆解这个世界，首先就得知道这个世界到底是怎么回事，了解很多人会什么会说那样的话，做那样的事。贫时靠狠穷靠忍，至于等到下下人翻身变成上上人，会不会变本加厉报复这个世界，到底是一门心思报复曾经的恶意，还是报答当年的某些善意，或者两者兼有，人各有志吧，都可以理解。 一个人在最没钱的时候，遇到的好人坏人好事坏事，都是真。 想都不敢想的事情，何谈做成，人生在世，与自己少说几句‘我不行’。道家讲究心斋坐忘，你就要独自一人坐断太虚，心斋独自成天地，佛家说面壁坐禅，你就要把蒲团坐穿把墙壁打破，即便前路不通就以拳开道。 只是当我们为某件事付诸努力，长久以往，也看得见，就是容易被视而不见，因为努力之人和旁观之人，都不觉得这是一种天赋。我一直觉得，不咬紧牙关真正努力过，是没资格谈天赋的，认准一条道路，再得其门而入，能够不分心，在正确的方向上，持之以恒，脚踏实地，再猛然抬头，这会儿你看不见背影的，走在你前边的人，就是天才，输给他们，是命，再有抱怨，就可以大大方方怨天不怪己了，吃饱穿暖，睡觉安稳，问心无愧。知道了自己与那些天才的差距，就是努力过后的收获，不要觉得没有用处，这对于你以后的人生，大有用处。 门外荣辱排队过，困穷之后福跟随，家教门风之所以重要，就是可以让人吃得住苦，接得住福。 很多苦难困顿坎坷，都可以用一个美好的童年来与之为敌，不落下风。 就像一个寒冬，可以用怀念暖春来抵御，不轻松的时日，总会过去的。 也可能很多人生后来的辛苦努力和沉默付出，都是在与各自不那么美好的童年，独自在心中做一场不为人知的艰难拔河，这场架，可能会伴随一生，至多打平，绝无胜算。 人生可能没有真正的同悲共喜，大概就像两个人，就是两座天地。 各有所思，你情我愿，此消彼长，教人间没个安排处。 很多书上看见很多道理，一个苦处明白一个道理。 只看见，不明白，就是幸运。 世人往往误以为天下远游，只是两腿走路，游子离乡，千山万水。 实则不然，每每心念起某事，到达成某事，就是一场心路上的远游。 夜路难行，低头赶路不难，就怕一抬头，四周疑目如盏盏鬼火，流言蜚语如汹汹洪水。 列星随旋，阴阳大化，并不围绕一人而转动，日月递炤，也不只为一人而高明，各有人生，各有缘法。 就像一个家里的长辈，大多如此，明明心里很在意，偏要假装不在意。 难得开口，说话也总是轻描淡写，晚辈稍不留心，就会错过老人们很多藏在平淡脸色、眼神、言语的意思。 有些事，我只是说有些事，看似大家都故意不说，其实反而就是一直故意在说了。这样的好心好意，当然是很好的，不过长久以往，兴许也是一种负担，有些时候还不如挑明了，不躲着它，它就自己跑开了。躲着它，它就跟我们的影子一样，他人看待我们的眼神，我们以为的那些私底下的议论，就像人生路上……白天的日光和晚上的月色，让我们心里边最放下的某件事，如影随形。 古人有云，注得一部古书，薪火相传，可称万世宏功。著得一部新书，文以载道，便是千秋大业。 什么叫真正的好书。 翻书看到开怀处，读完只觉得口齿留香。 揪心处，只觉得心头被扎钉子，合上书后，想要喘口气都难。 会心处，与书中某人，或是某句话，一见如故，它们仿佛在书山中，等候已久。 我等文字字等我。 我们都是一盏灯火，在天地间忽明忽暗。言行互为卯榫，人心共作灯火。 搭建屋舍，抱团取暖。 每一个我们不敢承认的自己。 就是一头徘徊笼中的困兽，就是一尊高坐大殿的神灵。 一个人的离乡远游，就像一场两手空空的搬家，只是在心中搬走了整个故乡。 道理可以是年年一换的春联、福字，是一场悄然来去的春风细雨，是总会消融殆尽的冬日积雪，是一去不复还的流水，是缝缝补补又一年的老宅子，是看似推倒重建、却始终保留地基的新屋子。 但是真相，只会是大夏天曝晒穷人后背的骄阳，是所有人抬头望向太阳时的视线灼烧，任你有千百道理，万千理由，不管明不明白道理，都得受着。 眼睛看不清耳朵聋，已经是个菩萨了。 恐怕一地方言的消散，就是一座故乡的消亡，就像一个老人的逝去，落土为安。 人生会有很多的结果，却没有任何一个如果。 一个人动用智慧，就是烧炭取暖，要学会韬光养晦，才能烧得长久。 人生有两事最有嚼劲，与故友久别重逢，喝酒半熏醉，回头看生平，饮茶有回甘。 一个乐观，一个悲观。 前者眼中，所有的远游，是为了重逢之日。 后者看来，所有的相逢，都是离别的铺垫。 有人曾经说过，一个人有两个年龄，一种是活在自己的世界里，一种是活在别人的世界里，前者是虚岁，后者是周岁。 一个人的不合群，只有两种情况，一种是鹤立鸡群，一种是鸡立鹤群。 对的事，好的事，眼前事，身后事，一时事，千古事，混淆在一起，怎么分得清楚？ 那些听上去不是那么好听的真心话呢，就跟酒一样，一开始喝，可能会难以下咽，可是喝着喝着，就发现这才是天底下最好喝的好酒呢。还有那些自顾自的生闷气，就跟会变味的酒一样，自己又喝不掉，一打开酒坛子，谁都不愿意喝。那股子酒气，就是一个人不太好的情绪，积攒多了，看上去谁都闻不着，其实谁都知道，但是只能假装闻不着，不知道。日子久了，看上去好像谁都在照顾对方，其实谁都委屈哩，很累人的。 一切言语反而是累赘，只需相视而笑，便是莫逆于心。 有些事，就只能走一步看一步，不然到头来就是个‘如果如何’，一笔糊涂账，满是怨怼。 每个人都该有自己的人生，大概好的人生，就是我们能够为自己的人生负责。 言语这个东西，很奇怪，是会一个字一个字，一句话一句话堆积起来的。 好话总是会让人难受，听了让人倍感轻松的道理，往往不是道理。 为人既不可妄自尊大，目中无人，看轻他人，也不可妄自菲薄，心中无我，看轻自己。只有不走极端，才算君子，才算正人。 手段多心机重则天机浅。 能受天磨是豪杰，最难难在永天真。 人生路上，被人给予希望越多，自己又不愿让他们失望，那么这个人就会比较辛苦。 为月忧云，为书忧蠹虫，为学问忧薪火，为百花忧风雨，为世道坎坷忧不平，为才子佳人忧命薄，为圣贤豪杰忧饮者寂寞，真是第一等菩萨心肠。 天下人事，莫向外求。 真正的沟通和讲理，是要学会先认可对方。你需要自己先做到心平气和，然后用很多个的认可，来讲清楚你真正想要说清楚的那一两个否定。当然，你的一切言语，仍需诚心诚意，不能是假的。这一点，极为重要，要搁在‘心平气和’的更前边。 所以就事论事本身，当然是好事，可一旦谁占理了，粗脖子，瞪眼睛，大嗓门说话，结果会如何？显而易见，道理本身是对的，讲理一事，却是失败的。 再就是一定要告诉自己，谁都不是没有半点火气的泥塑菩萨，谁都会有自己的情绪，情绪本身，就是道理，很多时候，看似是在跟人讲理，什么时候真真切切看在眼里了，却不觉得自己是在容忍，那就是我们真的修心有成了。 老话说，通达之人必有谋微之处，其实反过来说，也是个好道理，擅长谋微之人，也当有一颗通达之心。 大概人与人之间的诸多误会，可能就是不该说的无心之语，随便说，该说的有心之语，反而吝啬不说，两张嘴皮子关起门来的喃喃自语，却误以为对方早已都懂。 不管山上山下，好人坏人，人心善恶，成年之后的男人女人，谁没有几坛深埋心底的伤心酒？只是有些忘了放在哪里，有些是不敢打开。人生路上，每一次敢怒不敢言，还要与人低头赔笑脸之事，可能都是一坛苦酒，大概苦酒多了，最后教人只能闷不吭声，接连成片，就是苦海。 花好月圆人长寿，称心如意事顺遂。 一棵山中幽兰。 它从不曾见过世人，世人也不曾见过它。 便不开花吗？ 旁人的道理，老人的经验之谈，都不如你自己多加琢磨，来得牢靠。 君子之心，天青日白，秋水澄镜。君子之交，合则同道，散无恶语。君子之行，野草朝露，来也可人，去也可爱。 少年时的爱慕欣欣焉，回头再看，就是美好的怀念。 人生路上，总是有心栽花花不开，无心插柳柳成荫。 一心二意，不三不四，人五人六，乱七八糟，八九不离十，是个怂蛋。 当世道亏欠一个人的童年越多，当那个人长大之后，就会一直在缝补和弥补。 为人处世，力所未逮，那就尽量求个心安，是好习惯。 什么都不知道，很难不失望。知道得多了，哪怕还是失望，终究可以看到一点希望。 怕就怕一个人以自己的绝望，随意打杀他人的希望。 不用讲道理，有些时候，发泄情绪本身，就是一种道理。 当读书人自身利益受损，还能够保持一颗平常心，就算修身小成了。做不到，就是道貌岸然。 心中能不能活着一些已逝之人，只要想起他们的言行举止，就会觉得自己做得还不够好。 人生道路上，出现任何问题，先压情绪，所有思虑，直指症结所在。 求个良心管我，做个行善人，白昼天地大，行正身安，夜间一张床，魂定梦稳。 他不是不记仇，不是他觉得这是什么无所谓的事情，只是他自己愿意原谅你，但是别人的原谅，与我们犯下的错，是两回事。世事就是这么复杂，我们兴许做了好人做了好事，可是好多的错，还在，一直在，哪怕所有人都不记得了，自己还会记得。也不是你有很多很多的理由，真的有万般理由，去做了错事，错事就不是错事。 不是所有错误，都是我们知道错了，还能有弥补的机会，甚至很多错误，我们错了，想要改错，就是没有机会了，没有了。 不会自寻求，却数他人宝。数他宝，终无益，请君听我言。垢不染，光自明，无法不从心里生，出言便作狮子鸣。 世事大梦一场，饮酒不怕醉倒，不醉反是梦中人。 世间人人心独坐。 所有看似无所谓了的过往之事，只要还记得，那就不算真正的过往之事，而是今日之事，将来之事，此生都在心头打转。 那也收着点啊，不能一次就开心完了，得将今日之开心，余着点给明天后天大后天，那么以后万一有伤心的时候，就可以拿出来开心开心了。 希望此物，不单单是春风之中甘霖之下、绿水青山之间的渐次生长。往往是那夜幕沉沉，烂泥潭里或是贫瘠土地中，生长出来的一朵花儿，天未破晓，晨曦未至，便已开花。哪怕风雨摧折，那我再开花一朵。更大的真正希望，是无法开花，也不会结果，许多人生就注定只是一棵小草儿，也一定要见一见那春风，晒一晒那日头。人间多如此。为何不善待。 每个人都有自己的一本书，有些人写了一辈子的书，喜欢翻开书给人看，然后满篇的岸然巍峨、高风明月、不为利动，却唯独无善良二字，但是又有些人，在自家书本上从来不写善良二字，却是满篇的善良，一翻开，就是草长莺飞、向阳花木，哪怕是隆冬酷暑时节，也有那霜雪打柿、柿子红通通的活泼景象。 与人说理，不是自己有理即可，还要看风俗看氛围看时机，再看自己口气与心态。 知错能改善莫大焉，知错却无法再改，悔莫大焉，痛莫大焉。 往细微处推敲人心，并不是多舒服的事情，只会让人越来越不轻松。可如果这种一开始的不轻松，能够让身边的人活得更好些，安安稳稳的，其实自己最后也会轻松起来。所以先对自己负责，很重要。在这其中，对每一个敌人的尊重，就又是对自己的一种负责。 只要言语之人，初衷不坏，天底下就没有难听的言语，真要有，就是自己修心不够。 因为所谓的性格棱角，不是漏进鞋子里的小石子，处处硌脚，让人每走一步都难受。而是那种溪涧里的鹅卵石，瞧着任人拿捏，但真要咬一嘴，就会真正磕牙。 自己没做好，留不住人，就别给自己找理由，怪自己是什么好人，觉得痴心喜欢女子也是错，扯什么温柔待人，不如他人的嘴上抹蜜花里胡俏，自己眼光不行，就认。很多人喜欢谁，除了喜欢对方，其实也喜欢自己，陶醉其中，爱得要死要活，鼻涕眼泪，是做样子给自己看的。连自己瞎了眼、或是碰了运气喜欢的人，到底是怎么想的，连对方到底值不值得自己如此付出，完全不知道，反正先把自己感动了再说。 只是有些要好朋友之间的隔阂，看似可大可小，可有可无，一些伤过人的无心之语，不太愿意有心解释，会觉得太过刻意，也可能是觉得没面子，一拖，运气好，不打紧，拖一辈子而已，小事终究是小事，有那做得更好更对的大事弥补，便不算什么，运气不好，朋友不再是朋友，说与不说，也就更加无所谓。 惊蛰时分，天地生发，万物始荣。夜卧早行，广步于庭，君子缓行，以便生志。 心关难过，有些时候，以往百试不爽的一技之长，好像无法过关，最后发现，不是傍身立身的学问不好，不够用，而是自己学得浅了。 人生就是一壶浊酒，想起一些人事，便在饮酒。 每一个清晰认知的形成，都是在为自己树敌。简直就是与世为敌。大地之上的野草，反而远比高树，更经得起劲风摧折。 真正忧愁，只在无声处。 所有久别重逢的开怀，都将是未来离别之际的伤心。但这不妨碍那些还能再见的相逢，让人欢喜，让人饮酒，让人开心颜。 没有足够认知作为支撑的那种绝对自由，既不牢固，也是灾殃。 天下风景，不仔细看，都会相似。 站得高看得远，对人性就看得更全面。站得近看得细，对人心剖析便会更入微。 站在不同的地方，看待同一件事，便可能是一种人心两回事。 我们生于天地间，其实没太大区别，就是一场好似再没有机会回到故乡的远游求学，最终决定我们是谁的，不是日渐腐朽的皮囊，只会是我们怎么想，甚至不在于我们想要什么，要去多远的地方，就只是‘怎么’二字上的学问功夫，人生短暂，终有力再不能助我前行的停步之处，到时候回头一看，来时路线，便是一步步的怎么，走出来的一个什么。 问题没有答案，问题本身就是答案，许多看似答案，就是下一个问题。 世间万事，终究可以看个大致的结果。而结果，往往又是下一段因果的起因。 天底下没有什么所谓的无心之语，只有不小心说出口的有心之言。 很多时候，好像只是相差那么一口气，便会造就出天壤之别的是非对错，善恶之分。 所以有些人看上去朋友遍地，可以处处与人饮酒，仿佛人生无处不筵席，可人生一有难关便难过，离了酒桌便朋友一个也无，只得愤恨世态炎凉，便是如此。不以真心交友，何以赢取真心。精明人少有患难之交，更是如此。 谁都会有自己的隐私和秘密，如果双方真是朋友，对方愿意自己道出，即是信任，听者便要对得起说者的这份信任，守得住秘密，而不该是觉得既然身为朋友，便可以肆意探究，更不可以拿旧友的秘密，去换取新朋的友谊。 知道自己不知道，就是有点知道了。 许多事情，许多人，都以为自己脚下没有了回头路，其实是有的。 真正睁眼，便见光明。 读书和远游的好，便是可能一个偶然，翻到了一本书，就像被先贤们帮助后世翻书人拎起一串线，将世事人情串起了一串珠子，琳琅满目。 世间有山上山下之分，又有富贵贫贱之别，可是苦难的分量，未必有大小之分。落在每个人头上，有人听了一句言语的难熬，可能就是别人挨了一刀的疼痛，这很难去用道理解释什么，都是一般的难熬。 所有人都有做不好的事情，暂时的有心无力，不算什么过错。等到有心有力，还能一一补上，更是难得。 有些言语，话难听。可是愿意与人当面说出口，其实都还算好的。真正难听的言语，永远在别人的肚子里边，或者躲在阴暗处，阴阳怪气说上一两句所谓的中允之言，轻飘飘的，那才是最恶心人的。 有些言语，需要去看而不是听。 世人的聪明和愚笨，都是一把双刃剑。只要剑出了鞘，这个世道，就会有好事有坏事发生。 生死之间，人性会有大恶，死中求活，不择手段，可以理解，至于接不接受，看人。 生死之间，我一直相信求生之外，芥子之恶蓦然大如山，是可以理解的。但是有些人，可能不会太多，可一定会有那么一些人，在那些明知必死的关头，也会有星星点点的光亮，骤然点燃。 世事从来不简单，就看愿不愿意琢磨了。 这天在山林溪涧旁掬水月在手，低头看了眼手中月，喝了口水，微笑道：“留不住月，却可饮水。” 见人处处不不顺眼，自然是自己过得事事不如意，过得事事不如意，自然更会见人处处不顺眼。 同一件事，时过境迁，偏是两种难。 一个国家真正的强大，不是掩盖错误的能力，而是纠正错误的能力。 恶人恶行，不全是那凶神恶煞，瞧着很吓人的，滥杀无辜，一听就毛骨悚然的，更多的……就像那黄风谷的夜间阴风，我们行走无碍，就是觉得不自在，不好受。你将来一定要小心这些看不见摸不着的恶意。知道了这些，不是要你去学坏人，而是你才会对人世间大大小小的善意，更加珍惜，更加知道它们的来之不易。 一定要小心那些不那么明显的恶意，一种是聪明的坏人，藏得很深，算计极远，一种蠢的坏人，他们有着自己都浑然不觉的本能。所以我们，一定要比他们想得更多，尽量让自己更聪明才行。 所有能够被我们一眼看见、看穿的强大，飞剑，拳法，法袍，城府，家世，都不是真正的强大和凶险。 每一个栓不住的自己，果然都会变成另外一个人。你也当引以为戒。 其实人也好，鬼也罢，说什么，任你天花乱坠，很多时候都不如一个事实，一条脉络。 天底下的好买卖，从来不是一本万利的骤然富贵，只会是年年月月的细水长流。 人莫太闲，念头窃起，杂草丛生。太忙，则真性退去，作鸟兽散。所以说啊，身心无忧，风月之趣，很难兼得。 见微知著。一直看着光明璀璨的太阳，心如花木，向阳而生，那么自己身后的阴影，要不要回头看一看？ 豪门府邸，百尺高楼，撑得起一轮月色，市井坊间，挑水归家，也带得回两盏明月。 一时胜负在于力，万古胜负在于理。 一些已经跑到嘴边的伤人话，能不说就不说，切记切记。 人心细究之下，真是无趣。难怪你们山上修士，要时常扪心自问，心田之间，不长庄稼，就长杂草。 不是关系好，喝酒喝高了，就真的可以言行无忌。 多少最亲近之人的一两句无心之言，就成了一辈子的心结。 只是世事往往真话很假，假话很真。 一个孩子天真无邪，童心童趣，做长辈的，心里再喜欢，也不能真由着孩子在最需要立规矩的岁月里，信马由缰，无拘无束。 赤子之心，不是一味单纯，把复杂的世道，想得很简单。而是你知道了很多很多，世事，人情，规矩，道理。最终你还是愿意坚持当个好人，哪怕亲身经历了很多，突然觉得好人好像没好报，可你还是会默默告诉自己，愿意承受这份后果，坏人混得再好，那也是坏人，那终究是不对的。 穷的时候，被人说是非，唯有忍字可行，给人戳脊梁骨，也是没法子的事情，别给戳断了就行。若是家境富裕了，自己日子过得好了，别人眼红，还不许人家酸几句？各回各家，日子过好的那户人家，给人说几句，祖荫福气，不减半点，穷的那家，说不定还要亏减了自家阴德，雪上加霜。 我们每天说什么话，做什么事，真的就只是几句话几件事吗？不是的，这些言语和事情，一条条线，聚拢在一起，就像西边大山里边的溪涧，最后变成了龙须河，铁符江。这条江河，就像是我们每个人最根本的立身之本，是一条藏在我们心里边的主要脉络，会决定了我们人生最大的悲欢离合，喜怒哀乐。这条脉络长河，既可以容纳很多鱼虾啊螃蟹啊，水草啊石头啊，但是有些时候，也会干涸，但是又可能会发洪水，说不准，因为太多时候，我们自己都不知道为什么会变成这样。君子三省，克己复礼，制怒。我想如果做到了这些，心境上，就不会洪水滔天，遇桥冲桥，遇堤决堤，淹没两岸道路。 安身之地，可小。安心之地，需大。于芥子之地寻觅大自由。 世事洞明皆学问，只要有用，又避无可避，不如一早就调整好心态。 自家事，事情可做不可做的事情，可以做做看。说是非，话可说不可说的时候，最好就别说了。 多看多想，就会少错一点，并且能够时时刻刻做好知错改错的准备，生死之外，事事给自己留点余地，留有退路。路子不能越走越窄，不然哪天就突然发现身在一条断头路的死胡同了。 我们永远不知道，当我们走在苦难不堪的泥泞道路上，会不会遇到更大的风雨大雪，会不会遇到一个两个好人，如同一盏盏摇曳灯火。 天底下不用花钱的东西，很难去珍惜，若是花了钱，哪怕买了同样的米粥馒头，也许就会更好吃一些，最少不会骂骂咧咧，埋怨不已。 一些道理就是如此不讨喜，旁人说的再多，听者只要未曾经历过类似的遭遇，就很难感同身受，除非是苦难临头。 但是听不进某些道理的人，其实本就是幸运人。 因为经历过不幸之人，只要遇上了相似的事情，根本无需旁人说道理，早已心领神会。 男子见佳人美色而动容，女子见男子俊俏而动心，皆是颠簸不破的道理，不值得大惊小怪。 有些道理，是用来活命的，以及帮助自己过得更好，而有些呢，是用来安心的。至于哪些道理更好，更适合当下，得看每个人自己的家底和心境，反正我认为都是有用的道理。你以后也会知道这样那样的大小道理，遇到了事情，就拿出来，多想想，再做选择。 看似粗浅的道理，到底还是要亲身经历过，才会深刻，最少也该亲耳闻亲眼见。 人之惰性、贪念等等，多是如此，看似悄然生发，天经地义，实则在旁人眼中，早已有迹可循。 每个读书人，走到了高位上，就该好好想一想良心是何物了。 真正的大事，从不靠聪明。靠……“傻”。 知错能改善莫大焉。 原来真正难处不在改，而是在知。 只要一方始终心平气和，另外一方再满腔怒火，都不太容易被火上加油。 一旦感知到受损，这个人的内心深处，就会产生极大的质疑和焦虑，就要开始四处张望，想着必须从别处讨要回来，以及索取更多。 道理在书上，做人在书外。 昨天的道理会变得没有道理。 这个世道给予你一份善意，不是有一天当这个世道又给予我恶意之后，哪怕这个恶意远远大于善意，我就要全盘否定这个世界。那点善意还在的，记住，抓住，时时记起。 其实每个人长大后，不论读不读书，都会或多或少感到孤单，再聪明一些的人，冥冥之中，能够感知到天地人间，在刹那之间的某个时刻，好像不是寂然不动的，一些扪心自问，会得到一种模模糊糊的回应，愧疚，悔恨。可是这种感觉，绝对不会让一个人的生活，过得更好，只会让人更加难受，好人坏人，都是如此。 道理知道多了，偶尔心会乱的。 书上说，有些人心，就像一把照妖镜，让四周的鬼魅魍魉，无所遁形。 可他却希望自己的本心，只是一盏油灯，在家徒四壁的祖宅，桌上放着它，自己可以通过那点光明，看到那些与自己作伴的尘埃与飞蛾，若是有客人来家里了，便可以看到黄泥窗台上，他在那边摆放着一只粗劣小陶盆，里边有一棵摇曳生姿的小草。 久在樊笼里，复得返自然。 有些别人的故事，不用知道，知道作甚？ 积少成多，聚沙成塔是好事，只是不要钻牛角尖，事事处处吹毛求疵，不然要么心性很难澄澈皎然，要么劳心劳力，虽然筋骨雄壮，却早已心神憔悴。 推动历史踉跄前行的，往往是一些美妙的错误、某种极端的思想和几个必然的偶然。 你其实听进去了，只是暂时不懂而已，可都放在了你心上，比好多大人都要厉害，他们往往吃过亏后，只是学了些为人处世的小聪明。 你们这些涉世未深的小孩子，都是……怎么讲呢，就像是一件最漂亮却有最脆弱的瓷器，未来是登大雅之堂，还是沦为井边破罐，就看教得好不好，教得好，形制就正，教不好，就长歪了。言传身教，又以后者更重要，言传为虚，身教为实，因为孩子未必听得懂大人的那些个道理，但是对世界最好奇，要孩子耳朵里听得进、装得下道理，很难，孩子眼睛里看见更多，更容易记住这个世道的大致模样，比较浅显，黑白分明，稚嫩却尤为可贵，这么潜移默化下去，自己都浑然不觉，点点滴滴，年年月月，心目中的世界就定型了，再难更改。所以好些个看似长大成人后，有违旁人印象的一些莫名其妙的举措，其实早就有迹可循。在一个打磨器型的关键时刻，父母的言行，至关重要，一句做错了事却骂不到点子上的训斥，或是做错了，干脆就觉得自家孩子年纪太小，选择视而不见，最后可不就是害人害己害子女嘛。所以要赏罚分明，父母要学会给子女立规矩。仁义，理之本也。刑罚，理之末也。 所以在小的时候，父母以身教子女仁义，稍大一些，学塾先生教弟子书本上的仁义。两者相辅相成，前者往实处教，后者往高处教，缺一不可，相互拆台更不行。知道什么人相对比较愿意听人讲道理？一开始，孩子听父母。随后学生听先生。长大后，弱者听强者，贫者听富者，臣子听君王，又比如山下听山上，山上听山顶。那么问题来了，强者若是说的不对，弱者却将强者的所有言语道理，死心塌地奉为圭臬，怎么办？道德仁义，已经很难有效了，就需要有法，世上得有一种东西，比山上的所有仙家术法，更让人感到敬畏，让所谓的强者都束手束脚，让这些人像犯错的孩子畏惧父母的训斥，像是教书先生的鸡毛掸子和戒尺，一犯错就会立即敲在手心，知道疼。 读书人气节一事，本就唯有苦难砥砺而成。 时时刻刻都讲究表面上的待人以诚，对谁都掏心窝子，财帛动人心，反而只会让江湖更加险恶。真正的待人以诚，自是一件很美好的事情，但是如何呵护好它，不伤人不害己，就需要自己积攒江湖阅历了。 世间事世间人，世事人心，看似复杂，其实只要瞧得见极其细微处，皆有脉络可循。 你这个年纪，总有做不到，或是努力做了，也做不好的事情。有什么关系呢，没关系的。 可做得不好，与做错，是两回事。岁数小，犯了错不用怕，可这不是知错不改的理由。 世事玄妙，在饮啄间。 非此即彼，非黑即白。一把尺子的两端。尤其是涉及自身切实利益的事情，好似这才是人之常情. 翻多了史书，就会发现历史就是这么兜兜转转，朝堂，江湖，都一样，画圆圈。偶尔出个道德圣人，武学天才，那就走出去一点，圈子大一些，后边的人继续转一圈，偶尔也会拐来拐去，没个边儿。 我们所处的世道，总是这般复杂，走着走着，杂草丛生，荒庙破寺。走着走着，杨柳依依，桃花烂漫。走着走着，穷山恶水，夜幕深沉。走着走着，琼楼玉宇，大放光明。 孩子，少年，长大成人，我觉得大概都会有三个阶段吧，小草柔弱，但是根子一定要扎得牢固。稍有风吹，便是草动，其实这没什么，青草依依，摇来晃去嘛。接来下就是如山野青竹，有人厌恶，扬言要斩恶竹万竿，但又有读书人很喜欢竹子，之后才是青松挺且直。 世间道理，其实一直在，有人捡起，奉若圭臬，视为珍宝，有人不屑，甚至还有人会踩上几脚。这不是道理不对，不好，而是人心出了问题。 世间事，皆有脉络可供观看，世上人，所思所想皆有迹可循。 人生道路上，有些明知道是危险的坎，亲身涉险都是对的，可有些诱惑，就得听从那句老话了，命里八尺莫求一丈。 任何一位真正的强者，应该以弱者的自由作为边界。 人间悲欢离合，千千万万，各有苦衷福缘，世间没有两片相同的叶子，不会有相同的一条河流。 可有些道理是相通的。 遇见世间不平事，而认为是不平事者，意最难平。 欲多则心窄。 只是万般难事，可多思量多琢磨，却不可过于忧虑惊惧，否则就只能是自乱阵脚。 只不过书上故事，那些悲欢离合，纸上看来终究浅，淡，轻。看过就看过了，很快就会忘记的。可是人活着，饿得肚子咕咕叫，脚底磨出了水泡，给人打了一拳鼻青脸肿，都是实实在在的。 有些事，旁人可劝，有些事，不好劝。 许多时候，人心无需用眼看。 第一篇，分先后，世间事皆有脉络，来龙去脉，不可跳过任何一个环节，只拣选自己想要的来讲道理，不然世间万事，永远说不清对错，不然就成了只有立场而无对错，好似世间人皆可怜，都可恨？那还怎么真正讲理？难不成各说各话，道理说不通之后，仍是只能靠拳头说话？大谬矣！第二篇，审大小。对错有大小之分，便需要将法家之善法，和术家之术算，这两把尺子，借来一用。第三篇，定善恶。以礼仪规矩作为根本准绳，结合各地乡土风俗人情，以及人心道德，定人是非和功过，扪心自问善与恶。第四篇，知行合一！错则改之，无则加勉。 读书不在多，只看读进自己肚子有几字。 人心不是街面，能够一场大雨过后，就一下子变得干干净净。 吾心安处即吾乡。 人间悲欢，看过了一遍几遍，与看过了千万遍，是截然不同的观感。 行于水中，不避蛟龙，此是船子之勇。行于山林，不惧豺狼，此乃樵猎之勇。白刃交于身前，视死若生，此乃豪杰之勇。知人力有穷尽时，临大难而从容，方是圣人之勇。 每天必须吃饭，是为了活下去。在衣食无忧的前提下，读书讲理，不一定是为了做圣贤，而是为了让自己活得更好一些。当然，不一定真的更好，但是儒家圣人们的经典教诲，世世代代君子贤人们的金玉良言，最少最少，给了我们一种最‘没有错’的可能性，告诉我们原来日子可以这么过，过得让人心安理得。 道理和真心话，总是越说越明了的，如一面镜子时时擦拭，抹去尘埃，便会越擦越亮。 你如何看待这个世界，世界就会如何看你。你看着它，它也在看着你。 读过多少书，就敢说这个世道‘就是这样的’，见过多少人，就敢说男人女人‘都是这般德行’？你亲眼见过多少太平和苦难，就敢断言他人的善恶？ 万事莫走极端。与人讲道理，最怕‘我要道理全占尽’。最怕一旦与人交恶，便全然不见其善。 唯有读第一流的书，才有希望当第二流的人。 做生意，熟人登门，绝不可以杀熟，但是也不可以不收钱，不赚不亏，是最好的。否则越做生意，就越没朋友。你次次亏本，那人还喜欢时时登门，证明对方不把你当朋友。你次次赚得比平时还多，那就更明白了，你根本不曾将那人当做朋友。若是这般，反而爽利。若是前者，就要揪心了。 与女子相处，希望自己尽善尽美，未必是真喜欢她，与男子交往，能够全然不在乎自己的缺点，以诚相待，多半是真把他当朋友了。 一个随便把别人当朋友的人，往往不会有真正的朋友。 一个喜欢嘴上称兄道弟的人，心里其实没有真正的兄弟。 知道下棋的最高境界是什么吗？身前无人。 事有先后，对错分大小，顺序不可乱，之后才是权衡轻重，界定善恶，最终选择如何去做一件事。 不近恶，不知善。 退一步说，因利而聚的一群人，形势占据上风，那是人人猛如虎，可只要落了下风，那就是人心涣散，沦为乌合之众。 儒家的道理，从不在高处，不在到底有多高，而在道理是否落在了实处。 悲伤很难感同身受，快乐的分享总是一闪而逝，人生只是一场场告别…… 大道之上，运气很重要，但是接不接得住，更重要。福祸相依，天才早夭的例子，不计其数，便是此理。 斗茶之茶，要新。手法和茶具，要古。煮茶泉水要清且重。饮茶之人，要净且灵。 一桩没来由跑到自己跟前的缘分，不是孽缘就可以了，不用刻意追求善缘。 对世界不要失去希望，除了一定要好好活着之外，其实还有一层意思，就是当我们对这个世界给予善意后，如果非但没有得到善意的回报，甚至只有恶意，这个时候，能够不失望，才是真正的希望。 但是两看相厌，不等于可以只看对方惹人厌的地方。两看欢喜，则一样不可以只看到好的地方。 一个人的本性单纯淳朴，完全不等同于憨傻迟钝。要做真正的好人，得知道什么是坏人。一个好人能够好好活着，就是对这个世界最大的善意。 不妨试试看，反正事情已经不能再糟糕了。而有些事，不是你想躲就躲得掉的。人生在世，遇到一个坎不怕，努力走过去就是了，过不过得去，两说，你好歹尝试过。 我如此有钱，不是如何了不起的事情，但也不用跟人故意拿捏，刻意放低身价。与认定的朋友相处，从内而外，真正做到了平起平坐， 这才是真正有钱人该有的样子。 人生有起有落，官场商场，以及修行路上，有人人生的落，就有可能是别人人生的起。 有些委屈，无论大小，只有受过同样委屈的人，才可以真正体会。 否则旁人再好的善心善意，恐怕都无法让人真正心安。 衣食无忧的太平岁月里，年轻人才会觉得事事不如意。等到真正的事情临头，才会知道之前的种种不幸，亦是万幸。 混江湖的，恩怨情仇，都讲究一个青山绿水，来日方长。 那个老书生说，我们活在一个很复杂的世道里，很多人的言行，哪怕是学问极高的读书人，还是会自相矛盾，我们看多了没甚道理的事情，难免会问，是不是书上的道理，是错的，或者说，是那些道理还没有说透，没有说全。那么问题来了，怎么办呢？我们该怎么看待这个许多嘴上讲道理、做事没道理的世界？办法是有的，一种是活得纯粹，我拳头很硬，剑术很强，道法很强，就用这些来打破一些东西。复杂问题给简单解决掉，只要我开心就好。天地有规矩约束我，我便一拳打破，世间有大道压我，我有一剑破万法。哪怕暂时做到如此酣畅淋漓，可总是如此想，坚定不移，一直朝这个方向走在道路上。这种人可以有，但是不能人人如此的。一种是活得很聪明，怎么省心省力怎么来，规矩二字，就是用来钻漏洞的。读书人若是如此，便是犬儒了。或者在合情合理之间作取舍，选择合自己的情，不合世间的理，以至于熙熙攘攘，皆为利来利往，若是能够把这个‘利’换成‘礼’字，世道该有多好。最后一种是活得很没劲，把复杂问题往更复杂了想，掰碎道理，仔细梳理，慢慢思量。最后想明白一个为什么。可能做事情，绕了一个大圈，竟然发现只是回到了原地，但是真的没有用吗？还是有的，想通了之后，自己的心里头，会很舒服。就像……就像喝了一口陈酿老酒，暖洋洋，美滋滋。我们读书人推崇的儒家圣人们，其实没世人想得那么至善至美，是人味十足的，但是儒家的真正学问，却也绝不是那么不堪，哪怕不认同人性本善四个字，没关系，可到底是能够劝人向善的。 吃得住苦，享得了福，才是真英雄。吃苦头的时候，别见着人就跟人念叨我好苦哇。享福的时候，就只管心安理得受着，全是自己靠本事挣来的好日子，凭啥只能躲在被窝里偷着乐？ 是你的就好好抓住，不是你的就不要多想。 天底下没谁是欠你的，但是你欠了别人，就别不当回事。 看似好心的善举，未必是好人好事情。 比如斗米恩担米仇，比如看似鸡毛蒜皮的琐碎小事，最消磨孝心善心。 人是未醒佛，佛是已醒人。他写了欸乃一声山水绿。还写了夫子之道，忠恕而已矣。 读书百遍其义自见，读书破万卷下笔如有神，听上去很虚，其实半点不虚，说的就是一个勤字，熟能生巧，巧出玄妙，循序渐进，便知道了，知道了一法，一法通万法通，万法皆成。 不精不诚，不能动人。脚踏实地，自然而然。 吃亏是福，亏先吃着，要相信以后不会总是吃亏的。 坐而论道，是很好的事情。但是别忘了，起而行之，则更重要，否则一切道德文章就没了立身之处。 老先生坐而论道，少年郎起而行之，善，大善！ 井底之蛙，偶见圆月，便欣然忘忧。 有心就好，万事不难。 千里之行始于足下。以后三餐要准时吃。 行到水穷处，坐看云起时。到时候你自然而然会知道答案。 觉得对我来说，烧香拜佛也好，礼敬菩萨也好，都要自己先做到力所能及的事情，如果仍是达成不了心愿，实在没办法了，再去求，菩萨才会点头答应，要不然人家菩萨凭啥帮你啊，对吧。” “求佛先求己。” 就是任何自己觉得不好的事情，就干脆不要有第一次，一次也不要做，一小步也不能走出去，要不然回头来看，吃亏吃苦的还是自己。 人生于天地间，路要自己走，话要自己说，人要自己做。 情深不寿，慧极必伤。 你只是没有做得更好，而不是做错了。 该是你的，就拿好别丢。不该是你的，想都别想。 凡夫俗子不下苦功夫、死力气去努力做成一件事，根本就没资格去谈什么天赋不天赋。 只有真正努力之后的人，才会对真正有天赋的人，生出绝望的念头，那个时候，会幡然醒悟，留着眼泪告诉自己，原来我是真的比不上那个天才。 世间很多事情，珍贵之处不在结果，而在过程。 不迂腐的好人，他们的人心，会格外温暖灿烂，如向阳花木。 心境如镜，越擦越亮，故而心境修行，能够在道祖莲台上坐忘，当然大有裨益，可是偶尔在小泥塘里摸爬滚打，未必就没有好处。 人心是一面镜子，原本越是干净，越是纤尘不染，越是经不起推敲试探。 有些东西暂时没有，可以用钱挣来，可有些东西没了，这辈子就真的没了。 愿意把陌生人的些许善意，视为珍稀的瑰宝，却把身边亲近人的全部付出，当做天经地义的事情，对其视而不见，这是不对的。 千年暗室，一灯即明。 混吃等死，小富即安，飞黄腾达，是因为各有各的缘法，未必有高下之分。 到了外边，多做事少说话，管住自己这张嘴巴，吃些亏就吃些亏，别总想着嘴上讨回便宜，外边的人，不像我们，会很记仇的。 记住，以后不管遇到什么，你都不要对这个世界失去希望。 遇见不幸事，先有恻隐心，但是君子并不是迂腐人，他可以去井边救人，但绝对不会让自己身陷死地。 人吃土一生，土吃人一回。 道理全在书上，做人却在书外。 官场嘛，山上山下都一样，既怕不说不做是个错，更怕说错做错更是错。 想来一个男人在外辛苦挣钱的意味所在，就在于此。给值得花钱的人、在值得花钱的地方花钱。 一个男人，有了家庭，过日子，千万别让自己媳妇一直为难。所有的婆媳矛盾，如果哪天闹到了不可调和的地步，说到底，肯定还是那个男人，不靠谱，没主见，只会捣浆糊，才会落个两边不讨好。 各自修行，难免聚少离多，今天再跟你唠叨几句。一个男人，最好能够先对自己负责，再对整个家庭和更大的家族负起责来，最后，要是还愿意的话，再对这个世道，做点有意义的事情。如果一件事有意义的同时，还能让做事情的人觉得有意思，就更好了。做事情就得思前想后，谋而后动，偶尔遇到难关，不妨作退一步想。 只能把话关在心扉内，就叫不开心。愿意把话送出心门之外，就叫开心。 人生不求十全十美，偶有美中不足，月未全圆花半开，不是很好么。 听君诚心一席话，真觉娉娉袅袅。 可能所有自由的让渡，都在追求一个最大公约数。 当一件事需要我们去质疑、否定身边家人的时候，就一定是带着情绪的，难免会说一两句重话，有用吗？可能有用，但是更多可能是让事情变得更加糟糕，吵着吵着，自说自话，吵到最后，早就不是事情本身了，开始翻旧账，为自己的对，找种种理由，或是用某个对，否定对方的对，如此一来，我们当真可以‘就事论事’吗？男人都喜欢讲理，女人都注重感受。男人要多想一些。一个男人，如果始终想不明白，女人那边看似莫名其妙、不可理喻、无理取闹的那些奇怪情绪，本身就是一个道理，那就很难讲明白自己的道理喽。就更不用说讲理只是为了争个输赢，有个胜负，双方如此久处，自然而然，都会觉得对方是一个无法沟通的人。同床共枕的夫妻双方，逃无可逃，避无可避，大概最终就只有两两沉默、各自委屈了吧。我们对别人，对这个世界，所有的误会，可能都来自三个字，‘我觉得’。 人人都是个懒散鬼，天生有惰性的，所以我一直觉得书上的某个道理，或是从旁人嘴里听来的语句，所有那些一听就让人觉得轻松的道理，很难让我们的生活过得更好，好的道理，反而是一开始听着就会让我们倍感不适，做起来更难受的道理。 一个看似很简单的道理，到底需要用多少个道理来支撑呢？好像有太多的事情，就是一个只有一个确凿数字的加法，那么少了其中任何一个道理，答案就一定是错的。 有些不愿开口与人说的委屈，来自得不到身边人的回应，种种期许、憧憬、愿望之心声，在心中如擂鼓，响彻自己天地间。心外却哑然，永远寂静无声，这就像一个人把嗓子喊哑了，身边还是无人听见，这个人就会越来越不喜欢说话，一直沉默下去，直到变成一个哑巴。所以，先别管有理没理，对错是非，一定要愿意跟旁人说出自己的想法，为什么要说某句话，为什么要做某件事，直白无误告诉对方，我是这么想的，你觉得呢？ 道理总得有个落脚地，不然晓得了一箩筐的大道理，除了背着行走，除了受累，又有什么用处。不要死要面子活受罪，于人于己，都多些耐心，与身边亲近人，要敢认几个错，肯说几声对不起。尤其是没有害人之心、对这个世界充满善意的好人，尤其要注意自己的性格，一定要控制好情绪，不要给人、尤其是亲近人那种阴晴不定、喜怒无常的印象，不然有理也没理，到头来就太吃亏了。有个说法，形容一个人无缘无故的怒气，叫无名之火，名称的名，其实也可以形容为无明之火，明亮的明。想来一个人所有的委屈，点点滴滴积攒而来，只会积少成多，只是鸡毛蒜皮的琐碎事情，都转为很难自知的情绪了，自以为无所谓了，哪能呢，那是纸包不住火的。这种不自知，大概就叫无明。当我们想的太多，做的太少。如何能够一分耕耘一分收获呢。可如果做的太多，想的太少。又怎么可以保护好自己的善心。我们人啊，过日子，可不能总觉得自己已经很努力了。但是也不用害怕，同在一处屋檐下，所有发泄出来的恼火，都是有温度的。只要让旁人知晓，不要憋在心里，当然，也不要烫伤别人的人心，所以除了让对方知道自己是怎么想的，同时一定要知道对方是怎么想的，先别管双方的对错，各自有无道理。这里边有个小小的诀窍，就是别跟子女之外的亲近之人去就事论事，当然，对孩子，家教，立规矩，一定要没道理可言，某些事情就该如此这般，孩子能理解是最好，不能理解就照做，比如出门在外，见着长辈就得打声招呼，做错事得为了那件错事本身去跟人认错，而不是什么你这么做了，对方会不高兴，或是爹娘不高兴了，为人父母者，也不能代为认错。 男女之间，结为夫妇，是缘，无非是分出个孽缘和善缘。头等孽缘，此世此身，相互折磨，纠缠不休并不分开，长久心怀怨怼而终，还会延续至下辈子。中等孽缘，双方将就过日子，总不满意，觉得相互亏欠，那么贫寒富贵，不管有钱没钱，日子总是不快乐的。稍轻几分的孽缘，中途不欢而散，双方之间倒是没有太多怨恨心，缘浅，缘尽使然。唯有善缘，相互成就，白头偕老。那么所谓修行，不过是将心比心，将孽缘转为善缘，将此生善缘延续为下辈子的善缘，那么不管下辈子是以何种身份重逢，便会如见故人，心生欢喜。所以夫妇之间，想要白首同心，把日子过得好，起先是孽缘，那就解孽缘，结善缘，本是善缘，就更简单了，无非是续善缘；父母子女之间，是债。子女们来此世间，与父母或讨债，或还债。若是子女为讨债而来，那么做父母的，就要赶紧还债，越早还清越好。所以你会发现这世上，有些长辈明明都是忠厚人的殷实门户，偏偏就会出现个不可理喻的败家子。若是子女此生为还债而来，为人父母者，也当珍惜，不可挥霍。所以你也会看到一些门户，不管那些父母如何言语刻薄、行事自私，当子女的，总是过日子再辛苦，自己受了再大委屈，都还是愿意尽孝道。当然也有些子女，能够让一个原本贫寒的家庭就此福分生发，这就是他们的还债了。你以为天底下很多有了子女的夫妇，他们当真知道如何为人父母吗？其实是一开始都是不知道的，既然都是此生头一遭的事情，当爹做娘的，要么未曾做好准备，要么根本不知如何作为，总是有些糊涂的，于是我们足不出户，早早在自己家中，就有了可以为之哭、可以为之笑的悲欢离合。 若是所有心中美好，都成为了一种负担。那么美好的意义何在，如果如此，肯定是我们有哪里做得不对了。 公道求之，自有宽路。 远离一切颠倒梦想。 知道很多个为什么，却都无法解决问题，可能恰好才是痛苦的根源。这其实也是许多读书人的症结所在。心中知道的是一条道路，脚下行走的是另外一条路。既是知行不一，追本溯源，其实就是身心不一，身在此处，心在彼处。故而越是心思细腻者，越有撕心裂肺之痛苦。说错的话，做错的事，后悔的感觉，在身旁那条可望不可即的道路上，迤逦绵延成一线，教人不堪回首，不敢转头看。 自古观书喜夜长。 一个人侥幸生逢盛世，百虑可忘，若是再精通种植花草之术，宛如四时皆春，可教人不知老之将至。 人之双眼所见即天地。 所有习惯本身，就是一种自找的遗忘。 历来登顶天地宽，人间春色从容看。 如今的生活，我觉得就是最好的了。 有书真富贵，无官一身轻。 只有做好最坏的打算，才有资格期待那个最好的结果。 只要你们还在追求那种纯粹的自由，那么你们最大的敌人，就不是规矩了，而是所有他人的自由。 世间可有一法，可解万般愁，安顿无限心，心定莲花开。 真正的委屈，只会委屈得教人不知该不该流泪。 混账话，糊涂话，玩笑话，轻巧话重话，打开天窗的亮话，盖棺定论的明白话。 有事直接说，不管是大事小事，宁肯当场吵架，惹来对方的不高兴，也绝对不给“误会”留出丝毫余地。 我觉得姐姐可能就只是在做一件自己真心喜欢的事情吧，别人眼中的结果如何，好像不是那么重要，又可能这个过程就是最好的结果。 我与我之外，即是天地之别。有人与这个世界有过情人一般的旖旎和争执，也有人与这个世界有过仇人一般的怨怼与和解。 说话，得留点余地。说话结实，跟个糯米团似的，好吃是好吃，就是容易撑到，不如一碗白米粥，养胃。 一棵参天大树，有些原本粗壮的树枝会在风雨中腐朽剥落，有些纤细枝条，却会逐渐成长为粗壮的枝干，再生长延伸出更多的枝丫，绿叶葱郁，供后世子孙乘凉者，就是祖荫福报。 我们如何看待这个世界，这个世界就如何看待我们。 人生可能没有真正的同悲共喜，大概就像两个人，就是两座天地。各有所思，你情我愿，此消彼长，教人间没个安排处。 你想说什么，我当然知道，可那只是我们想的，我真正在意的，是XXX自己怎么想的。 做买卖的人，有自己的生财之道，自古而然，只是生意人，归根结底还是做人，还是要讲一讲底线的。”“买卖想长久，跟着大势走。”“可要是亏心事做多了，人不收天收。 有些人，历经坎坷，总能峰回路转，柳暗花明。但是有些人生如船搁浅，水道提纲如一线，进不得，退也不得，原地鬼打墙。好像做多错多，就只能破罐子破摔。 一个每天把无所谓摆在脸上的人，可能才是真正有所谓的。 一个人，如果连做梦都不敢了，得多苦啊。昔去花如雪，今来雪如花，良辰美景总不虚设，如何安顿无限心。可能我们都与这个世界，有过情人一般的缱绻，互为仇寇一般的怒目相向，聋子与瞎子一般的自说自话，无话可说之人与不可言说之人，相对而视，哑口无言。 有心为善与无心为恶两事。 违心的事情，不要做。发自本心的事情，但是有违江湖道义的事情，也不要做。今日做不成，未来有望做成的事情，切不可为达目的不择手段，不要着急去做。 过尽千帆皆不是，当时只道是寻常。 既然已经想了那么多，还想那么多做什么。 多少人来看明月，谁知倒被明月看。 一个心里边装着很多人的人，就容易心肠软，看待世界的目光太温柔。 一个人知识上的充沛，会给自身带来一个巨大陷阱，计算力和智力上的优越感，那种习惯性居高临下看待所有人的眼光，迟早要出问题，大问题！ 心怀远望又谨慎之人，能成大功。秉性忠良敦厚之人，可托大事。 想要省心省力，就得花大价钱，用足够的钱填平人心大坑。 挣钱小心，花钱大方，自家钱财不管多寡，都从正门出入，就是一家门风所在。钱要挣，积德也别耽误。不然夜路走多了，偏门财攒得越多，就越容易出事情，还会祸及子孙。世间钱难挣，祖荫福报更难积攒。 一穷二白的时候，挣点偏门钱，以此发家，无可厚非，等到有钱了，就得挣正门钱了。否则德不配位，坐拥金山银山，福祸转换只在一夕之间，钱算什么，前人田地后人收。 大钱是上辈子带来的，书是给下辈子读的。 可能我们都曾对这个世界感到失望，但是我们都愿意对这个世界寄予希望。 我们都是一盏灯火，在天地间忽明忽暗。言行互为卯榫，人心共作灯火。搭建屋舍，抱团取暖。 始终被他人寄予希望，会让自己觉得不孤单。 畏强者凌弱，媚上者欺下，很难有例外之人事。你要是没有与强者心平气和说道理的心气，就定然会对弱者容易失去耐心。 何谓豪杰，总有那么几件事，天下人都做不到，我做得。何谓圣贤，总有那么几件事，天下人都可做，我做不得。 放开眼界看，世上几百年旧家无非积德行善，头顶三尺有神明。理当如此说，天下第一件好事还是立志读书，功夫不负苦心人。 “吃年夜饭，再跟人一起守夜，无法想象的事情。”——“等到新鲜事不新鲜了，还能照旧，才算是件无法想象的事情。” 学者立身希圣希贤，释者发心成佛成祖。取法乎上，仅得乎中，总是先有一等心思才能有二等人三等事。 人生路上，有时接纳一个极有分量的道理，哪怕这个道理再好，就是一个登山之人的背篓里增添了一块大石头。会让人步履蹒跚，不堪重负，苦不堪言。 万事尽量从最坏处打算，未雨绸缪，思虑周全，之后一切，就都可以视为往好处好一点点转变之事了。 苦中作乐，只是处世法，苦不自知，才是立身道。 是一个“书本上不说，老话都不提”的狗屁道理。有些自愿去做的好事，那么行事之人，最好别把好事当做一件好事去做，就可以为自己省去许多麻烦。既符合书上道理所谓的君子施恩不图报，关键是可以保证未来不管发生了什么，都不会有任何失望，再有他人之回报，就都是意外之喜了。 天底下不少好人做好事，好人是真，好事也是真，唯一问题，在于他们兴许可以不求利字之上的丝毫回报，却难免会索求他人人心之上的某种回响，一旦如此，那么在某些被施恩之人眼中，甚至还不如前者来得清爽、轻松。 穷人的钱财，就是手心汗，不累就无，累过也无。 攒钱给子孙，未必是福，接不住还是接不住，唯独行善积德，留给子孙的福报，他们想不接住都不行，最重要的，是老话说，家家户户都有一块田叫福田，福田里边容易生出慧根，所以余给子孙一块福田，比什么都强，比钱财，甚至是比书籍都要好。 何谓遗憾，不可再得之物，不可再遇之人，就是遗憾。 我们的言语，既会千山万水，迷障横生，也能铺路搭桥，柳暗花明。故而与亲近之人朝夕久处，不可说气话，不可说反话，不可不说话。 可能是因为人间所有的悲欢相通，都只会来自感同身受。 心里多知道，嘴上少说道。 着手处不多，用心处不少，还是很辛苦的。 小有早慧，老有晚福，是两大人生幸事。一个靠上辈子积德，一个靠这辈子行善。 知其不可奈何而安之若命，德之至也。 当某一人太过瞩目，其余人等，难免黯淡失色，旁人要么生出惰性，躺在大树底下好乘凉，要么容易提不起心气。 别人愿意说几句心里话，就得好好记住，不能听过就忘，因为天底下好听的心里话，其实不在嘴边，在眼睛里边呢。所以听在耳朵里的心里话，往往就不那么好听了，一来二去，要是总记不住对方说什么，脾气再好的人也要当哑巴了，同时还要让自己不往心里去，不然以后就没人愿意跟我们说心里话喽。 行路窄处留一步与人行，便是行大道，滋味浓时减三分让人尝，便是真滋味。 师父觉得最远的路程，都不是什么去远方，不是去大隋书院，甚至都不是去剑气长城，是师父的小时候，在山上遇到了一场暴雨，然后隔着一条发洪水的溪涧，师父在一边，回家的路，在另外一边。 人生总是冷不丁的，来上那么一拳，不轻不重的，只是让人无力招架，大概这就是所谓的无力之感了。 很多的自欺欺人，在外人看来是可悲可笑的。但是对当局者而言，是幸运美好且是必须的。 看得真切，记得牢靠，才能真正记得打念得好。 一句顶美好的言语，只要被人在耳边唠叨千百遍，就要变得俗不可耐，面目可憎。 强扭的瓜，蘸了蜂蜜糖水，吃到最后，还是苦的，先甜后苦最麻烦。 不付出，就不会珍惜。付出越多越在意。跟好人坏人没什么关系。同样一壶酒，不管原因为何，涨价了还是降价了，喝出来的滋味，喝酒的快慢，都是不一样的。 一个经历越多、攒下故事越多的人，心狠起来最心狠。 人心不是水中月，月会常来水常在。人容易老心易变，人心再难是少年。 以前我总喜欢听好话，听不得半句不好听的。后来遇到了老爷，他就跟我说，好话坏话都会听着的，都别太当真，何况十句好话，往往给一句坏话就打死了。所以每听人一句好话，让我就先余着九成，到时候攒够了好话，就可以等那一句坏话登门做客了，半点不伤心。 世上人，少有这么算账精明、晓得自补心路的，都喜欢只拣好听的听。不然就是富贵得闲了，吃饱了撑着只挑难看的看。 许多时候，看见了一部分的真相，最让人自以为是，只不过寻常人越自以为是，活得越轻松就是了 有心者有所累，无心者无所谓。 所有久别重逢的开怀，都将是未来离别之际的伤心。 所有能够言说之苦，终究可以缓缓消受。唯有偷偷隐藏起来的伤感，只会细细碎碎，聚少成多，年复一年，像个孤僻的小哑巴，躲在心房的角落，蜷缩起来，那个孩子只是一抬头，便与长大后的每一个自己，默默对视，不言不语。 人生路上，许多人都愿意自己朋友过得好，只是却未必愿意朋友过得比自己更好，尤其是好太多。 独处容易让人生出孤单之感，孤独却往往生起于熙熙攘攘的人群中。 今日事之果，看似已经了解昨日之因，却往往又是明日事之因。 人的肚子，便是世间最好的酒缸，故人故事，就是最好的原浆，加上那颗苦胆，再勾兑了悲欢离合，就能酿造出最好的酒水，滋味无穷。 君子之心，天青日白，秋水澄镜。君子之交，合则同道，散无恶语。君子之行，野草朝露，来也可人，去也可爱 心田之间，田垄分明，行走有路，仿佛每一步都不逾越规矩，每天都能够守着庄稼收成，如此约束人心，好事自然是好事，却会让一个人显得无趣 与人坦诚相待，不是时时刻刻掏心掏肺，一方掏出去了，对方一个不小心没接好，伤人伤己。 礼轻情意重，关系没好到那个份上，才会在礼物礼节上过多客气，真是朋友了，反而随意。 明则有王法，幽则有鬼神，幽明皆浑浊，良知还在心。天地乾坤，日月光明。 年轻时记性好，每逢思乡，人事历历在目，心之所动，身临其境，宛如返乡。 上了岁数，记忆模糊，每逢思乡，反而感觉离乡更远。人生无奈，大概在此。 一年逢好夜，万里见月明。 自知者不怨人，知命者不怨天。 碎碎平碎碎安，碎碎平安，岁岁平安…… 初念浅，转念深，再转念头深见底。此念渐深，见得人心，未必见得本心。 很多时候，过分的聪明，本身就是一把无鞘无柄的长剑，出剑伤人，握剑伤己。 山上传闻，真真假假，山水邸报之上，一些个大义凛然言之凿凿的言语，反而就那么回事，一部分真相，只会远离真相，倒是某些三言两语一笔带过的，反而藏着余味无穷的浩然正气。 人心如水，所以与人交心，就是涉水而行，或小河溪涧，清澈见底，或江河滚滚，浑浊不堪，或古井深渊，深不见底，一着不慎，就会淹死人。 时时在法中，处处法无碍。 人心炎炎酷暑中，可得一剂清凉散。 与虎谋皮，是火中取栗之举。但是君子之交，才是天高月白。 有些道理，其实是真懂，只不过懂了，不太愿意懂。好像不懂事，好歹还能做点什么。懂事了，就什么都做不成了。 热闹处守口，僻静时守心。 冬天的积雪，是落在夏天的贫家子身上的一件狐裘，好看是好看，就是穿着难熬。 一个人的气清气浊，其实就看有无一颗平恕心。 只能活在别人心中，活成另外一个自己，一定很辛苦。 书上将道理说破了，好像很简单。只可惜人生各有症结，太难知道一个自己不知道了。 一家和乐，即是大年。 一个人的学问多寡，很其次，做人其实最怕拎不清。 心地就是福田，言行就是风水。所以要懂得惜福，要能够藏风聚水。 桌上灯半黑，窗外月半明，有人觉得不够亮，有人觉得不算黑。 很多时候，一个人的眼睛里，脸上的细微处，那些未说之话，反而比开口所说言语，更接近真相。 肚子坏水晃荡来晃荡去，归根结底，得有一颗坏胆撑起那份胆识。当一颗坏胆给彻底碾碎了，变成满是苦胆苦水，坏人就会老实很多。 何谓失望，无非就是万般努力过后，不得不求，求了没用，好像与天地与人求遍都无用。 新一辈江湖人的为人处世，往往劝酒只是为了看人醉后的丑态。老江湖，是自己酒不够喝，才会劝酒不停，让朋友喝够。或是不缺酒水的时候，劝酒是为多听几句心里话。可能每个老江湖，都像个酒缸，装满了一种酒水，名为“曾经”。 大事心静，小事心稳，有事心平，无事心清。 人生路上，真正的过失，错过和失去的，不是什么擦肩而过的机缘，不是失之交臂的贵人，而是那些原本有机会改正的错误，然后错过就失去。 为人处世，安身立命，其中一个大不容易，就是让身边人不误会。亲近之人，若想久处无厌，就得靠这个“明明明白”，不会因为诸多意外，或是种种琐碎事情，某天突然让人觉得“你原来是这样一个人”。其实许多误会，往往来自自身的捣浆糊。 人生不能总是处处事事迁就他人，不然老好人一辈子都只能是个老好人。往往老好人的问心无愧，就会让亲近之人吃亏吃苦。 真正的书生意气，不是什么都不懂，就偏要与所有老规矩、风俗为敌。 而是很多都懂了，我再来无所谓，单凭自己喜好，说话做事，来跟这个世道，毫不圆滑地打交道。 玉在山而草木润，渊生珠而崖不枯。 许多偶然，实则必然。但是一连串的必然，又会出现万一和偶然。 每一个生性乐观的人，都是主观世界里的王。 那么一个天生悲观的人，就更需要在心境的小天地之内，构建屋舍，行亭渡口，遮风挡雨，停步休歇。 良心在夜气清明之候。 君子立业，贫不足羞。 富贵门户，常有穷苦亲戚来往，不曾空手而返，便是忠厚之家。 路过高门，百姓不会如避灾殃，刻意快步走过，正是积善之门。 无心为之，最是有心。 与亲近之人，不要说气话，不可说反话，尤其不要不说话。 一个大老爷们，记仇确实不好，不大气。多半是修心不够。 很多时候咱们不得不承认，不是所有事情，都可以未雨绸缪的，还真就只能事情来了，再去解决，才能解决。 坏事不怕早，好事不怕晚，早点与之面对，才好早做准备。 只有偷着乐的乐呵，最值得乐呵。 安稳日子过久了，难免乏味，这是人之常情。人间乐事如饮醇酒，往往醒来就无，极难留住，唯有失落，倒是苦事如茶，往往有机会苦尽甘来，让人倍感珍惜。平淡事就是喝水了，没什么滋味，可就是每天都得喝，不喝还不行。 世上人事无穷，我辈光阴有限，可能正因为如此，所以我们才会更珍惜人间这趟逆旅远游。 每个结局伤感的故事，都有个温暖的开头，每年的大雪隆冬，都是从春暖花开中走来。 何谓自由，就是我们下雨天出门，手里边有把伞，唯一的不自由，就是得撑着伞，别走出伞之外。 很多好道理为何会空，因为说理之人，其实未曾感同身受，与听理之人并未悲欢相通，无法真的将心比心。 一张白纸最易下笔，稚子都可以随便涂抹，一幅画卷题跋钤印无数，好似布满牛皮癣，还让人如何落笔，两者各有好坏吧。 吃了亏就长点记性，不然就白吃顿苦头了。下了山出门在外，不是爹不是娘的，谁也不会惯着谁。 生活不是处处屠狗场，没那么多狗血。 世道又处处是屠狗场，遍地洒落狗血。 粗粮养胃，糙话活人。 力能胜贫，谨能胜祸，年年有余，每年年关就能年年好过一年，不用苦熬。 约莫是每个人都会有自己的主心骨，行走在复杂的世道上边，帮助我们用来对抗整个世界。输了，就是苦难。赢了，就是安稳。 每个人都是各自生活的写书人，与此同时，看别人就是翻书。 可能世界把我们看得很轻，但是我们又把自己看得太重。 一个人的理性，是后天积累的学问汇总，是我们自己开辟出来的条条道路。我们的感性，则是天生的，发乎心，心者君主之官也，神明出焉。可惜人为物累，心为形役。故而修行，说一千道一万，终究绕不过一个心字。 人与人两心不契，稍有间隙，便如隔山川，不可逾越。阿良曾经说过，世间言语，皆是桥梁。此言不虚。 人生一传舍，无处是吾乡。世间万物各有归属，哪来的什么主人，我们都只是个当铺伙计。 富贵可不用尽，余点就是积福。贫贱不可自欺，敬己就是敬天。 好看的风景，值钱的草药，往往都在险峻处。 人间事，其实好坏之别，往往就只差那么一两句话，就可以好坏颠倒。 气头上，多了一两句不该有的重话反话，平日里，少了一两句宽慰人心的废话好话。 因为越是亲近之人，越容易觉得对方做什么事都是天经地义的，都觉得一切只需要在不言中。 结果越是觉得对方应该什么都懂的时候，往往就是对方什么都不懂的时候。 粒善因，只要能够真的开花结果，是有可能花开一片的。 其实不是我在行善事，施舍钱财给他人，而是他人施舍善缘与我。 我们称赞一个人，有分寸感，其实就是保持一种妥当的、得体的距离，远了，就是疏离，过近了，就容易苛求他人。所以得给所有亲近之人，一点余地，甚至是犯错的余地，只要不涉及大是大非，就不用太过揪着不放。心细之人，往往会不小心就会去求全责备，问题在于我们浑然不觉，但是身边人，早已受伤颇多。 有些时候，与不讲理之人不讲理，就是讲理。 善解人意，人解善意，善人解意，人意善解。 天降之福，先开其慧。最不起眼，也最重要。 十八般武艺傍身，绝不会闲置，总有用到时。 这就叫屋大人少，多生精怪作祟。屋小人多，易生口舌是非。 总得有些人，得比坏人更聪明些，才能有更多的好人有好报，就可以让更多好人做好事，能够可以完全不计后果。 就算注定人力有穷尽时，也要先竭尽人事，再来听天命。无非是能够做成眼前一事是一事，能够手边出力一分是一分。 有些不堪言说的苦难，当一个人好不容易熬过去了，自己默默消受着就是了，别与正在吃苦的旁人说什么轻巧话了，那是作妖作怪。 不惮以最大恶意揣测他人，与愿意对他人给予最大善意，两者只是看似矛盾，其实双方并不冲突。 天底下最难学问在努力，天底下最简单学问在结果。 有些为难，并非全是当局者迷旁观者清，也可能是当局者想得太透彻。 骂人先骂己，立于不败之地。多说了一句气话，往往节外生枝，功亏一篑，之前苦口婆心的百般道理，悉数阵亡。少说了一句废话，便起误会，人心处处，杂草丛生，猜忌，失望，怨怼，此起彼伏。唯独老江湖，只在不言中。相逢投缘，下马饮君酒。 我们与人讲理，不是为了否定他人。此外，给予他人善意，除了我们自身的问心无愧，也需要讲究一个分寸感。这就是道术之别了，大道唯一，术却有千百种，因人而异，因地而异，所以说当好人，很难嘛。” 什么都知道，跟什么都不知道，一向没什么两样。 做事千百件，还是做一个人 有为 于道各努力 请君放眼看，平地构大厦，何曾一日成。 年少即须臾，于道各努力。 飒飒松风，一天天的，就这么撞罢晨钟又暮鼓，每天做完课业吃完饭，睡觉醒来又是一天，光阴如水悠悠过。 昨夜又下了一场大雪。 有些事情，想是想不明白的，莫怕，且前行，且慢行，有错就改，无错求更好，对了求最对，万般功夫，所有学问，还不是落在一个行字上？ 自童年起，我便独自一人，照顾着历代星辰。 圣人有云，天将降大任于斯人也，必先苦其心志，劳其筋骨。 有些努力，不是下死力气就行的，否则只会越忙越乱。 世间修行，修力可见，步步为营，只需要往上走，差异只是每一步的步子，各有大小。修心则缥缈，四面八方，处处是路，仿佛条条道路能证得大道，但又好像条条道路都是旁门左道，谁也给不了指点。 在修心一事上，身怀道心之人，叫一步登天。 世间成事者，超世之才不过其次，坚忍不拔之志，方为首要。 得众动天，美意延年。 真正的强者，不独有令人侧目的壮举事迹，还有坚持不懈的细微付出。 莫道君行早，更有早行人。莫道君行高，早有山巅路。 有人日丽中天，云霞四护。 有人一味蝇营狗苟。 有人随日开眼界，随月息心。 有人只顾着低头刨食。 有人只恨读书写字，不到古人佳处。 有人在辛苦过活，不奢谈安心之所，只求立锥之地。 有好人某天在做错事，有坏人某天在做好事。 可能学塾里读书最好的少年，飞黄腾达，当了大官，再不返乡。 可能学塾里的顽劣少年，混迹市井，横行乡野，某天在陋巷遇见了教书先生，恭敬让路。 人生有很多的必然，却有一样多的偶然，都是一个个的可能，大大小小的，就像悬在天上的星辰，明亮昏暗不定。 那日丽中天之人，有天骤然跌落泥泞，身上都是过客的鞋印。 那蝇营狗苟之辈，也能为身边人庇护出一方荫凉。 那眼界大开之人，突然有一天对世界充满了失望，人生开始下山。 那些低头刨食之辈，偶然一抬头，便对生活生出希望，走向了远方和高处。 有人觉得人生没意义，没劲，只需要有意思。 有人觉得人生没意思，很苦，但是得有意义。 有些少年暮气沉沉，有些老人少年意气。 有人大梦一场，不曾醒过。有人痛苦万分，难求一醉。 有人觉得只有书上的圣贤才能说道理，有人觉得庄稼汉辛勤劳作就是道理，一位孤苦无依的老妪也能把生活过得很从容。 有人觉得自己什么道理都懂，过不好，怪道理。 如果一辈子都过不好了，咬牙切齿，怨天尤人。白走一遭。 有人觉得自己什么都不懂，过不好，是道理还懂得太少。 如果一辈子还是过不好，对自己说，那就这样吧。到底走过。 有人自己从不曾杨柳依依，草场莺飞。人生道路上，却一直在铺路搭桥，一路栽种杨柳。 有人瞪大眼睛，费劲气力，寻找着这个世界的阴影。等到夜幕沉沉就酣睡，等到日上三竿，就再起床。 明月山头，荆棘林中，绿水池塘，春浪桃花。一样米养百样人，不同的人生不同的道路上，可能都曾昨夜梦魂中，花月正春风。 世道如此，你想如何，你能如何，你该如何。 自律，自省，自求，自由。 多读古书开眼界，少管闲事养精神。 那些人生意外，就像一场突如其来的磅礴大雨，强者手中有伞，弱者两手空空。 强者撑伞而行。要为这个世界遮风挡雨，片刻也好。 一棵山中幽兰。它从不曾见过世人，世人也不曾见过它。便不开花吗？ 明月皎皎，一样的月光，照过历代圣贤，文人名士，剑仙豪客，照过窗边书生凭栏美人，水上艄公山中樵子，照过夜不能寐的帝王将相，一样也照过鼾声如雷的贩夫走卒，照过高高的华宅飞檐，低低的田埂坟茔，照过元宵的灯市清明的黄纸中秋的月饼年关的春联，照过无人处千百年的白云青山绿水黄花…… 他说有意思的事，有意义的事，都不容易做到。有意思的难事，做成了，未必有什么意义。但是一件有意义的事情，做成了，一定很有意思。 在家乡在异乡，在远游在归途，在山中在山外，在人间在人心，在山河锦绣里，在日月乾坤中，在人间大美处，在世道泥泞上，在剑修如云处，在希望失望重新希望后，先生皆在独自练拳，与天地问拳，与自己问拳。 从容 脚步再匆匆，人生需从容 一天无事，亲人闲坐，灯火可亲。 做个脚踏实地的本分人，就是个心宽体胖的快活人，吃饭香喝酒香睡觉也香。 人生路途漫漫，得个休歇处，还能喝一瓢水解渴，就是善缘法。 一个人侥幸生逢盛世，百虑可忘，若是再精通种植花草之术，宛如四时皆春，可教人不知老之将至。 如今的生活，我觉得就是最好的了。 日上三竿晒衣服，下雨天出门收衣服，可要是……忘了就忘了。 思无邪即从容。 无事澄然，有事斩然。 世事难平，事情摆不平，先将自己心坎摆平了，日子就总能过下去，甚至都不会觉得有多苦。 人间且慢行。 人生实难，大道多歧，既然路难走，就停下来，偷个懒，好好想一想。 背竹箱，穿草鞋，百万拳，翩翩少年最从容。 背仙剑，穿白袍，千万里，人间最好小师叔。 有人说，下五境修士修了个长寿，中五境修士在求长生不朽，上五境修士在更高处更远处大道独行，几乎一刻不得停歇。这样没什么不对，忙碌充实，不辜负光阴，只是偶尔还是需要停下脚步，或者是放缓脚步，静下心来，欣赏修行路上的风景。 行到水穷处，坐看云起时。 若无闲事挂心头，便是人间好时节。 虽然一直待在屁大地方，可这点道理我还是想得通的。一家人，安安稳稳的，谁都饿不着，儿女媳妇想吃就吃得上肉，嘴馋了我也能喝得上口酒，比啥都强。 比如修行，寻常练气士，目标肯定是中五境，天才一些的，会选择上五境。又比如为官，野心小的，是入流品就行，志向大的，是做黄紫公卿。然后在漫长的登山途中，很多人会一直抬着头盯着山顶的风光，身边的树木葱茏，脚下的春花烂漫，都是看不到的，就算看到了，也不会驻足欣赏，枉费了圣人的谆谆教导，天地有大美而不言啊。 命里有时终须有，命里无时莫强求，以平常心看待无常事，便是第一等万全法。 就像端着小碗，春暖花开，天清气朗，今日无事，平平安安。于事，不问收获问耕耘，莫向外求。于心，勤勉修行戒定慧，与天祈福。 做事情，要急缓得当，松弛有度。 天上皎皎明月光，人间匆匆少年郎，脚步最匆匆。 得之我幸，失之我命。墙外花开，也是开花。 是一棵参天大树，便力所能及，庇护一方人心荫凉，是尚未成长起来的花草儿，就无忧无虑，慢慢长大，天暖花开，一样是春。 余了好多，余着好久。 天变未必变天。 心湛静，笑白云多事，等闲为雨出山来。 心神越小，念头越小，步子越小，我们反而走得快些。 心中积郁，随雪落地。 良辰美景十六事，都比不上个‘今日无事’。 小米粒好像从裴钱袖子上双指捻住了一粒瓜子，往自己嘴里一丢，“小小忧愁，一吃就没。” 那么读书识字，图什么呢。为人少点戾气，处世多点耐心，渐渐的把脚下道路越走越宽，在世道中，走得稳当些，从容些。 未过 过去已过去，未来还未来。时时是过去，刻刻有未来。过去曾未来，未来会过去 突然忘记某个字，又突然记起某件事，好像曾经经历过…… 所有的昨天都不可追回，所有的明天又都在明天。 每个今天的昨天都不曾虚度，每个明天都是今天的希望。 今年我们家年景好，希望明年年景更好啊，相信肯定会更好的！ 万物如草木，有荣枯生死。天地所以能长且久，以其不自生，故能长生。 今天明天后天。 世事短如春梦，春梦了无痕，譬如春梦，黄粱未熟蕉鹿走。 人生有聚终有散，所幸有散又有聚。 只愁春风秋花，聚散真容易。惟愿春花秋月，重逢不太难。 所有能够重新翻出来说道说道的陈年旧事，才是真正的解开了心结。 今日得失，可能就是明日失得。 许多过往之人事，可想可念不可及。 人生重重磨难过后，往往柳暗花明又一村。 爹娘的某句牢骚，夫子先生的某句教诲，一翻而过又重头翻回再看的书上语句，某个听了很多遍终于在某天蓦然开窍的老话、道理，看过的青山绿水，错过的心仪女子，走散的的朋友，皆是所有人心田里的一粒粒种子，等待着开花。 昨日种种昨日死，今日种种今日生。 这些磨难，未必全是坏事，熬过去，就会是另一种好事。 秉烛夜游，天就亮了。 春者天之本怀，秋者天之别调，花开花落又开。 雾失楼台，月迷津渡，往事已空，如一梦中。一身犹在，乱山深处。枯木犹能逢春，老树尚可着花。故人呢？ 飒飒松风，一天天的，就这么撞罢晨钟又暮鼓，每天做完课业吃完饭，睡觉醒来又是一天，光阴如水悠悠过。 书上说，天下没有不散的筵席，但是不要怕，书上还说了，人生何处不相逢。 光阴总是最不讲道理的，就像一个跟人打架从没输过的，偷东西从没落空过的蟊贼。 世间聚散苦匆匆，一回相见一回老。历史就像一只火盆，装着一堆有余温的灰烬。所有的灰烬，都是已经被彻底遗忘的逝去之人，而那些火星，就是已逝之人却依然留在天地间的痕迹。 曾几何时，溪水渐浅，井水愈寒，槐树更老，铁锁生锈，大云低垂。今年桃叶见不到桃花。如今却是，积雪消融，青山解冻，冰下水声，叶底黄莺，又一年桃花开，报今年春色最好。 天边泛起鱼肚白，先是米粒之光，然后大放光明。 青山不改绿水长流。 昨日对未必是今日对，今日错未必是明日错。 老百姓过日子，有些撕心裂肺的伤心事，摔落在世道的坑洼里，人跌倒了，还得爬起来继续往前走，伤心事掉下去就起不来了，甚至人熬过去，就是事过去了。 有些事情来了，不会等我做好准备，好像不打个商量就劈头盖脸冲到了眼前，让人只能受着。同时有些事情要走，又怎么拦也拦不住，一样只能让人熬着，都没法跟人说什么好，不说心里憋屈，多说了矫情，所以就想找个长辈，诉几句苦。 聚散都随缘。 但愿， 跋山涉水，风景秀丽；久别重逢，故人无恙； 入庙烧香，有求有应；异乡游子，又逢佳节。 “先生希望落魄山永远是今天的落魄山，我希望先生永远是明天的先生。” “学生相信每个明天的先生，一定会比每个今天更好吧。” 那就什么都别多想，过日子嘛，还真就有很多事情，只能是船到桥头自然直。","link":"/2023/12/30/%E5%A4%A7%E9%81%93/"},{"title":"二战记","text":"谨以此文，记录二战日常！ 2024 Nov Date Event 2024.11.29 复习数学错题复习知识点学习英语学习政治 2024.11.28 复习数学错题复习知识点学习英语学习政治 2024.11.27 校对数学二李林六套卷第六套并复习错题校对中科大2024部分并复习知识点学习英语学习政治 2024.11.26 完成数学二李林六套卷第六套完成并校对11.24测试题 + 校对中科大2024部分学习英语学习政治 2024.11.25 复习数学错题完成并校对南师大2020学习英语学习政治 2024.11.24 复习数学错题完成并校对中科院物化2024部分学习英语学习政治 2024.11.23 完成数学二李林六套卷第五套并校对完成中科大物化2024部分 + 完成中科院物化2024部分学习英语学习政治 2024.11.22 复习数学错题完成并校对北京化工大学物化样题部分学习英语学习政治 2024.11.21 完成数学二李林六套卷第四套并校对完成并校对北京化工大学物化样题部分完成英语真题2013部分学习政治 2024.11.20 复习数学知识点复习物化知识点学习政治 2024.11.19 完成并校对数学二张宇八套卷第七套复习物化知识点学习政治 2024.11.18 完成数学二李林六套卷第三套并校对复习知识点完成英语真题2014部分学习政治 2024.11.17 复习数学知识点复习物化知识点学习政治 2024.11.16 完成数学二李林六套卷第二套并校对完成并校对湖南师范大学2024完成英语真题2015部分学习政治 2024.11.15 完成并校对数学二张宇八套卷第六套完成并校对中国海洋大学2020完成英语真题2016部分学习政治 2024.11.14 完成并校对数学二张宇八套卷第五套完成并校对中国石油大学华东2024部分完成英语真题2017部分学习政治 2024.11.13 完成数学二真题2023并校对完成并校对中国石油大学华东2024部分完成英语真题2018部分学习政治 2024.11.12 完成并校对数学二张宇八套卷第四套完成并校对测试题+完成并校对中国石油大学华东2024部分完成英语真题2019部分学习政治 2024.11.11 完成数学二真题2022并校对复习知识点完成英语真题2019部分学习政治 2024.11.10 复习错题复习知识点完成英语真题2019部分学习政治 2024.11.09 完成数学二李永乐六套卷第六套并校对完成并校对南大六版课后题部分完成英语真题2019部分学习政治 2024.11.08 完成数学二真题2021并校对完成并校对南大六版课后题部分完成英语真题2019部分学习政治 2024.11.07 完成数学二李永乐六套卷第五套并校对校对西北大学2024真题部分完成英语真题2018部分学习政治 2024.11.06 完成数学二真题2020并校对完成西北大学2024真题并校对部分完成英语真题2018部分学习政治 2024.11.05 完成数学二李林六套卷第一套并校对完成11.3测试题并校对完成英语真题2017部分+2018部分学习政治 2024.11.04 完成数学二真题2019并校对复习物化知识点完成英语真题2017部分学习政治 2024.11.03 复习数学错题复习物化知识点完成英语真题2017部分学习政治 2024.11.02 完成数学二李永乐六套卷第四套并校对完成复习物化知识点完成英语真题2017部分学习政治 2024.11.01 完成并校对数学二真题2018完成并校对南大六版课后题部分完成英语真题2016部分学习政治 2024 Oct Date Event 2024.10.31 完成并校对数学二张宇八套卷第三套完成并校对南大六版课后题部分完成英语真题2016部分学习政治 2024.10.30 完成数学二真题2017并校对完成并校对南大六版课后题部分完成英语真题2015部分学习政治 2024.10.29 完成数学二李永乐六套卷第三套并校对完成10.27测试题并校对+完成湖南大学2024真题部分完成英语真题2015部分学习政治 2024.10.28 完成数学二真题2016并校对复习知识点完成英语真题2014部分学习政治 2024.10.27 校对数学二张宇八套卷第二套复习知识点完成英语真题2014部分学习政治 2024.10.26 完成数学二张宇八套卷第二套并校对部分完成并校对南大六版课后题部分完成英语真题2013+2014部分学习政治 2024.10.25 完成数学二真题2015并校对完成并校对南大六版课后题部分完成英语真题2013部分学习政治 2024.10.24 完成数学二李永乐六套卷第二套并校对校对南京师范大学2022真题部分+完成并校对南大六版课后题部分完成英语真题2013部分学习政治 2024.10.23 完成数学二真题2014并校对校对南京师范大学2022真题部分并校对部分完成英语真题2012部分学习政治 2024.10.22 完成数学二李永乐六套卷第一套并校对校对南京师范大学2022真题部分并校对部分完成英语真题2012部分学习政治 2024.10.21 完成数学二真题2013并校对校对哈工大2023真题部分完成英语真题2011年部分+2012部分学习政治 2024.10.20 完成数学二李艳芳模拟题第二套完成哈工大2023真题部分完成英语真题2011年部分学习政治 2024.10.19 完成数学二真题2012并校对校对哈工大2022真题部分+完成哈工大2023真题部分完成英语真题2011年部分学习政治 2024.10.18 完成数学二李艳芳模拟题第一套并校对完成哈工大2022真题学习政治 2024.10.17 完成数学二真题2011并校对完成天大精选题目2部分并校对+完成10.13测试题并校对+完成哈工大2022真题部分完成英语真题2010年部分学习政治 2024.10.16 完成数学二模拟题张宇8套第一套并校对完成天大精选题目2部分并校对完成英语真题2010年部分学习政治 2024.10.15 完成数学二真题2010并校对完成天大精选题目2部分完成英语真题2003年部分+2010年部分学习政治 2024.10.14 完成数学二真题2009并校对完成天大精选题目1部分并校对+完成天大精选题目2部分并校对完成英语真题2003年部分学习政治 2024.10.13 完成数学二真题2008并校对完成天大精选题目1部分完成英语真题2003年部分学习政治 2024.10.12 完成数学二真题2007并校对完成天大精选题目1部分完成英语真题2003年部分学习政治 2024.10.11 完成数学二真题2006并校对完成天大精选题目1部分完成英语真题2002年部分+2003年部分学习政治 2024.10.10 校对数学二真题2005完成天大精选题目1部分并校对部分完成英语真题2002年部分学习政治3h 2024.10.09 完成数学二真题2005完成天大精选题目1部分完成英语真题2001年部分学习政治2h 2024.10.08 校对数学二真题2004校对哈工大真题2015部分完成英语真题2001年两篇阅读学习政治2h 2024.10.07 完成数学二真题2004校对物化真题重庆大学2023部分+完成哈工大真题2015部分 2024.10.06 校对数学二真题2003校对物化真题重庆大学2023部分 2024.10.05 完成数学二真题2003完成物化真题重庆大学2023部分 2024.10.04 校对数学二真题2002校对物化真题南京师范2021部分 2024.10.03 完成数学二真题2002校对物化真题南京师范2021部分 2024.10.02 回顾数学二错题完成物化真题南京师范2021部分 2024.10.01 校对张宇1000题测试二完成物化真题南京师范2021部分 2024 Sep Date Event 2024.09.30 完成张宇1000题测试二校对物化真题华中师范2023 2024.09.29 None 2024.09.28 None 2024.09.27 None 2024.09.26 None 2024.09.25 完成张宇1000题测试二部分完成物化真题华中师范2023 2024.09.24 校对张宇1000题测试一校对物化真题合工大2015 2024.09.23 完成张宇1000题测试一完成物化真题合工大2015 2024.09.22 回顾数学二错题校对物化真题西北大学2020 2024.09.21 回顾数学二错题完成物化真题西北大学2020部分 2024.09.20 回顾数学二错题完成物化真题西北大学2020部分 2024.09.19 完成660第一阶段填空501-545完成物理化学南师大2022并校对其部分 2024.09.18 完成660第一阶段填空485-500完成物理化学南师大2022 2024.09.17 完成660第一阶段填空465-484校对物理化学南师大2023 2024.09.16 完成660第一阶段填空446-464完成物理化学南师大2023 2024.09.15 完成660第一阶段填空418-445校对完成物理化学南大课后题动力学1部分 2024.09.14 完成660第一阶段填空337-417校对完成物理化学南大课后题动力学1部分 2024.09.13 完成660第一阶段填空283-336完成物理化学南大课后题动力学1部分+校对完成物理化学南大课后题动力学1部分 2024.09.12 完成660第一阶段填空266-282完成物理化学南大课后题动力学1部分 2024.09.11 完成660第一阶段填空246-265校对中国石油大学2023真题部分 2024.09.10 完成660第一阶段填空221-245完成中国石油大学2023真题部分+校对中国石油大学2023真题部分 2024.09.09 完成660第一阶段填空201-220校对南大课后题表面化学部分+完成中国石油大学2023真题部分 2024.09.08 完成660第一阶段填空165-200校对南大课后题表面化学部分 2024.09.07 完成660第一阶段填空146-164完成南大课后题表面化学部分 2024.09.06 完成660第一阶段填空106-145校对物理化学安徽大学2023真题部分+完成南大课后题表面化学部分 2024.09.05 完成660第一阶段填空76-105校对物理化学安徽大学2023真题部分 2024.09.04 完成660第一阶段填空58-75校对物理化学8.25测试题+完成物理化学安徽大学2023真题部分 2024.09.03 完成660第一阶段填空33-57校对物理化学南大课后题可逆电池部分 2024.09.02 完成660第一阶段填空题8-32完成物理化学安徽大学2023真题部分 2024.09.01 复习高数第一章+完成660第一阶段测试一、测试二、填空题1-7 2024 Aug Date Event 2024.08.31 校对李林880第十二章综合题目+复习高数第一章部分物理化学完成南大课后题可逆电池部分 2024.08.30 校对李林880第十二章综合题目+复习高数第一章部分物理化学完成南大课后题可逆电池部分 2024.08.29 完成李林880第12章综合题目物理化学完成南大课后题可逆电池部分 2024.08.28 完成李林880第12章综合题目物理化学校对哈工大2016真题 2024.08.27 完成李林880第12章综合题目物理化学完成哈工大2016真题 2024.08.26 校对李林880第11章综合题目物理化学校对南大课后题电解液部分 2024.08.25 完成李林880第11章综合题目物理化学完成南大课后题电解液部分 2024.08.24 校对李林880第10章综合题目物理化学完成南大课后题电解液部分 2024.08.23 完成李林880第10章综合题目物理化学校对哈工大2017真题 2024.08.22 校对李林880第9章综合题目物理化学完成哈工大2017真题 2024.08.21 校对李林880第9章综合题目部分物理化学完成哈工大2017真题 2024.08.20 完成李林880第8章综合题目物理化学校对南大课后题化学平衡部分+8.11测试题 2024.08.19 完成李林880第8章综合题目部分物理化学校对南大课后题化学平衡部分 2024.08.18 校对李林880第7章综合题目+完成李林880第二章综合题目部分物理化学校对南大课后题化学平衡部分 2024.08.17 完成李林880第7章综合题目部分物理化学完成南大课后题化学平衡部分 2024.08.16 完成并校对李林880第六章物理化学校对哈工大2018部分 2024.08.15 完成并校对李林880第五章物理化学完成哈工大2018部分 2024.08.14 完成并校对李林880第四章物理化学校对南大课后题相平衡部分 2024.08.13 校对李林880第三章部分物理化学完成南大课后题相平衡部分 2024.08.12 校对李林880第三章部分物理化学校对哈工大2019+完成南大课后题相平衡部分 2024.08.11 复习张宇1000题第三章+李林880第三章部分物理化学校对哈工大2019 2024.08.10 李林880校对第二章部分物理化学完成哈工大2019部分 2024.08.09 复习张宇1000题第二章+李林880第二章部分物理化学完成哈工大2019部分 2024.08.08 李林880校对第一章物理化学校对南大课后题多组分部分 2024.08.07 复习张宇1000题第一章+李林880第一章物理化学完成南大课后题多组分部分 2024.08.06 张宇1000题强化线性代数校对第9章部分物理化学校对7.28测试题+完成南大课后题多组分部分 2024.08.05 张宇1000题强化线性代数完成第9章部分物理化学完成7.28测试题 2024.08.04 张宇1000题强化线性代数校对第8章部分物理化学校对哈工大真题2021 2024.08.03 张宇1000题强化线性代数第8章部分物理化学哈工大真题2021 2024.08.02 张宇1000题强化线性代数第6、7章物理化学校对南大课后题热力学第二定律 2024.08.01 张宇1000题强化线性代数第4、5章物理化学校对南大课后题热力学第二定律 2024 Jun&amp;Jul Date Event 2024.07.31 张宇1000题强化线性代数第三章部分物理化学校对南大课后题热力学第二定律 2024.07.30 张宇1000题强化线性代数第二、三章部分物理化学南大课后题热力学第二定律 2024.07.29 张宇1000题强化校对第十五章部分 + 张宇1000题强化线性代数第一章部分物理化学校对20哈工大真题部分 2024.07.28 张宇1000题强化校对第十五章部分物理化学校对20哈工大真题部分 2024.07.27 张宇1000题强化完成第十五章部分物理化学20哈工大真题 2024.07.26 张宇1000题强化完成第十五章部分物理化学南大课后题校对热力学第一定律部分 2024.07.25 张宇1000题强化完成第十五章部分物理化学南大课后题校对热力学第一定律部分 2024.07.24 张宇1000题强化完成校对第十四章部分物理化学南大课后题热力学第一定律部分 2024.07.23 张宇1000题强化完成第十四章部分物理化学完成南大课后题热力学第一定律部分 2024.07.22 张宇1000题强化完成第十四章部分物理化学校对二轮测试题、完成南大课后题热力学第一定律部分 2024.07.21 张宇1000题强化校对第13章， 完成第十四章部分物理化学复习条件反射41、完成二轮测试题 2024.07.20 张宇1000题强化完成第13章部分物理化学复习条件反射25-40 2024.07.19 张宇1000题强化完成第13章部分物理化学复习条件反射19-24 2024.07.18 张宇1000题强化完成第11、 12、13部分章、校对第11、12章物理化学复习条件反射6-18 2024.07.17 Math: HM Zhangyu 1000 Lv2 U10 2024.07.16 Math: HM Zhangyu 1000 Lv2 U9Review Physical Chemistry “条件反射” 2024.07.15 Math: HM Zhangyu 1000 Lv2 U9Learn Physical Chemistry “胶体测试题”、“动力学2” 2024.07.14 Math: HM Zhangyu 1000 Lv2 U8Learn Physical Chemistry “动力学测试2”、“胶体1”、“胶体2” 2024.07.13 Math: HM Zhangyu 1000 Lv2 U8Learn Physical Chemistry “动力学测试2” 2024.07.12 Math: HM Zhangyu 1000 Lv2 U7Learn Physical Chemistry “动力学测试2” 2024.07.11 Math: HM Zhangyu 1000 Lv2 U6Learn Physical Chemistry “动力学1”、”动力学2” 2024.07.10 Math: HM Zhangyu 1000 Lv2 U5Learn Physical Chemistry “动力学测试” 2024.07.09 Math: HM Zhangyu 1000 Lv2 U5Learn Physical Chemistry “表面化学测试” 、 “表面化学3”、“动力学1” 2024.07.08 None 2024.07.07 Math: HM Zhangyu 1000 Lv2 U5Learn Physical Chemistry “表面化学2” 2024.07.06 Learn Physical Chemistry “电解极化+表面化学” 2024.07.05 Math: HM Zhangyu 1000 Lv2 U4Learn Physical Chemistry “可逆电池测试题” 2024.07.04 Math: HM Zhangyu 1000 Lv2 U3 + 校对Lv2 U3Learn Physical Chemistry “可逆电池4” 2024.07.03 Math: HM Zhangyu 1000 Lv2 U1 85-100% + 校对Lv2 U1Learn Physical Chemistry “可逆电池3” 2024.07.02 Math: Linear algebra Zhangyu 1000 Lv2 U1 45%-85%Learn Physical Chemistry “五一测试” 、 “可逆电池” 2024.07.01 Math: Linear algebra Zhangyu 1000 Lv2 U1 0%-45%Learn Physical Chemistry “电解质溶液+可逆电池” 2024.06.30 Math: Review HM U1-13Learn Physical Chemistry “电解质溶液3” + 校对电解液化学平衡测试题 2024.06.29 Math: Linear algebra Zhangyu 1000 U6Learn Physical Chemistry “电解液化学平衡测试”Learn Physical Chemistry “电解质溶液2” 2024.06.28 Review Math: Linear algebra U4-6Math: Linear algebra Zhangyu 1000 U5Learn Physical Chemistry “电解质溶液1” 2024.06.27 Review Math: Linear algebra U1-3Review Physical Chemistry V1 227-447/447 2024.06.26 Math: Linear algebra Zhangyu 1000 U4Review Physical Chemistry V1 1-226/447 2024.06.25 Put file in order 2024.06.24 None 2024.06.23 None 2024.06.22 None 2024.06.21 None 2024.06.20 Put file in order 2024.06.19 DeepLearning 2024.06.18 Coursera： Deep LearningMeetingGraduate symposium 2024.06.17 Coursera： Deep LearningOrganize materials 2024.06.16 Coursera： Deep Learning 2024.06.15 Coursera： Deep Learning 2024.06.14 Coursera： Deep Learning 2024.06.13 Coursera： Deep Learning 2024.06.12 Coursera： Deep LearningWriting thought reportOrganize materials 2024.06.11 Writing thought reportOrganize materials 2024.06.10 Coursera： Deep Learning 2024.06.09 Coursera： Deep Learning 2024.06.08 None 2024.06.07 None 2024.06.06 Coursera： Deep Learning 2024.06.05 Coursera： Deep Learning 2024.06.04 Coursera： Deep Learning 2024.06.03 Coursera： Deep Learning 2024.06.02 Coursera： Deep LearningWriting paper 2024.06.01 Coursera： Deep Learning 2024 May Date Event 2024.05.31 Coursera： Deep Learning 2024.05.30 Coursera： Deep LearningTaking PhotoMaking PPT 2024.05.29 Coursera： Deep Learning 2024.05.28 Coursera： Deep LearningWriting paper 2024.05.27 Coursera： Deep Learning 2024.05.26 Coursera： Deep LearningPlan for travel 2024.05.25 Coursera： Deep LearningMaking PPTPlan for travel 2024.05.24 Coursera： Deep LearningMaking PPT 2024.05.23 Coursera： Deep LearningMaking PPT 2024.05.22 Coursera： Deep LearningMaking PPT 2024.05.21 Coursera： Deep Learning 2024.05.20 Coursera： Deep Learning 2024.05.19 Coursera： Deep LearningCoding new web’s HTML/CSS/JS 2024.05.18 Coursera： Deep Learning 2024.05.17 Coursera： Deep Learning 2024.05.16 Coursera： Deep Learning 2024.05.15 Coursera： Deep Learning 2024.05.14 Writing paper 2024.05.13 Writing paperCoursera： Deep Learning 2024.05.12 Writing paper 2024.05.11 Writing paper 2024.05.10 Writing paper 2024.05.09 Writing paper 2024.05.08 Writing paper 2024.05.07 Writing paper 2024.05.06 Coursera: Deep LearningApply WerobotWriting paper 2024.05.05 Coursera: Deep LearningApply WerobotWriting paper 2024.05.04 Coursera: Deep LearningApply Werobot 2024.05.03 Coursera: Generative AI for everyoneCoursera: Deep Learning 2024.05.02 写论文 2024.05.01 清理板子接受论文指导写论文 2024 Apr Date Event 2024.04.30 None 2024.04.29 Improve AutoSignIn system 2024.04.28 None 2024.04.27 孵育细菌以及涂板子Learning cs231nUpdate ChatBog Training Code 2024.04.26 孵育细菌以及涂板子Update AutoSignIn System 2024.04.25 孵育细菌以及涂板子修改论文初稿并提交修改中期报告并提交 2024.04.24 孵育细菌以及涂板子修改论文初稿 2024.04.23 孵育细菌以及涂板子写指导日志以及论文初稿学习物理化学化学平衡2+电化学 2024.04.22 孵育细菌写论文 2024.04.21 写论文 2024.04.20 进行抗菌实验写论文 2024.04.19 进行抗菌实验查找自动签到系统bug写论文 2024.04.18 进行抗菌实验完善自动签到系统写论文 2024.04.17 进行抗菌实验编写自动签到系统（通过网页自定义签到时间和座位） 2024.04.16 完成并校对物理化学相平衡+化学平衡测试题进行PTHP抗菌导尿管实验Design shortcuts 2024.04.15 学习物理化学”相平衡3”、”相平衡收尾+化学平衡1”填写学籍卡草稿统计班级上岸名单以及收集照片完成心理测试 2024.04.14 Making VideoWriting paper 2024.04.13 Making Video参加普通话考试 2024.04.12 进行抗菌实验打扫实验室卫生DALL-EcomfyUI图生视频规范论文格式 2024.04.11 修改开题报告、任务书、论文进行抗菌实验 2024.04.10 Add AutoUploadBlog.sh to Mac进行抗菌实验学习comfyui并应用于图生视频 2024.04.09 校对物理化学“清明节测试题” 2024.04.08 学习ColossalAI 2024.04.07 学习“实战AI大模型”这本书修改开题报告完成物理化学“清明节测试题” 2024.04.06 学习“实战AI大模型”这本书 2024.04.05 学习“实战AI大模型”这本书 2024.04.04 Training ChatBot完成并校对物理化学“相平衡测试题”学习物理化学“相平衡1”统计上月收支 2024.04.03 None 2024.04.02 None 2024.04.01 进行PTHP合成实验 2024 Mar Date Event 2024.03.31 None 2024.03.30 None 2024.03.29 Training ChatBot进行抗菌实验Learning ‘GPT-SoVITS’ (voice clone project) 2024.03.28 Training ChatBotLearning Pytorch Chapter6-9Learning Stable diffusion进行稀释细菌/红外实验学习物理化学“多组分3” 2024.03.27 Training ChatBotLearning Pytorch Chapter4 and Chapter5 2024.03.26 Training ChatBotLearning Pytorch Chapter3 3.3进行测红外实验校对物理化学“多组分测试题” 2024.03.25 Training ChatBotLearning Pytorch Chapter2 and Chapter3 ~3.2Finish Opening Report完成物理化学“多组分测试题” 2024.03.24 Training ChatBotLearning Pytorch Chapter2 and Chapter3 20% 2024.03.23 进行抗菌实验 2024.03.22 进行PTHP后处理实验Training ChatBot 2024.03.21 进行测红外实验Training ChatBot学习物理化学“多组分1” 2024.03.20 进行制备PTHP实验Debug ChatBot Training 2024.03.19 完成并校对物理化学“热力第二定律测试题”进行抗菌实验训练ChatBot: chatbot_0319_2107 2024.03.18 训练ChatBot: chatbot_0318_0727 2024.03.17 训练ChatBot 数据预处理 2024.03.16 旁观“PTHP后处理”实验以及”清理培养皿中的细菌“实验学习物理化学“热力学第二定律2”参加本学期第一次班会 2024.03.15 进行”离心“实验参加本学期第一次班团会议 2024.03.14 学习物理化学“热力学第二定律1”进行毕业论文“培养细菌”实验撰写开题报告 2024.03.13 完成并校对物理化学“热力第一定律测试题2”后67%进行毕业论文“培养细菌”实验学习Chatbot Tutorial代码 进度：后45%完善ChatBot网页显示效果（按钮） 2024.03.12 完成并校对物理化学“热力第一定律测试题2”33%进行毕业论文“纺丝”实验 2024.03.11 完成张宇1000题线性代数 第一、二章学习物理化学“热力学第一定律4”学习Chatbot Tutorial代码 进度：55% 2024.03.10 学习Chatbot Tutorial代码 进度：35% 2024.03.09 优化OYM’s AI Chat Bot(V2) 2024.03.08 复习张宇1000题高数 错题第十～十五章发布OYM’s AI Chat Bot(V2) 2024.03.07 复习张宇1000题高数 错题第一～九章学习物理化学“热力学第一定律3”部署wukong-robot项目并最终验证不可行（过于复杂）部署ChatBot V1项目至云服务器并验证可行 2024.03.06 完成张宇1000题高数 第十四、十五章 2024.03.05 完成张宇1000题高数 第十一、十二、十三章完成并校对物理化学“热力第一定律测试题” 2024.03.04 完成张宇1000题高数 第九、十章学习物理化学“气体基础知识点2”、“气体基础知识点1” 2024.03.03 完成张宇1000题高数 第七、八章 2024 Feb Date Event 2024.02.26 搜集相关资料 2024.02.27 搜集相关资料；完成张宇1000题第一、二章 2024.02.28 搜集相关资料完成张宇1000题第三、四章学习物理化学第一章绪论部分 2024.02.29 搜集相关资料完成张宇1000题第五、六章学习物理化学“气体基础知识点1”","link":"/2024/02/29/%E4%BA%8C%E6%88%98%E8%AE%B0/"},{"title":"AI Chat Bot","text":"This article records my process of creating an AI Chat Bot. Chatbot Tutorial http://fancyerii.github.io/2019/02/14/chatbot/ 使用PyTorch实现Chatbot本教程会介绍使用seq2seq模型实现一个chatbot，训练数据来自Cornell电影对话语料库。对话系统是目前的研究热点，它在客服、可穿戴设备和智能家居等场景有广泛应用。 传统的对话系统要么基于检索的方法——提前准备一个问答库，根据用户的输入寻找类似的问题和答案。这更像一个问答系统，它很难进行多轮的交互，而且答案是固定不变的。要么基于预先设置的对话流程，这主要用于slot-filling(Task-Oriented)的任务，比如查询机票需要用户提供日期，达到城市等信息。这种方法的缺点是比较死板，如果用户的意图在设计的流程之外，那么就无法处理，而且对话的流程也一般比较固定，要支持用户随意的话题内跳转和话题间切换比较困难。 因此目前学术界的研究热点是根据大量的对话数据，自动的End-to-End的使用Seq2Seq模型学习对话模型。它的好处是不需要人来设计这个对话流程，完全是数据驱动的方法。它的缺点是流程不受人(开发者)控制，在严肃的场景(比如客服)下使用会有比较大的风险，而且需要大量的对话数据，这在很多实际应用中是很难得到的。因此目前seq2seq模型的对话系统更多的是用于类似小冰的闲聊机器人上，最近也有不少论文研究把这种方法用于task-oriented的任务，但还不是太成熟，在业界还很少被使用。 效果本文使用的Cornell电影对话语料库就是偏向于闲聊的语料库。 本教程的主要内容参考了PyTorch官方教程。读者可以从这里获取完整代码。 下面是这个教程实现的对话效果示例： 1234567891011121314151617181920&gt; hello?Bot: hello .&gt; where am I?Bot: you re in a hospital .&gt; who are you?Bot: i m a lawyer .&gt; how are you doing?Bot: i m fine .&gt; are you my friend?Bot: no .&gt; you're under arrestBot: i m trying to help you !&gt; i'm just kiddingBot: i m sorry .&gt; where are you from?Bot: san francisco .&gt; it's time for me to leaveBot: i know .&gt; goodbyeBot: goodbye . 准备首先我们通过下载链接下载训练语料库，这是一个zip文件，把它下载后解压到项目目录的子目录data下。接下来我们导入需要用到的模块，这主要是PyTorch的模块： 1234567891011121314151617181920212223from __future__ import absolute_importfrom __future__ import divisionfrom __future__ import print_functionfrom __future__ import unicode_literalsimport torchfrom torch.jit import script, traceimport torch.nn as nnfrom torch import optimimport torch.nn.functional as Fimport csvimport randomimport reimport osimport unicodedataimport codecsfrom io import openimport itertoolsimport mathUSE_CUDA = torch.cuda.is_available()device = torch.device(&quot;cuda&quot; if USE_CUDA else &quot;cpu&quot;) 加载和预处理数据接下来我们需要对原始数据进行变换然后用合适的数据结构加载到内存里。 Cornell电影对话语料库是电影人物的对话数据，它包括： 10,292对电影人物(一部电影有多个人物，他们两两之间可能存在对话)的220,579个对话 617部电影的9,035个人物 总共304,713个utterance(utterance是对话中的语音片段，不一定是完整的句子) 这个数据集是比较大并且多样的(diverse)，语言形式、时代和情感都有很多样。这样的数据可以使得我们的chatbot对于不同的输入更加鲁棒(robust)。 首先我们来看一下原始数据长什么样： 12345678910corpus_name = &quot;cornell movie-dialogs corpus&quot;corpus = os.path.join(&quot;data&quot;, corpus_name)def printLines(file, n=10): with open(file, 'rb') as datafile: lines = datafile.readlines() for line in lines[:n]: print(line)printLines(os.path.join(corpus, &quot;movie_lines.txt&quot;)) 解压后的目录有很多文件，我们会用到的文件包括movie_lines.txt。上面的代码输出这个文件的前10行，结果如下： 12345678910b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'b&quot;L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n&quot;b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'b&quot;L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n&quot;b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding. You know how sometimes you just become this &quot;persona&quot;? And you don\\'t know how to quit?\\n'b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n' 注意：上面的move_lines.txt每行都是一个utterance，但是这个文件看不出哪些utterance是组成一段对话的，这需要 movie_conversations.txt 文件： 1234567891011data/cornell movie-dialogs corpus$ head movie_conversations.txt u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L271', 'L272', 'L273', 'L274', 'L275']u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L276', 'L277']u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L280', 'L281']u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L363', 'L364']u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L365', 'L366'] 每一行用”+++$+++”分割成4列，第一列表示第一个人物的ID，第二列表示第二个人物的ID，第三列表示电影的ID，第四列表示这两个人物在这部电影中的一段对话，比如第一行的表示人物u0和u2在电影m0中的一段对话包含ID为L194、L195、L196和L197的4个utterance。注意：两个人物在一部电影中会有多段对话，中间可能穿插其他人之间的对话，而且即使中间没有其他人说话，这两个人物对话的内容从语义上也可能是属于不同的对话(话题)。所以我们看到第二行还是u0和u2在电影m0中的对话，它包含L198和L199两个utterance，L198是紧接着L197之后的，但是它们属于两个对话(话题)。 数据处理为了使用方便，我们会把原始数据处理成一个新的文件，这个新文件的每一行都是用TAB分割问题(query)和答案(response)对。为了实现这个目的，我们首先定义一些用于parsing原始文件 movie_lines.txt 的辅助函数。 loadLines 把movie_lines.txt 文件切分成 (lineID, characterID, movieID, character, text) loadConversations 把上面的行group成一个个多轮的对话 extractSentencePairs 从上面的每个对话中抽取句对 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 把每一行都parse成一个dict，key是lineID、characterID、movieID、character和text# 分别代表这一行的ID、人物ID、电影ID，人物名称和文本。# 最终输出一个dict，key是lineID，value是一个dict。# value这个dict的key是lineID、characterID、movieID、character和textdef loadLines(fileName, fields): lines = {} with open(fileName, 'r', encoding='iso-8859-1') as f: for line in f: values = line.split(&quot; +++$+++ &quot;) # 抽取fields lineObj = {} for i, field in enumerate(fields): lineObj[field] = values[i] lines[lineObj['lineID']] = lineObj return lines# 根据movie_conversations.txt文件和上输出的lines，把utterance组成对话。# 最终输出一个list，这个list的每一个元素都是一个dict，# key分别是character1ID、character2ID、movieID和utteranceIDs。# 分别表示这对话的第一个人物的ID，第二个的ID，电影的ID以及它包含的utteranceIDs# 最后根据lines，还给每一行的dict增加一个key为lines，其value是个list，# 包含所有utterance(上面得到的lines的value)def loadConversations(fileName, lines, fields): conversations = [] with open(fileName, 'r', encoding='iso-8859-1') as f: for line in f: values = line.split(&quot; +++$+++ &quot;) # 抽取fields convObj = {} for i, field in enumerate(fields): convObj[field] = values[i] # convObj[&quot;utteranceIDs&quot;]是一个字符串，形如['L198', 'L199'] # 我们用eval把这个字符串变成一个字符串的list。 lineIds = eval(convObj[&quot;utteranceIDs&quot;]) # 根据lineIds构造一个数组，根据lineId去lines里检索出存储utterance对象。 convObj[&quot;lines&quot;] = [] for lineId in lineIds: convObj[&quot;lines&quot;].append(lines[lineId]) conversations.append(convObj) return conversations# 从对话中抽取句对 # 假设一段对话包含s1,s2,s3,s4这4个utterance# 那么会返回3个句对：s1-s2,s2-s3和s3-s4。def extractSentencePairs(conversations): qa_pairs = [] for conversation in conversations: # 遍历对话中的每一个句子，忽略最后一个句子，因为没有答案。 for i in range(len(conversation[&quot;lines&quot;]) - 1): inputLine = conversation[&quot;lines&quot;][i][&quot;text&quot;].strip() targetLine = conversation[&quot;lines&quot;][i+1][&quot;text&quot;].strip() # 如果有空的句子就去掉 if inputLine and targetLine: qa_pairs.append([inputLine, targetLine]) return qa_pairs 接下来我们利用上面的3个函数对原始数据进行处理，最终得到formatted_movie_lines.txt。 1234567891011121314151617181920212223242526272829303132# 定义新的文件 datafile = os.path.join(corpus, &quot;formatted_movie_lines.txt&quot;)delimiter = '\\t'# 对分隔符delimiter进行decode，这里对tab进行decode结果并没有变delimiter = str(codecs.decode(delimiter, &quot;unicode_escape&quot;))# 初始化dict lines，list conversations以及前面我们介绍过的field的id数组。lines = {}conversations = []MOVIE_LINES_FIELDS = [&quot;lineID&quot;, &quot;characterID&quot;, &quot;movieID&quot;, &quot;character&quot;, &quot;text&quot;]MOVIE_CONVERSATIONS_FIELDS = [&quot;character1ID&quot;, &quot;character2ID&quot;, &quot;movieID&quot;, &quot;utteranceIDs&quot;]# 首先使用loadLines函数处理movie_lines.txt print(&quot;\\nProcessing corpus...&quot;)lines = loadLines(os.path.join(corpus, &quot;movie_lines.txt&quot;), MOVIE_LINES_FIELDS)# 接着使用loadConversations处理上一步的结果，得到conversationsprint(&quot;\\nLoading conversations...&quot;)conversations = loadConversations(os.path.join(corpus, &quot;movie_conversations.txt&quot;), lines, MOVIE_CONVERSATIONS_FIELDS)# 输出到一个新的csv文件print(&quot;\\nWriting newly formatted file...&quot;)with open(datafile, 'w', encoding='utf-8') as outputfile: writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n') # 使用extractSentencePairs从conversations里抽取句对。 for pair in extractSentencePairs(conversations): writer.writerow(pair)# 输出一些行用于检查 print(&quot;\\nSample lines from file:&quot;)printLines(datafile) 上面的代码会生成一个新的文件formatted_movie_lines.txt，这文件每一行包含一对句对，用tab分割。下面是前十行： 12345678910b&quot;Can we make this quick? Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad. Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n&quot;b&quot;Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part. Please.\\n&quot;b&quot;Not the hacking and gagging and spitting part. Please.\\tOkay... then how 'bout we try out some French cuisine. Saturday? Night?\\n&quot;b&quot;You're asking me out. That's so cute. What's your name again?\\tForget it.\\n&quot;b&quot;No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n&quot;b&quot;Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser. My sister. I can't date until she does.\\n&quot;b&quot;The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser. My sister. I can't date until she does.\\tSeems like she could get a date easy enough...\\n&quot;b'Why?\\tUnsolved mystery. She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n'b&quot;Unsolved mystery. She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n&quot;b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n' 创建词典接下来我们需要构建词典然后把问答句对加载到内存里。 我们的输入是一个句对，每个句子都是词的序列，但是机器学习只能处理数值，因此我们需要建立词到数字ID的映射。 为此，我们会定义一个Voc类，它会保存词到ID的映射，同时也保存反向的从ID到词的映射。除此之外，它还记录每个词出现的次数，以及总共出现的词的个数。这个类提供addWord方法来增加一个词， addSentence方法来增加句子，也提供方法trim来去除低频的词。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# 预定义的tokenPAD_token = 0 # 表示padding SOS_token = 1 # 句子的开始 EOS_token = 2 # 句子的结束 class Voc: def __init__(self, name): self.name = name self.trimmed = False self.word2index = {} self.word2count = {} self.index2word = {PAD_token: &quot;PAD&quot;, SOS_token: &quot;SOS&quot;, EOS_token: &quot;EOS&quot;} self.num_words = 3 # 目前有SOS, EOS, PAD这3个token。 def addSentence(self, sentence): for word in sentence.split(' '): self.addWord(word) def addWord(self, word): if word not in self.word2index: self.word2index[word] = self.num_words self.word2count[word] = 1 self.index2word[self.num_words] = word self.num_words += 1 else: self.word2count[word] += 1 # 删除频次小于min_count的token def trim(self, min_count): if self.trimmed: return self.trimmed = True keep_words = [] for k, v in self.word2count.items(): if v &gt;= min_count: keep_words.append(k) print('keep_words {} / {} = {:.4f}'.format( len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index) )) # 重新构造词典 self.word2index = {} self.word2count = {} self.index2word = {PAD_token: &quot;PAD&quot;, SOS_token: &quot;SOS&quot;, EOS_token: &quot;EOS&quot;} self.num_words = 3 # Count default tokens # 重新构造后词频就没有意义了(都是1) for word in keep_words: self.addWord(word) 有了上面的Voc类我们就可以通过问答句对来构建词典了。但是在构建之前我们需要进行一些预处理。 首先我们需要使用函数unicodeToAscii来把unicode字符变成ascii，比如把à变成a。注意，这里的代码只是用于处理西方文字，如果是中文，这个函数直接会丢弃掉。接下来把所有字母变成小写同时丢弃掉字母和常见标点(.!?)之外的所有字符。最后为了训练收敛，我们会用函数filterPairs去掉长度超过MAX_LENGTH的句子(句对)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162MAX_LENGTH = 10 # 句子最大长度是10个词(包括EOS等特殊词)# 把Unicode字符串变成ASCII# 参考https://stackoverflow.com/a/518232/2809427def unicodeToAscii(s): return ''.join( c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn' ) def normalizeString(s): # 变成小写、去掉前后空格，然后unicode变成ascii s = unicodeToAscii(s.lower().strip()) # 在标点前增加空格，这样把标点当成一个词 s = re.sub(r&quot;([.!?])&quot;, r&quot; \\1&quot;, s) # 字母和标点之外的字符都变成空格 s = re.sub(r&quot;[^a-zA-Z.!?]+&quot;, r&quot; &quot;, s) # 因为把不用的字符都变成空格，所以可能存在多个连续空格 # 下面的正则替换把多个空格变成一个空格，最后去掉前后空格 s = re.sub(r&quot;\\s+&quot;, r&quot; &quot;, s).strip() return s# 读取问答句对并且返回Voc词典对象 def readVocs(datafile, corpus_name): print(&quot;Reading lines...&quot;) # 文件每行读取到list lines中。 lines = open(datafile, encoding='utf-8').\\ read().strip().split('\\n') # 每行用tab切分成问答两个句子，然后调用normalizeString函数进行处理。 pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines] voc = Voc(corpus_name) return voc, pairsdef filterPair(p): return len(p[0].split(' ')) &lt; MAX_LENGTH and len(p[1].split(' ')) &lt; MAX_LENGTH# 过滤太长的句对 def filterPairs(pairs): return [pair for pair in pairs if filterPair(pair)]# 使用上面的函数进行处理，返回Voc对象和句对的list def loadPrepareData(corpus, corpus_name, datafile): print(&quot;Start preparing training data ...&quot;) voc, pairs = readVocs(datafile, corpus_name) print(&quot;Read {!s} sentence pairs&quot;.format(len(pairs))) pairs = filterPairs(pairs) print(&quot;Trimmed to {!s} sentence pairs&quot;.format(len(pairs))) print(&quot;Counting words...&quot;) for pair in pairs: voc.addSentence(pair[0]) voc.addSentence(pair[1]) print(&quot;Counted words:&quot;, voc.num_words) return voc, pairs# Load/Assemble voc and pairs# save_dir = os.path.join(&quot;data&quot;, &quot;save&quot;)voc, pairs = loadPrepareData(corpus, corpus_name, datafile)# 输出一些句对print(&quot;\\npairs:&quot;)for pair in pairs[:10]: print(pair) 上面的代码的输出为： 123456Start preparing training data ...Reading lines...Read 221282 sentence pairsTrimmed to 64271 sentence pairsCounting words...Counted words: 18008 我们可以看到，原来共有221282个句对，经过处理后我们只保留了64271个句对。 另外为了收敛更快，我们可以去除掉一些低频词。这可以分为两步： 使用voc.trim函数去掉频次低于MIN_COUNT 的词。 去掉包含低频词的句子(只保留这样的句子——每一个词都是高频的，也就是在voc中出现的) 1234567891011121314151617181920212223242526272829303132333435MIN_COUNT = 3 # 阈值为3def trimRareWords(voc, pairs, MIN_COUNT): # 去掉voc中频次小于3的词 voc.trim(MIN_COUNT) # 保留的句对 keep_pairs = [] for pair in pairs: input_sentence = pair[0] output_sentence = pair[1] keep_input = True keep_output = True # 检查问题 for word in input_sentence.split(' '): if word not in voc.word2index: keep_input = False break # 检查答案 for word in output_sentence.split(' '): if word not in voc.word2index: keep_output = False break # 如果问题和答案都只包含高频词，我们才保留这个句对 if keep_input and keep_output: keep_pairs.append(pair) print(&quot;Trimmed from {} pairs to {}, {:.4f} of total&quot;.format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs))) return keep_pairs# 实际进行处理pairs = trimRareWords(voc, pairs, MIN_COUNT) 代码的输出为： 12keep_words 7823 / 18005 = 0.4345Trimmed from 64271 pairs to 53165, 0.8272 of total 18005个词之中，频次大于等于3的只有43%，去掉低频的57%的词之后，保留的句子为53165，占比为82%。 为模型准备数据前面我们构建了词典，并且对训练数据进行预处理并且滤掉一些句对，但是模型最终用到的是Tensor。最简单的办法是一次处理一个句对，那么上面得到的句对直接就可以使用。但是为了加快训练速度，尤其是重复利用GPU的并行能力，我们需要一次处理一个batch的数据。 对于某些问题，比如图像来说，输入可能是固定大小的(或者通过预处理缩放成固定大小），但是对于文本来说，我们很难把一个二十个词的句子”缩放”成十个词同时还保持语义不变。但是为了充分利用GPU等计算自由，我们又必须变成固定大小的Tensor，因此我们通常会使用Padding的技巧，把短的句子补充上零使得输入大小是(batch, max_length)，这样通过一次就能实现一个batch数据的forward或者backward计算。当然padding的部分的结果是没有意义的，比如某个句子实际长度是5，而max_length是10，那么最终forward的输出应该是第5个时刻的输出，后面5个时刻计算是无用功。方向计算梯度的时候也是类似的，我们需要从第5个时刻开始反向计算梯度。为了提高效率，我们通常把长度接近的训练数据放到一个batch里面，这样无用的计算是最少的。因此我们通常把全部训练数据根据长度划分成一些组，比如长度小于4的一组，长度4到8的一组，长度8到12的一组，…。然后每次随机的选择一个组，再随机的从一组里选择batch个数据。不过本教程并没有这么做，而是每次随机的从所有pair里随机选择batch个数据。 原始的输入通常是batch个list，表示batch个句子，因此自然的表示方法为(batch, max_length)，这种表示方法第一维是batch，每移动一个下标得到的是一个样本的max_length个词(包括padding)。因为RNN的依赖关系，我们在计算t+1时刻必须知道t时刻的结果，因此我们无法用多个核同时计算一个样本的forward。但是不同样本之间是没有依赖关系的，因此我们可以在根据t时刻batch样本的当前状态计算batch个样本的输出和新状态，然后再计算t+2时刻，…。为了便于GPU一次取出t时刻的batch个数据，我们通常把输入从(batch, max_length)变成(max_length, batch)，这样使得t时刻的batch个数据在内存(显存)中是连续的，从而读取效率更高。这个过程如下图所示，原始输入的大小是(batch=6, max_length=4)，转置之后变成(4,6)。这样某个时刻的6个样本数据在内存中是连续的。 因此我们会用一些工具函数来实现上述处理。 inputVar函数把batch个句子padding后变成一个LongTensor，大小是(max_length, batch)，同时会返回一个大小是batch的list lengths，说明每个句子的实际长度，这个参数后面会传给PyTorch，从而在forward和backward计算的时候使用实际的长度。 outputVar函数和inputVar类似，但是它输出的第二个参数不是lengths，而是一个大小为(max_length, batch)的mask矩阵(tensor)，某位是0表示这个位置是padding，1表示不是padding，这样做的目的是后面计算方便。当然这两种表示是等价的，只不过lengths表示更加紧凑，但是计算起来不同方便，而mask矩阵和outputVar直接相乘就可以把padding的位置给mask(变成0)掉，这在计算loss时会非常方便。 batch2TrainData 则利用上面的两个函数把一个batch的句对处理成合适的输入和输出Tensor。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667# 把句子的词变成IDdef indexesFromSentence(voc, sentence): return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]# l是多个长度不同句子(list)，使用zip_longest padding成定长，长度为最长句子的长度。def zeroPadding(l, fillvalue=PAD_token): return list(itertools.zip_longest(*l, fillvalue=fillvalue))# l是二维的padding后的list# 返回m和l的大小一样，如果某个位置是padding，那么值为0，否则为1def binaryMatrix(l, value=PAD_token): m = [] for i, seq in enumerate(l): m.append([]) for token in seq: if token == PAD_token: m[i].append(0) else: m[i].append(1) return m# 把输入句子变成ID，然后再padding，同时返回lengths这个list，标识实际长度。# 返回的padVar是一个LongTensor，shape是(batch, max_length)，# lengths是一个list，长度为(batch,)，表示每个句子的实际长度。def inputVar(l, voc): indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l] lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) padList = zeroPadding(indexes_batch) padVar = torch.LongTensor(padList) return padVar, lengths# 对输出句子进行padding，然后用binaryMatrix得到每个位置是padding(0)还是非padding，# 同时返回最大最长句子的长度(也就是padding后的长度)# 返回值padVar是LongTensor，shape是(batch, max_target_length)# mask是ByteTensor，shape也是(batch, max_target_length)def outputVar(l, voc): indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l] max_target_len = max([len(indexes) for indexes in indexes_batch]) padList = zeroPadding(indexes_batch) mask = binaryMatrix(padList) mask = torch.ByteTensor(mask) padVar = torch.LongTensor(padList) return padVar, mask, max_target_len# 处理一个batch的pair句对 def batch2TrainData(voc, pair_batch): # 按照句子的长度(词数)排序 pair_batch.sort(key=lambda x: len(x[0].split(&quot; &quot;)), reverse=True) input_batch, output_batch = [], [] for pair in pair_batch: input_batch.append(pair[0]) output_batch.append(pair[1]) inp, lengths = inputVar(input_batch, voc) output, mask, max_target_len = outputVar(output_batch, voc) return inp, lengths, output, mask, max_target_len# 示例small_batch_size = 5batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])input_variable, lengths, target_variable, mask, max_target_len = batchesprint(&quot;input_variable:&quot;, input_variable)print(&quot;lengths:&quot;, lengths)print(&quot;target_variable:&quot;, target_variable)print(&quot;mask:&quot;, mask)print(&quot;max_target_len:&quot;, max_target_len) 示例的输出为： 123456789101112131415161718192021222324252627282930input_variable: tensor( [[ 92, 101, 76, 50, 34], [ 7, 250, 37, 6, 4], [ 123, 279, 628, 2, 2], [ 40, 75, 4, 0, 0], [ 359, 53, 7216, 0, 0], [ 2763, 217, 4, 0, 0], [ 637, 4, 2, 0, 0], [ 6, 2, 0, 0, 0], [ 2, 0, 0, 0, 0]])lengths: tensor([ 9, 8, 7, 3, 3])target_variable: tensor( [[ 25, 34, 404, 7, 25], [ 283, 4, 76, 24, 1464], [ 25, 2, 37, 4, 70], [ 72, 0, 7217, 2, 1465], [ 829, 0, 4, 0, 6], [ 234, 0, 2, 0, 2], [ 4, 0, 0, 0, 0], [ 2, 0, 0, 0, 0]])mask: tensor( [[ 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1], [ 1, 1, 1, 1, 1], [ 1, 0, 1, 1, 1], [ 1, 0, 1, 0, 1], [ 1, 0, 1, 0, 1], [ 1, 0, 0, 0, 0], [ 1, 0, 0, 0, 0]], dtype=torch.uint8)max_target_len: 8 我们可以看到input_variable的每一列表示一个样本，而每一行表示batch(5)个样本在这个时刻的值。而lengths表示真实的长度。类似的target_variable也是每一列表示一个样本，而mask的shape和target_variable一样，如果某个位置是0，则表示padding。 定义模型Seq2Seq 模型我们这个chatbot的核心是一个sequence-to-sequence(seq2seq)模型。 seq2seq模型的输入是一个变长的序列，而输出也是一个变长的序列。而且这两个序列的长度并不相同。一般我们使用RNN来处理变长的序列，Sutskever等人的论文发现通过使用两个RNN可以解决这类问题。这类问题的输入和输出都是变长的而且长度不一样，包括问答系统、机器翻译、自动摘要等等都可以使用seq2seq模型来解决。其中一个RNN叫做Encoder，它把变长的输入序列编码成一个固定长度的context向量，我们一般可以认为这个向量包含了输入句子的语义。而第二个RNN叫做Decoder，初始隐状态是Encoder的输出context向量，输入是(表示句子开始的特殊Token)，然后用RNN计算第一个时刻的输出，接着用第一个时刻的输出和隐状态计算第二个时刻的输出和新的隐状态，…，直到某个时刻输出特殊的(表示句子结束的特殊Token)或者长度超过一个阈值。Seq2Seq模型如下图所示。 EncoderEncoder是个RNN，它会遍历输入的每一个Token(词)，每个时刻的输入是上一个时刻的隐状态和输入，然后会有一个输出和新的隐状态。这个新的隐状态会作为下一个时刻的输入隐状态。每个时刻都有一个输出，对于seq2seq模型来说，我们通常只保留最后一个时刻的隐状态，认为它编码了整个句子的语义，但是后面我们会用到Attention机制，它还会用到Encoder每个时刻的输出。Encoder处理结束后会把最后一个时刻的隐状态作为Decoder的初始隐状态。 实际我们通常使用多层的Gated Recurrent Unit(GRU)或者LSTM来作为Encoder，这里使用GRU，读者可以参考Cho等人2014年的[论文]。 此外我们会使用双向的RNN，如下图所示。 注意在接入RNN之前会有一个embedding层，用来把每一个词(ID或者one-hot向量)映射成一个连续的稠密的向量，我们可以认为这个向量编码了一个词的语义。在我们的模型里，我们把它的大小定义成和RNN的隐状态大小一样(但是并不是一定要一样)。有了Embedding之后，模型会把相似的词编码成相似的向量(距离比较近)。 最后，为了把padding的batch数据传给RNN，我们需要使用下面的两个函数来进行pack和unpack，后面我们会详细介绍它们。这两个函数是： torch.nn.utils.rnn.pack_padded_sequence torch.nn.utils.rnn.pad_packed_sequence 计算图: 把词的ID通过Embedding层变成向量。 2) 把padding后的数据进行pack。 3) 传入GRU进行Forward计算。 4) Unpack计算结果 5) 把双向GRU的结果向量加起来。 6) 返回(所有时刻的)输出和最后时刻的隐状态。 输入: input_seq: 一个batch的输入句子，shape是(max_length, batch_size) input_lengths: 一个长度为batch的list，表示句子的实际长度。 hidden: 初始化隐状态(通常是零)，shape是(n_layers x num_directions, batch_size, hidden_size) 输出: outputs: 最后一层GRU的输出向量(双向的向量加在了一起)，shape(max_length, batch_size, hidden_size) hidden: 最后一个时刻的隐状态，shape是(n_layers x num_directions, batch_size, hidden_size) EncoderRNN代码如下，请读者详细阅读注释。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354class EncoderRNN(nn.Module): def __init__(self, hidden_size, embedding, n_layers=1, dropout=0): super(EncoderRNN, self).__init__() self.n_layers = n_layers self.hidden_size = hidden_size self.embedding = embedding # 初始化GRU，这里输入和hidden大小都是hidden_size，这里假设embedding层的输出大小是hidden_size # 如果只有一层，那么不进行Dropout，否则使用传入的参数dropout进行GRU的Dropout。 self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), bidirectional=True) def forward(self, input_seq, input_lengths, hidden=None): # 输入是(max_length, batch)，Embedding之后变成(max_length, batch, hidden_size) embedded = self.embedding(input_seq) # Pack padded batch of sequences for RNN module # 因为RNN(GRU)要知道实际长度，所以PyTorch提供了函数pack_padded_sequence把输入向量和长度 # pack到一个对象PackedSequence里，这样便于使用。 packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) # 通过GRU进行forward计算，需要传入输入和隐变量 # 如果传入的输入是一个Tensor (max_length, batch, hidden_size) # 那么输出outputs是(max_length, batch, hidden_size*num_directions)。 # 第三维是hidden_size和num_directions的混合，它们实际排列顺序是num_directions在前面， # 因此我们可以使用outputs.view(seq_len, batch, num_directions, hidden_size)得到4维的向量。 # 其中第三维是方向，第四位是隐状态。 # 而如果输入是PackedSequence对象，那么输出outputs也是一个PackedSequence对象，我们需要用 # 函数pad_packed_sequence把它变成shape为(max_length, batch, hidden*num_directions)的向量以及 # 一个list，表示输出的长度，当然这个list和输入的input_lengths完全一样，因此通常我们不需要它。 outputs, hidden = self.gru(packed, hidden) # 参考前面的注释，我们得到outputs为(max_length, batch, hidden*num_directions) outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs) # 我们需要把输出的num_directions双向的向量加起来 # 因为outputs的第三维是先放前向的hidden_size个结果，然后再放后向的hidden_size个结果 # 所以outputs[:, :, :self.hidden_size]得到前向的结果 # outputs[:, :, self.hidden_size:]是后向的结果 # 注意，如果bidirectional是False，则outputs第三维的大小就是hidden_size， # 这时outputs[:, : ,self.hidden_size:]是不存在的，因此也不会加上去。 # 对Python slicing不熟的读者可以看看下面的例子： # &gt;&gt;&gt; a=[1,2,3] # &gt;&gt;&gt; a[:3] # [1, 2, 3] # &gt;&gt;&gt; a[3:] # [] # &gt;&gt;&gt; a[:3]+a[3:] # [1, 2, 3] # 这样就不用写下面的代码了： # if bidirectional: # outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # 返回最终的输出和最后时刻的隐状态。 return outputs, hidden DecoderDecoder也是一个RNN，它每个时刻输出一个词。每个时刻的输入是上一个时刻的隐状态和上一个时刻的输出。一开始的隐状态是Encoder最后时刻的隐状态，输入是特殊的。然后使用RNN计算新的隐状态和输出第一个词，接着用新的隐状态和第一个词计算第二个词，…，直到遇到，结束输出。普通的RNN Decoder的问题是它只依赖与Encoder最后一个时刻的隐状态，虽然理论上这个隐状态(context向量)可以编码输入句子的语义，但是实际会比较困难。因此当输入句子很长的时候，效果会很长。 为了解决这个问题，Bahdanau等人在论文里提出了注意力机制(attention mechanism)，在Decoder进行t时刻计算的时候，除了t-1时刻的隐状态，当前时刻的输入，注意力机制还可以参考Encoder所有时刻的输入。拿机器翻译来说，我们在翻译以句子的第t个词的时候会把注意力机制在某个词上。当然常见的注意力是一种soft的注意力，假设输入有5个词，注意力可能是一个概率，比如(0.6,0.1,0.1,0.1,0.1)，表示当前最关注的是输入的第一个词。同时我们之前也计算出每个时刻的输出向量，假设5个时刻分别是𝑦1,…,𝑦5�1,…,�5，那么我们可以用attention概率加权得到当前时刻的context向量0.6𝑦1+0.1𝑦2+…+0.1𝑦50.6�1+0.1�2+…+0.1�5。 注意力有很多方法计算，我们这里介绍Luong等人在论文提出的方法。它是用当前时刻的GRU计算出的新的隐状态来计算注意力得分，首先它用一个score函数计算这个隐状态和Encoder的输出的相似度得分，得分越大，说明越应该注意这个词。然后再用softmax函数把score变成概率。那机器翻译为例，在t时刻，ℎ𝑡ℎ�表示t时刻的GRU输出的新的隐状态，我们可以认为ℎ𝑡ℎ�表示当前需要翻译的语义。通过计算ℎ𝑡ℎ�与𝑦1,…,𝑦𝑛�1,…,��的得分，如果ℎ𝑡ℎ�与𝑦1�1的得分很高，那么我们可以认为当前主要翻译词𝑥1�1的语义。有很多中score函数的计算方法，如下图所示： 上式中ℎ𝑡ℎ�表示t时刻的隐状态，比如第一种计算score的方法，直接计算ℎ𝑡ℎ�与ℎ𝑠ℎ�的内积，内积越大，说明这两个向量越相似，因此注意力也更多的放到这个词上。第二种方法也类似，只是引入了一个可以学习的矩阵，我们可以认为它先对ℎ𝑡ℎ�做一个线性变换，然后在与ℎ𝑠ℎ�计算内积。而第三种方法把它们拼接起来然后用一个全连接网络来计算score。 注意，我们前面介绍的是分别计算ℎ𝑡ℎ�和𝑦1�1的内积、ℎ𝑡ℎ�和𝑦2�2的内积，…。但是为了效率，可以一次计算ℎ𝑡ℎ�与ℎ𝑠=[𝑦1,𝑦2,…,𝑦𝑛]ℎ�=[�1,�2,…,��]的乘积。 计算过程如下图所示。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# Luong 注意力layerclass Attn(torch.nn.Module): def __init__(self, method, hidden_size): super(Attn, self).__init__() self.method = method if self.method not in ['dot', 'general', 'concat']: raise ValueError(self.method, &quot;is not an appropriate attention method.&quot;) self.hidden_size = hidden_size if self.method == 'general': self.attn = torch.nn.Linear(self.hidden_size, hidden_size) elif self.method == 'concat': self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size) self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size)) def dot_score(self, hidden, encoder_output): # 输入hidden的shape是(1, batch=64, hidden_size=500) # encoder_outputs的shape是(input_lengths=10, batch=64, hidden_size=500) # hidden * encoder_output得到的shape是(10, 64, 500)，然后对第3维求和就可以计算出score。 return torch.sum(hidden * encoder_output, dim=2) def general_score(self, hidden, encoder_output): energy = self.attn(encoder_output) return torch.sum(hidden * energy, dim=2) def concat_score(self, hidden, encoder_output): energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh() return torch.sum(self.v * energy, dim=2) # 输入是上一个时刻的隐状态hidden和所有时刻的Encoder的输出encoder_outputs # 输出是注意力的概率，也就是长度为input_lengths的向量，它的和加起来是1。 def forward(self, hidden, encoder_outputs): # 计算注意力的score，输入hidden的shape是(1, batch=64, hidden_size=500)， # 表示t时刻batch数据的隐状态 # encoder_outputs的shape是(input_lengths=10, batch=64, hidden_size=500) if self.method == 'general': attn_energies = self.general_score(hidden, encoder_outputs) elif self.method == 'concat': attn_energies = self.concat_score(hidden, encoder_outputs) elif self.method == 'dot': # 计算内积，参考dot_score函数 attn_energies = self.dot_score(hidden, encoder_outputs) # Transpose max_length and batch_size dimensions # 把attn_energies从(max_length=10, batch=64)转置成(64, 10) attn_energies = attn_energies.t() # 使用softmax函数把score变成概率，shape仍然是(64, 10)，然后用unsqueeze(1)变成 # (64, 1, 10) return F.softmax(attn_energies, dim=1).unsqueeze(1) 上面的代码实现了dot、general和concat三种score计算方法，分别和前面的三个公式对应，我们这里介绍最简单的dot方法。代码里也有一些注释，只有dot_score函数比较难以理解，我们来分析一下。首先这个函数的输入输入hidden的shape是(1, batch=64, hidden_size=500)，encoder_outputs的shape是(input_lengths=10, batch=64, hidden_size=500)。 怎么计算hidden和10个encoder输出向量的内积呢？为了简便，我们先假设batch是1，这样可以把第二维(batch维)去掉，因此hidden是(1, 500)，而encoder_outputs是(10, 500)。内积的定义是两个向量对应位相乘然后相加，但是encoder_outputs是10个500维的向量。当然我们可以写一个for循环来计算，但是效率很低。这里用到一个小的技巧，利用broadcasting，hidden * encoder_outputs可以理解为把hidden从(1,500)复制成(10, 500)（当然实际实现并不会这么做），然后两个(10, 500)的矩阵进行乘法。注意，这里的乘法不是矩阵乘法，而是所谓的Hadamard乘法，其实就是把对应位置的乘起来，比如下面的例子： [142536]∗[121212]=[1∗14∗22∗15∗23∗16∗2][123456]∗[111222]=[1∗12∗13∗14∗25∗26∗2] 因此hidden * encoder_outputs就可以把hidden向量(500个数)与encoder_outputs的10个向量(500个数)对应的位置相乘。而内积还需要把这500个乘积加起来，因此后面使用torch.sum(hidden * encoder_output, dim=2)，把第2维500个乘积加起来，最终得到10个score值。当然我们实际还有一个batch维度，因此最终得到的attn_energies是(10, 64)。接着在forward函数里把attn_energies转置成(64, 10)，然后使用softmax函数把10个score变成概率，shape仍然是(64, 10)，为了后面使用方便，我们用unsqueeze(1)把它变成(64, 1, 10)。 有了注意力的子模块之后，我们就可以实现Decoder了。Encoder可以一次把一个序列输入GRU，得到整个序列的输出。但是Decoder t时刻的输入是t-1时刻的输出，在t-1时刻计算完成之前是未知的，因此只能一次处理一个时刻的数据。因此Encoder的GRU的输入是(max_length, batch, hidden_size)，而Decoder的输入是(1, batch, hidden_size)。此外Decoder只能利用前面的信息，所以只能使用单向(而不是双向)的GRU，而Encoder的GRU是双向的，如果两种的hidden_size是一样的，则Decoder的隐单元个数少了一半，那怎么把Encoder的最后时刻的隐状态作为Decoder的初始隐状态呢？这里是把每个时刻双向结果加起来的，因此它们的大小就能匹配了（请读者参考前面Encoder双向相加的部分代码）。 计算图: 把词ID输入Embedding层 2) 使用单向的GRU继续Forward进行一个时刻的计算。 3) 使用新的隐状态计算注意力权重 4) 用注意力权重得到context向量 5) context向量和GRU的输出拼接起来，然后再进过一个全连接网络，使得输出大小仍然是hidden_size 6) 使用一个投影矩阵把输出从hidden_size变成词典大小，然后用softmax变成概率 7) 返回输出和新的隐状态 输入: input_step: shape是(1, batch_size) last_hidden: 上一个时刻的隐状态， shape是(n_layers x num_directions, batch_size, hidden_size) encoder_outputs: encoder的输出， shape是(max_length, batch_size, hidden_size) 输出: output: 当前时刻输出每个词的概率，shape是(batch_size, voc.num_words) hidden: 新的隐状态，shape是(n_layers x num_directions, batch_size, hidden_size) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class LuongAttnDecoderRNN(nn.Module): def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1): super(LuongAttnDecoderRNN, self).__init__() # 保存到self里，attn_model就是前面定义的Attn类的对象。 self.attn_model = attn_model self.hidden_size = hidden_size self.output_size = output_size self.n_layers = n_layers self.dropout = dropout # 定义Decoder的layers self.embedding = embedding self.embedding_dropout = nn.Dropout(dropout) self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout)) self.concat = nn.Linear(hidden_size * 2, hidden_size) self.out = nn.Linear(hidden_size, output_size) self.attn = Attn(attn_model, hidden_size) def forward(self, input_step, last_hidden, encoder_outputs): # 注意：decoder每一步只能处理一个时刻的数据，因为t时刻计算完了才能计算t+1时刻。 # input_step的shape是(1, 64)，64是batch，1是当前输入的词ID(来自上一个时刻的输出) # 通过embedding层变成(1, 64, 500)，然后进行dropout，shape不变。 embedded = self.embedding(input_step) embedded = self.embedding_dropout(embedded) # 把embedded传入GRU进行forward计算 # 得到rnn_output的shape是(1, 64, 500) # hidden是(2, 64, 500)，因为是两层的GRU，所以第一维是2。 rnn_output, hidden = self.gru(embedded, last_hidden) # 计算注意力权重， 根据前面的分析，attn_weights的shape是(64, 1, 10) attn_weights = self.attn(rnn_output, encoder_outputs) # encoder_outputs是(10, 64, 500) # encoder_outputs.transpose(0, 1)后的shape是(64, 10, 500) # attn_weights.bmm后是(64, 1, 500) # bmm是批量的矩阵乘法，第一维是batch，我们可以把attn_weights看成64个(1,10)的矩阵 # 把encoder_outputs.transpose(0, 1)看成64个(10, 500)的矩阵 # 那么bmm就是64个(1, 10)矩阵 x (10, 500)矩阵，最终得到(64, 1, 500) context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # 把context向量和GRU的输出拼接起来 # rnn_output从(1, 64, 500)变成(64, 500) rnn_output = rnn_output.squeeze(0) # context从(64, 1, 500)变成(64, 500) context = context.squeeze(1) # 拼接得到(64, 1000) concat_input = torch.cat((rnn_output, context), 1) # self.concat是一个矩阵(1000, 500)， # self.concat(concat_input)的输出是(64, 500) # 然后用tanh把输出返回变成(-1,1)，concat_output的shape是(64, 500) concat_output = torch.tanh(self.concat(concat_input)) # out是(500, 词典大小=7826) output = self.out(concat_output) # 用softmax变成概率，表示当前时刻输出每个词的概率。 output = F.softmax(output, dim=1) # 返回 output和新的隐状态 return output, hidden 定义训练过程Masked损失forward实现之后，我们就需要计算loss。seq2seq有两个RNN，Encoder RNN是没有直接定义损失函数的，它是通过影响Decoder从而影响最终的输出以及loss。Decoder输出一个序列，前面我们介绍的是Decoder在预测时的过程，它的长度是不固定的，只有遇到EOS才结束。给定一个问答句对，我们可以把问题输入Encoder，然后用Decoder得到一个输出序列，但是这个输出序列和”真实”的答案长度并不相同。 而且即使长度相同并且语义相似，也很难直接知道预测的答案和真实的答案是否类似。那么我们怎么计算loss呢？比如输入是”What is your name?”，训练数据中的答案是”I am LiLi”。假设模型有两种预测：”I am fine”和”My name is LiLi”。从语义上显然第二种答案更好，但是如果字面上比较的话可能第一种更好。 但是让机器知道”I am LiLi”和”My name is LiLi”的语义很接近这是非常困难的，所以实际上我们通常还是通过字面上里进行比较。我们会限制Decoder的输出，使得Decoder的输出长度和”真实”答案一样，然后逐个时刻比较。Decoder输出的是每个词的概率分布，因此可以使用交叉熵损失函数。但是这里还有一个问题，因为是一个batch的数据里有一些是padding的，因此这些位置的预测是没有必要计算loss的，因此我们需要使用前面的mask矩阵把对应位置的loss去掉，我们可以通过下面的函数来实现计算Masked的loss。 12345678def maskNLLLoss(inp, target, mask): # 计算实际的词的个数，因为padding是0，非padding是1，因此sum就可以得到词的个数 nTotal = mask.sum() crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1)) loss = crossEntropy.masked_select(mask).mean() loss = loss.to(device) return loss, nTotal.item() 上面的代码有几个需要注意的地方。首先是masked_select函数，我们来看一个例子： 123456789101112&gt;&gt;&gt; x = torch.randn(3, 4)&gt;&gt;&gt; xtensor([[ 0.3552, -2.3825, -0.8297, 0.3477], [-1.2035, 1.2252, 0.5002, 0.6248], [ 0.1307, -2.0608, 0.1244, 2.0139]])&gt;&gt;&gt; mask = x.ge(0.5)&gt;&gt;&gt; masktensor([[ 0, 0, 0, 0], [ 0, 1, 1, 1], [ 0, 0, 0, 1]], dtype=torch.uint8)&gt;&gt;&gt; torch.masked_select(x, mask)tensor([ 1.2252, 0.5002, 0.6248, 2.0139]) 它要求mask和被mask的tensor的shape是一样的，然后从crossEntropy选出mask值为1的那些值。输出的维度会减1。 另外为了实现交叉熵这里使用了gather函数，这是一种比较底层的实现方法，更简便的方法应该使用CrossEntropyLoss或者NLLLoss，其中CrossEntropy等价与LogSoftmax+NLLLoss。 交叉熵的定义为：𝐻(𝑝,𝑞)=−∑𝑥𝑝(𝑥)𝑙𝑜𝑔𝑞(𝑥)�(�,�)=−∑��(�)����(�)。其中p和q是两个随机变量的概率分布，这里是离散的随机变量，如果是连续的需要把求和变成积分。在我们这里p是真实的分布，也就是one-hot的，而q是模型预测的softmax的输出。因为p是one-hot的，所以只需要计算真实分类对应的那个值。 比如假设一个5分类的问题，当前正确分类是2(下标从0-4)，而模型的预测是(0.1,0.1,0.4,0.2,0.2)，则H=-log(0.4)。用交叉熵作为分类的Loss是比较合理的，正确的分类是2，那么模型在下标为2的地方预测的概率𝑞2�2越大，则−𝑙𝑜𝑔𝑞2−����2越小，也就是loss越小。 假设inp是： 120.3 0.2 0.4 0.10.2 0.1 0.4 0.3 也就是batch=2，而分类数(词典大小)是4，inp是模型预测的分类概率。 而target = [2,3] ，表示第一个样本的正确分类是第三个类别(概率是0.4），第二个样本的正确分类是第四个类别(概率是0.3)。因此我们需要计算的是 -log(0.4) - log(0.3)。怎么不用for循环求出来呢？我们可以使用torch.gather函数首先把0.4和0.3选出来： 1234567inp = torch.tensor([[0.3, 0.2, 0.4, 0.1], [0.2, 0.1, 0.4, 0.3]])target = torch.tensor([2, 3])selected = torch.gather(inp, 1, target.view(-1, 1))print(selected)输出：tensor([[ 0.4000], [ 0.3000]]) 一次迭代的训练过程函数train实现一个batch数据的训练。前面我们提到过，在训练的时候我们会限制Decoder的输出，使得Decoder的输出长度和”真实”答案一样长。但是我们在训练的时候如果让Decoder自行输出，那么收敛可能会比较慢，因为Decoder在t时刻的输入来自t-1时刻的输出。如果前面预测错了，那么后面很可能都会错下去。另外一种方法叫做teacher forcing，它不管模型在t-1时刻做什么预测都把t-1时刻的正确答案作为t时刻的输入。但是如果只用teacher forcing也有问题，因为在真实的Decoder的是是没有老师来帮它纠正错误的。所以比较好的方法是更加一个teacher_forcing_ratio参数随机的来确定本次训练是否teacher forcing。 另外使用到的一个技巧是梯度裁剪(gradient clipping) 。这个技巧通常是为了防止梯度爆炸(exploding gradient)，它把参数限制在一个范围之内，从而可以避免梯度的梯度过大或者出现NaN等问题。注意：虽然它的名字叫梯度裁剪，但实际它是对模型的参数进行裁剪，它把整个参数看成一个向量，如果这个向量的模大于max_norm，那么就把这个向量除以一个值使得模等于max_norm，因此也等价于把这个向量投影到半径为max_norm的球上。它的效果如下图所示。 操作步骤: 把整个batch的输入传入encoder 2) 把decoder的输入设置为特殊的，初始隐状态设置为encoder最后时刻的隐状态 3) decoder每次处理一个时刻的forward计算 4) 如果是teacher forcing，把上个时刻的”正确的”词作为当前输入，否则用上一个时刻的输出作为当前时刻的输入 5) 计算loss 6) 反向计算梯度 7) 对梯度进行裁剪 8) 更新模型(包括encoder和decoder)参数 注意，PyTorch的RNN模块(RNN, LSTM, GRU)也可以当成普通的非循环的网络来使用。在Encoder部分，我们是直接把所有时刻的数据都传入RNN，让它一次计算出所有的结果，但是在Decoder的时候(非teacher forcing)后一个时刻的输入来自前一个时刻的输出，因此无法一次计算。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH): # 梯度清空 encoder_optimizer.zero_grad() decoder_optimizer.zero_grad() # 设置device，从而支持GPU，当然如果没有GPU也能工作。 input_variable = input_variable.to(device) lengths = lengths.to(device) target_variable = target_variable.to(device) mask = mask.to(device) # 初始化变量 loss = 0 print_losses = [] n_totals = 0 # encoder的Forward计算 encoder_outputs, encoder_hidden = encoder(input_variable, lengths) # Decoder的初始输入是SOS，我们需要构造(1, batch)的输入，表示第一个时刻batch个输入。 decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]]) decoder_input = decoder_input.to(device) # 注意：Encoder是双向的，而Decoder是单向的，因此从下往上取n_layers个 decoder_hidden = encoder_hidden[:decoder.n_layers] # 确定是否teacher forcing use_teacher_forcing = True if random.random() &lt; teacher_forcing_ratio else False # 一次处理一个时刻 if use_teacher_forcing: for t in range(max_target_len): decoder_output, decoder_hidden = decoder( decoder_input, decoder_hidden, encoder_outputs ) # Teacher forcing: 下一个时刻的输入是当前正确答案 decoder_input = target_variable[t].view(1, -1) # 计算累计的loss mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t]) loss += mask_loss print_losses.append(mask_loss.item() * nTotal) n_totals += nTotal else: for t in range(max_target_len): decoder_output, decoder_hidden = decoder( decoder_input, decoder_hidden, encoder_outputs ) # 不是teacher forcing: 下一个时刻的输入是当前模型预测概率最高的值 _, topi = decoder_output.topk(1) decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]]) decoder_input = decoder_input.to(device) # 计算累计的loss mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t]) loss += mask_loss print_losses.append(mask_loss.item() * nTotal) n_totals += nTotal # 反向计算 loss.backward() # 对encoder和decoder进行梯度裁剪 _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip) _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip) # 更新参数 encoder_optimizer.step() decoder_optimizer.step() return sum(print_losses) / n_totals 训练迭代过程最后是把前面的代码组合起来进行训练。函数trainIters用于进行n_iterations次minibatch的训练。 值得注意的是我们定期会保存模型，我们会保存一个tar包，包括encoder和decoder的state_dicts(参数),优化器(optimizers)的state_dicts, loss和迭代次数。这样保存模型的好处是从中恢复后我们既可以进行预测也可以进行训练(因为有优化器的参数和迭代的次数)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename): # 随机选择n_iteration个batch的数据(pair) training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)] # 初始化 print('Initializing ...') start_iteration = 1 print_loss = 0 if loadFilename: start_iteration = checkpoint['iteration'] + 1 # 训练 print(&quot;Training...&quot;) for iteration in range(start_iteration, n_iteration + 1): training_batch = training_batches[iteration - 1] input_variable, lengths, target_variable, mask, max_target_len = training_batch # 训练一个batch的数据 loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip) print_loss += loss # 进度 if iteration % print_every == 0: print_loss_avg = print_loss / print_every print(&quot;Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}&quot; .format(iteration, iteration / n_iteration * 100, print_loss_avg)) print_loss = 0 # 保存checkpoint if (iteration % save_every == 0): directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}' .format(encoder_n_layers, decoder_n_layers, hidden_size)) if not os.path.exists(directory): os.makedirs(directory) torch.save({ 'iteration': iteration, 'en': encoder.state_dict(), 'de': decoder.state_dict(), 'en_opt': encoder_optimizer.state_dict(), 'de_opt': decoder_optimizer.state_dict(), 'loss': loss, 'voc_dict': voc.__dict__, 'embedding': embedding.state_dict() }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint'))) 效果测试模型训练完成之后，我们需要测试它的效果。最简单直接的方法就是和chatbot来聊天。因此我们需要用Decoder来生成一个响应。 贪心解码(Greedy decoding)算法最简单的解码算法是贪心算法，也就是每次都选择概率最高的那个词，然后把这个词作为下一个时刻的输入，直到遇到EOS结束解码或者达到一个最大长度。但是贪心算法不一定能得到最优解，因为某个答案可能开始的几个词的概率并不太高，但是后来概率会很大。因此除了贪心算法，我们通常也可以使用Beam-Search算法，也就是每个时刻保留概率最高的Top K个结果，然后下一个时刻尝试把这K个结果输入(当然需要能恢复RNN的状态)，然后再从中选择概率最高的K个。 为了实现贪心解码算法，我们定义一个GreedySearchDecoder类。这个类的forwar的方法需要传入一个输入序列(input_seq)，其shape是(input_seq length, 1)， 输入长度input_length和最大输出长度max_length。就是过程如下： 把输入传给Encoder，得到所有时刻的输出和最后一个时刻的隐状态。 2) 把Encoder最后时刻的隐状态作为Decoder的初始状态。 3) Decoder的第一输入初始化为SOS。 4) 定义保存解码结果的tensor 5) 循环直到最大解码长度 a) 把当前输入传入Decoder b) 得到概率最大的词以及概率 c) 把这个词和概率保存下来 d) 把当前输出的词作为下一个时刻的输入 6) 返回所有的词和概率 123456789101112131415161718192021222324252627282930313233class GreedySearchDecoder(nn.Module): def __init__(self, encoder, decoder): super(GreedySearchDecoder, self).__init__() self.encoder = encoder self.decoder = decoder def forward(self, input_seq, input_length, max_length): # Encoder的Forward计算 encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length) # 把Encoder最后时刻的隐状态作为Decoder的初始值 decoder_hidden = encoder_hidden[:decoder.n_layers] # 因为我们的函数都是要求(time,batch)，因此即使只有一个数据，也要做出二维的。 # Decoder的初始输入是SOS decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token # 用于保存解码结果的tensor all_tokens = torch.zeros([0], device=device, dtype=torch.long) all_scores = torch.zeros([0], device=device) # 循环，这里只使用长度限制，后面处理的时候把EOS去掉了。 for _ in range(max_length): # Decoder forward一步 decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs) # decoder_outputs是(batch=1, vob_size) # 使用max返回概率最大的词和得分 decoder_scores, decoder_input = torch.max(decoder_output, dim=1) # 把解码结果保存到all_tokens和all_scores里 all_tokens = torch.cat((all_tokens, decoder_input), dim=0) all_scores = torch.cat((all_scores, decoder_scores), dim=0) # decoder_input是当前时刻输出的词的ID，这是个一维的向量，因为max会减少一维。 # 但是decoder要求有一个batch维度，因此用unsqueeze增加batch维度。 decoder_input = torch.unsqueeze(decoder_input, 0) # 返回所有的词和得分。 return all_tokens, all_scores 测试对话函数解码方法完成后，我们写一个函数来测试从终端输入一个句子然后来看看chatbot的回复。我们需要用前面的函数来把句子分词，然后变成ID传入解码器，得到输出的ID后再转换成文字。我们会实现一个evaluate函数，由它来完成这些工作。我们需要把一个句子变成输入需要的格式——shape为(batch, max_length)，即使只有一个输入也需要增加一个batch维度。我们首先把句子分词，然后变成ID的序列，然后转置成合适的格式。此外我们还需要创建一个名为lengths的tensor，虽然只有一个，来表示输入的实际长度。接着我们构造类GreedySearchDecoder的实例searcher，然后用searcher来进行解码得到输出的ID，最后我们把这些ID变成词并且去掉EOS之后的内容。 另外一个evaluateInput函数作为chatbot的用户接口，当运行它的时候，它会首先提示用户输入一个句子，然后使用evaluate来生成回复。然后继续对话直到用户输入”q”或者”quit”。如果用户输入的词不在词典里，我们会输出错误信息(当然还有一种办法是忽略这些词)然后提示用户重新输入。 1234567891011121314151617181920212223242526272829303132333435363738394041def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH): ### 把输入的一个batch句子变成id indexes_batch = [indexesFromSentence(voc, sentence)] # 创建lengths tensor lengths = torch.tensor([len(indexes) for indexes in indexes_batch]) # 转置 input_batch = torch.LongTensor(indexes_batch).transpose(0, 1) # 放到合适的设备上(比如GPU) input_batch = input_batch.to(device) lengths = lengths.to(device) # 用searcher解码 tokens, scores = searcher(input_batch, lengths, max_length) # ID变成词。 decoded_words = [voc.index2word[token.item()] for token in tokens] return decoded_wordsdef evaluateInput(encoder, decoder, searcher, voc): input_sentence = '' while(1): try: # 得到用户终端的输入 input_sentence = input('&gt; ') # 是否退出 if input_sentence == 'q' or input_sentence == 'quit': break # 句子归一化 input_sentence = normalizeString(input_sentence) # 生成响应Evaluate sentence output_words = evaluate(encoder, decoder, searcher, voc, input_sentence) # 去掉EOS后面的内容 words = [] for word in output_words: if word == 'EOS': break elif word != 'PAD': words.append(word) print('Bot:', ' '.join(words)) except KeyError: print(&quot;Error: Encountered unknown word.&quot;) 训练和测试模型最后我们可以来训练模型和进行评测了。 不论是我们像训练模型还是测试对话，我们都需要初始化encoder和decoder模型参数。在下面的代码，我们从头开始训练模型或者从某个checkpoint加载模型。读者可以尝试不同的超参数配置来进行调优。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 配置模型model_name = 'cb_model'attn_model = 'dot'#attn_model = 'general'#attn_model = 'concat'hidden_size = 500encoder_n_layers = 2decoder_n_layers = 2dropout = 0.1batch_size = 64# 从哪个checkpoint恢复，如果是None，那么从头开始训练。loadFilename = Nonecheckpoint_iter = 4000 # 如果loadFilename不空，则从中加载模型 if loadFilename: # 如果训练和加载是一条机器，那么直接加载 checkpoint = torch.load(loadFilename) # 否则比如checkpoint是在GPU上得到的，但是我们现在又用CPU来训练或者测试，那么注释掉下面的代码 #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu')) encoder_sd = checkpoint['en'] decoder_sd = checkpoint['de'] encoder_optimizer_sd = checkpoint['en_opt'] decoder_optimizer_sd = checkpoint['de_opt'] embedding_sd = checkpoint['embedding'] voc.__dict__ = checkpoint['voc_dict']print('Building encoder and decoder ...')# 初始化word embeddingembedding = nn.Embedding(voc.num_words, hidden_size)if loadFilename: embedding.load_state_dict(embedding_sd)# 初始化encoder和decoder模型encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)if loadFilename: encoder.load_state_dict(encoder_sd) decoder.load_state_dict(decoder_sd)# 使用合适的设备encoder = encoder.to(device)decoder = decoder.to(device)print('Models built and ready to go!') 训练下面的代码进行训练，我们需要设置一些训练的超参数。初始化优化器，最后调用函数trainIters进行训练。 1234567891011121314151617181920212223242526# 配置训练的超参数和优化器 clip = 50.0teacher_forcing_ratio = 1.0learning_rate = 0.0001decoder_learning_ratio = 5.0n_iteration = 4000print_every = 1save_every = 500# 设置进入训练模式，从而开启dropout encoder.train()decoder.train()# 初始化优化器 print('Building optimizers ...')encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)if loadFilename: encoder_optimizer.load_state_dict(encoder_optimizer_sd) decoder_optimizer.load_state_dict(decoder_optimizer_sd)# 开始训练print(&quot;Starting Training!&quot;)trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename) 测试我们使用下面的代码进行测试。 123456789# 进入eval模式，从而去掉dropout。 encoder.eval()decoder.eval()# 构造searcher对象 searcher = GreedySearchDecoder(encoder, decoder)# 测试evaluateInput(encoder, decoder, searcher, voc) 下面是测试的一些例子： 123456789&gt; helloBot: hello .&gt; what's your name?Bot: jacob .&gt; I am sorry.Bot: you re not .&gt; where are you from?Bot: southern .&gt; q 结论上面介绍了怎么从零开始训练一个chatbot，读者可以用自己的数据训练一个chatbot试试，看看能不能用来解决一些实际业务问题。 NEXT POST → 显示DISQUS评论(需要科学上网) FEATURED TAGS人工智能 深度学习 chatbot PyTorch Java BERT git 编程 OCR 汪曾祺 语音识别 Kaldi Linux XLNet 情感分析 [sentiment analysis](https://fancyerii.github.io/tags/#sentiment analysis) 语法纠错 Transformer Tensorflow Huggingface Ubuntu TensorFlow 深度学习框架 Tensor2Tensor 机器翻译 微信 wechat automation selenium webdriver pywinauto CentOS GPU Appium t2t 代码阅读 中英翻译 公众号 爬虫 ocr tesseract pytesseract python 默认参数 位置参数 VPN JSON Jackson huggingface PagedAttention vLLM Pre-training LLM CPT qlora quantization transformers cmake pip pipenv conda padding vscode [source code](https://fancyerii.github.io/tags/#source code) build Speech ASR linux FRIENDS Li Li","link":"/2024/03/13/AI%20Chat%20Bot/"},{"title":"ChatBot Training Tour","text":"This article records my process of training an AI Chat Bot. weibo.pth: 4435959 (41X) Corpus.pth: 106478 (batch_size = 2048) Summary Version Epoch Loss Parameter V14-4 68 4.748 Same as before V14-3 32 4.753 Same as before V14-2 92 4.744 Same as before V14-1 13 4.796 Same as before V14 42 4.773 batch_size = 13750 V13-9 18 4.829 Same as before V13-8 21 4.823 Same as before V13-7 18 4.837 Same as before V13-6 17 4.860 Same as before V13-5 16 4.835 Same as before V13-4 20 4.866 Same as before V13-3 6 4.889 Same as before V13-2 16 4.881 Same as before V13-1 10 4.874 Same as before V13 20 4.876 batch_size = 11000 V12-2 21 4.938 Same as before V12-1 19 4.914 Same as before V12 17 4.970 batch_size = 10000learning_rate = 0.0001 V11 6 4.965 batch_size = 12000 V10 14 4.976 batch_size = 10000 V9 7 5.059 batch_size = 5000learning_rate = 0.001 V8 10 5.092 batch_size = 4096learning_rate = 0.001 V7 11 5.181 batch_size = 2048learning_rate = 0.0001 V6 7 5.198 batch_size = 2048learning_rate = 0.0005 V5 22 5.226 batch_size = 2048learning_rate = 0.001 V4 8 5.917 dropout = 0.2teacher_forcing_ratio = 0.85batch_size = 2048learning_rate = 0.0001 V3 14 5.966 dropout = 0.2clip = 60.0teacher_forcing_ratio = 0.9batch_size = 2048learning_rate = 0.0001 V2 44 5.055 Origin 2024.04.23 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: d89c96166e3ca903aed3bd960c7d8dab8fdaa5f0) 训练平台：百度飞浆 Epoch: 68 Average loss: 4.748495200444133 2024.04.23 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: d89c96166e3ca903aed3bd960c7d8dab8fdaa5f0) 训练平台：百度飞浆 Epoch: 32 Average loss: 4.753096600420531 2024.04.22 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: d89c96166e3ca903aed3bd960c7d8dab8fdaa5f0) 训练平台：百度飞浆 Epoch: 92 Average loss: 4.743738563202844 2024.04.21 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: d89c96166e3ca903aed3bd960c7d8dab8fdaa5f0) 训练平台：百度飞浆 Epoch: 13 Average loss: 4.796416420097512 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: d89c96166e3ca903aed3bd960c7d8dab8fdaa5f0) 训练平台：百度飞浆 Epoch: 42 Average loss: 4.772935219662937 2024.04.20 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 87160e2723ede287d60a68b82700e63bdc1a5c2b) 训练平台：百度飞浆 Epoch: 18 Average loss: 4.828708153485162 2024.04.19 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 87160e2723ede287d60a68b82700e63bdc1a5c2b) 训练平台：百度飞浆 Epoch: 21 Average loss: 4.822652317165321 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 87160e2723ede287d60a68b82700e63bdc1a5c2b) 训练平台：百度飞浆 Epoch: 18 Average loss: 4.837234509962312 2024.04.16 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 87160e2723ede287d60a68b82700e63bdc1a5c2b) 训练平台：百度飞浆 Epoch: 17 Average loss: 4.860292780773252 2024.04.15 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 87160e2723ede287d60a68b82700e63bdc1a5c2b) 训练平台：百度飞浆 Epoch: 16 Average loss: 4.83524939768576 2024.04.14 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 87160e2723ede287d60a68b82700e63bdc1a5c2b) 训练平台：百度飞浆 Epoch: 20 Average loss: 4.86629382181837 2024.04.07 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 87160e2723ede287d60a68b82700e63bdc1a5c2b) 训练平台：百度飞浆 Epoch: 6 Average loss: 4.889181085683352 2024.04.07 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 87160e2723ede287d60a68b82700e63bdc1a5c2b) 训练平台：百度飞浆 Epoch: 16 Average loss: 4.881189099620858 2024.04.07 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 87160e2723ede287d60a68b82700e63bdc1a5c2b) 训练平台：百度飞浆 Epoch: 10 Average loss: 4.8736686256889294 2024.04.04 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 87160e2723ede287d60a68b82700e63bdc1a5c2b) 训练平台：百度飞浆 Epoch: 20 Average loss: 4.876422007442719 2024.03.29 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 8e5ffe59fa69e32fb938f1883db340d87b261f7a) 训练平台：百度飞浆 Epoch: 21 Average loss: 4.9384859099953164 2024.03.28 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 8e5ffe59fa69e32fb938f1883db340d87b261f7a) 训练平台：百度飞浆 Epoch: 19 Average loss: 4.914344686694272 2024.03.27 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 8e5ffe59fa69e32fb938f1883db340d87b261f7a) 训练平台：百度飞浆 Epoch: 17 Average loss: 4.969914259826938 2024.03.26 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: cb49184c8e7092ca1edfee4e3e8275607b975b07) 训练平台：vc Epoch: 6 Average loss: 4.96543633802544 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: faa387b6875bbcdf8e3adc250684ae55a2de3812) 训练平台：百度飞浆 Epoch: 14 Average loss: 4.97629819865179 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: ee22690c8e35bce2f99fb3f4b190908187256fbe)训练平台：百度飞浆 Epoch: 7 Average loss: 5.059292034180287 2024.03.25 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 88a3fe81e66cba7f529182d17ef886b444720542)训练平台：百度飞浆 Epoch: 10 Average loss: 5.092367821139052 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 1ce617ee78368acfaa489b5dd1c8021d2f506e27)训练平台：百度飞浆 Epoch: 11 Average loss: 5.18057608768361 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 6bba047016a0f05a2fe1e3dda8ae4389f018c9ca)训练平台：百度飞浆 Epoch: 7 Average loss: 5.197861174456669 2024.03.24 Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 6435d7d7bb7bf52b4dc6e1fe46c76f55a63d8c98)训练平台：百度飞浆 Epoch: 22 Average loss: 5.2257363110720165 The loss curve you’ve provided suggests that the loss has decreased significantly and then plateaued, which is common in training deep learning models as they begin to converge. When the loss curve plateaus, it often means that the model has reached a point where it’s not learning much more from the data given the current settings. Here are some strategies you could consider to further decrease the loss: Learning Rate Adjustment: If you’re using a constant learning rate, you might want to switch to a learning rate schedule that decreases the learning rate as you continue to train. If you’re already using a learning rate scheduler, consider adjusting it to decrease the learning rate more slowly or to use a learning rate warm-up period. Early Stopping: If the loss isn’t improving for a significant number of epochs, it might be time to stop training to prevent overfitting. Regularization Techniques: Implement or increase regularization to prevent overfitting if you suspect that’s an issue. This can include techniques like dropout, L2 regularization, or using a more robust optimizer like AdamW which incorporates weight decay. Model Architecture: Consider whether your model might be too simple or too complex for the problem. Adding layers or units can increase model capacity, while reducing them can help the model generalize better if it’s overfitting. Data Augmentation or Cleaning: More or cleaner data can help improve model performance. If you’re already using all your data, consider data augmentation techniques. Sometimes, reviewing and cleaning the dataset can also yield improvements, especially if there are noisy labels or irrelevant features. Batch Size: Adjust the batch size if possible. A smaller batch size can provide a regularizing effect and larger batch sizes can provide more stable gradient estimates. Gradient Clipping: This prevents the exploding gradient problem by capping the gradients during backpropagation to avoid very large updates to the weights. Different Optimizer: If you’re not already, consider using an optimizer like Adam, which adapts the learning rate for each parameter and is generally good for large datasets and complex models. Loss Function: Consider if the loss function you’re using is appropriate for your data and problem. Sometimes, tweaking the loss function or using a custom one can yield better results. Hyperparameter Tuning: Use techniques like grid search, random search, or Bayesian optimization to systematically explore different hyperparameter settings. To apply these strategies, you would need to adjust the parameters in your training code (train_eval.py, config.py, etc.). Since I can’t inspect the code within those files directly, I would recommend looking for the following in your code: Where the learning rate is defined and how it’s updated during training. Where regularization settings are configured. Where the model’s architecture is defined. How data is preprocessed and augmented. The batch size settings. Config.py: https://github.com/OnlyourMiracle/Chinese-Chatbot-PyTorch-Implementation/blob/master/config.py(SHA: 23108d169e8bcb8e777fdc4a21e6165d82dbb0a8)训练平台：百度飞浆 Epoch: 306 Average loss: 1.412546370036195 使用chatbot_0324_1543模型训练得到的部分对话 Config.py: https://github.com/OnlyourMiracle/ChatBot/blob/main/ChatBotV4/config.py (SHA: 921086296f841570df5b210c57e2054232e0c874)训练平台：百度飞浆 Epoch: 8 Average loss: 5.917065527910856 2024.03.22 Config.py: https://github.com/OnlyourMiracle/ChatBot/blob/main/ChatBotV4/config.py (SHA: e747321aa47db35adfcccfcb2f5b24788c13b1ca) 训练平台：百度飞浆 Epoch: 14 Average loss : 5.96649232743882 2024.03.21 Github: https://github.com/OnlyourMiracle/ChatBot/tree/main/ChatBotV3 最新 ChatBot 训练代码（V3） 训练平台：百度飞浆 Epoch: 44 Average loss : 5.055076121505371 观察结果: 快速下降期: 损失值在最初迅速下降，这表明模型在最初的训练阶段学习到了数据集的显著特征。 稳定期: 随着迭代次数增加，损失值下降放缓并在5.1左右波动。这表明模型可能已经达到了当前参数和数据配置下的性能瓶颈。 改进建议: 调整学习率: 如果损失值在一个水平线上波动，可能意味着学习率设置得太高或太低。您可以尝试使用学习率衰减策略，或者尝试更多的学习率值。 引入或增加 Dropout: 您当前的模型没有使用dropout，dropout可以帮助防止过拟合并提高模型的泛化能力。考虑在RNN层和/或全连接层之间添加dropout。 梯度裁剪: 您设置的梯度裁剪阈值为50，这通常用于防止梯度爆炸。观察模型是否经历过任何梯度爆炸的迹象，如果没有，可以尝试放宽裁剪阈值。 调整优化器: 如果当前优化器（可能是Adam，因为这是学习率1e-3的常用选择）的性能已经达到瓶颈，可以尝试其他优化器，如SGD或Adagrad。 改变教师强制比例: 教师强制比例目前为1.0，意味着您总是使用真实的输出作为下一时刻的输入。随着训练的进行，逐渐降低这个比例可能会帮助模型学会更加自信地依靠自己的预测。 模型容量: 如果模型太简单，可能无法捕捉数据的全部复杂性。您可以尝试增加隐藏层的大小或添加更多的RNN层。 批次大小: 您的批次大小相对较大（2048）。虽然较大的批次可以提供更稳定的梯度估计，但它们也可能导致优化过程的探索性下降。尝试减少批次大小可能有助于模型找到更优的损失值。 早停 (Early Stopping): 如果验证集上的性能不再提升，可以停止训练以避免过拟合。 及时保存模型：考虑到目前在百度飞浆平台上训练的速度以及受每日GPU使用限额影响，应调整“save_every”参数大小，改为10较为合适。 在尝试这些改进策略时，重要的是一次只改变一个参数，并且监视其对训练动态的影响。另外，确保有一个稳定的验证集来监测这些改变如何影响模型在未见数据上的表现。 2024.03.20​ 前几天一直出现一个问题：当训练到 118 epoch 时 loss=nan，今天终于找到问题的答案了，原来是datapreprocess过程出错了，原来的代码： 12345678for line in combined_lines: sentences = [] for value in line: sentence = cop.sub(&quot;&quot;, value).split() #sentence = jieba.lcut(cop.sub(&quot;&quot;, value)) sentence = sentence[:max_sentence_length] + [eos] sentences.append(sentence) data.append(sentences) 今天找到问题所在后改为如下代码就成功地解决了问题。 12345678for line in combined_lines: sentences = [] for value in line: sentence = cop.sub(&quot;&quot;, value) sentence = jieba.lcut(sentence) sentence = sentence[:max_sentence_length] + [eos] sentences.append(sentence) data.append(sentences)","link":"/2024/03/20/ChatBot%20Training%20Tour/"},{"title":"Pytorch Learning Tour","text":"This article records my process of learning Pytorch. Chapter2神经网络神经网络的通用训练步骤 （1）定义一个包含可学习参数的神经网络。 （2）加载用于训练该网络的数据集。 （3）进行前向传播得到网络的输出结果，计算损失（网络输出结果与正确结果的差距）。 （4）进行反向传播，更新网络参数。 （5）保存网络模型。 定义网络在定义网络时，模型需要继承nn.Module，并实现它的forward方法。其中，网络里含有可学习参数的层应该放在构造函数__init__()中，如果某一层（如ReLU）不含有可学习参数，那么它既可以放在构造函数中，又可以放在forward方法中。这里将这些不含有可学习参数的层放在forward方法中，并使用nn.functional实现： 1234567891011121314151617181920212223242526272829303132import torch.nn as nnimport torch.nn.functional as Fclass Net(nn.Module): def __init__(self): # nn.Module子类的函数必须在构造函数中执行父类的构造函数 # 下式等价于nn.Module.__init__(self) super().__init__() # 卷积层，'1'表示输入图片为单通道, '6'表示输出通道数，'5'表示卷积核为5×5 self.conv1 = nn.Conv2d(1, 6, 5) # 卷积层，'6'表示输入图片为单通道, '16'表示输出通道数，'5'表示卷积核为5×5 self.conv2 = nn.Conv2d(6, 16, 5) # 仿射层/全连接层，y = Wx + b self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # 卷积 -&gt; 激活 -&gt; 池化 x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) x = F.max_pool2d(F.relu(self.conv2(x)), 2) # 改变Tensor的形状，-1表示自适应 x = x.view(x.size()[0], -1) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return xnet = Net()print(net) 1234567Out:Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True) ) 用户只需要在nn.Module的子类中定义了forward函数，backward函数就会自动实现（利用autograd）。在forward函数中不仅可以使用Tensor支持的任何函数，还可以使用if、for、print、log等Python语法，写法和标准的Python写法一致。 使用net.parameters()可以得到网络的可学习参数，使用net.named_parameters()可以同时得到网络的可学习参数及其名称。 注意：torch.nn只支持输入mini-batch，不支持一次只输入一个样本。如果只输入一个样本，那么需要使用 input.unsqueeze(0)将batch_size设为1。例如， nn.Conv2d的输入必须是4维，形如$\\text{nSamples} \\times \\text{nChannels} \\times \\text{Height} \\times \\text{Width}$。如果一次输入只有一个样本，那么可以将$\\text{nSample}$设置为1，即$1 \\times \\text{nChannels} \\times \\text{Height} \\times \\text{Width}$。 损失函数torch.nn实现了神经网络中大多数的损失函数，例如nn.MSELoss用来计算均方误差，nn.CrossEntropyLoss用来计算交叉熵损失等。 1234567input = t.randn(1, 1, 32, 32)output = net(input)target = t.arange(0, 10).view(1, 10).float() criterion = nn.MSELoss()loss = criterion(output, target)print(loss)# Out:tensor(28.1249, grad_fn=&lt;MseLossBackward&gt;) 当调用loss.backward()时，计算图会动态生成并自动微分，自动计算图中参数（parameters）的导数，示例如下： 123456789101112In: # 运行.backward，观察调用之前和调用之后的grad net.zero_grad() # 把net中所有可学习参数的梯度清零 print('反向传播之前 conv1.bias的梯度') print(net.conv1.bias.grad) loss.backward() print('反向传播之后 conv1.bias的梯度') print(net.conv1.bias.grad) '''Out:反向传播之前 conv1.bias的梯度 tensor([0., 0., 0., 0., 0., 0.]) 反向传播之后 conv1.bias的梯度 tensor([ 0.0020, -0.0619, 0.1077, 0.0197, 0.1027, -0.0060])''' 优化器在完成反向传播中所有参数的梯度计算后，需要使用优化方法来更新网络的权重和参数。常用的随机梯度下降法（SGD）的更新策略如下： 1234# weight = weight - learning_rate * gradientlearning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) # inplace减法 torch.optim中实现了深度学习中大多数优化方法，例如RMSProp、Adam、SGD等， 随机梯度下降（SGD） 优点 简单易实现。 在大规模数据集和高维空间中依然有效，因为它每次更新只考虑一个样本，计算效率较高。 缺点 收敛速度可能比较慢，尤其是在梯度较小的平坦区域。 可能会陷入局部最优解。 动量（Momentum）SGD 优点 通过累积过去梯度的信息来加速SGD，在相关方向上加快学习速度，减缓在非相关方向上的学习速度，从而加快收敛。 缺点 需要选择额外的动量参数，增加了调参的复杂度。 RMSProp 优点 通过调整学习率来加快训练速度，适合处理非平稳目标——对于RNN的效果很好。 能够在很多非凸优化问题中快速收敛。 缺点 和Momentum一样，需要设置更多的超参数（如学习率和衰减系数）。 Adam（Adaptive Moment Estimation） 优点 结合了Momentum和RMSProp的优点，对学习率进行自适应调整。 通常在很多不同的深度学习模型中表现良好，被广泛使用。 缺点 相对于简单的SGD，计算资源消耗更大。 在某些情况下，Adam的自适应学习率可能导致收敛到次优解。 因此，通常情况下用户不需要手动实现上述代码。下面举例说明如何使用torch.optim进行网络的参数更新： 1234567891011121314151617In: import torch.optim as optim #新建一个优化器，指定要调整的参数和学习率 optimizer = optim.SGD(net.parameters(), lr = 0.01) # 在训练过程中 # 先梯度清零(与net.zero_grad()效果一样) optimizer.zero_grad() # 计算损失 output = net(input) loss = criterion(output, target) #反向传播 loss.backward() #更新参数 optimizer.step() 数据加载与预处理在深度学习中，数据加载及预处理是非常繁琐的过程。幸运的是，PyTorch提供了一些可以极大简化和加快数据处理流程的工具：Dataset与DataLoader。同时，对于常用的数据集，PyTorch提供了封装好的接口供用户快速调用，这些数据集主要保存在torchvision中。torchvision是一个视觉工具包，它提供了许多视觉图像处理的工具，主要包含以下三部分。 datasets：提供了常用的数据集，如MNIST、CIFAR-10、ImageNet等。 models：提供了深度学习中经典的网络结构与预训练模型，如ResNet、MobileNet等。 transforms：提供了常用的数据预处理操作，主要包括对Tensor、PIL Image等的操作。 Chapter3","link":"/2024/03/24/Pytorch%20Learning%20Tour/"},{"title":"AI Project","text":"This article records some of AI project that I have learned. 使用ERNIE Bot SDK调用模型 2024.04.08 modul URL: https://aistudio.baidu.com/modelsdetail/754 GPT-SoVITS 2024.03.29 This is a project of voice clone. Github: https://github.com/RVC-Boss/GPT-SoVITS Official Image: https://www.codewithgpu.com/i/RVC-Boss/GPT-SoVITS/GPT-SoVITS-Official ​ - This Image can be used by Autodl. Reference: ​ - AutoDL教程: https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/tkemqe8vzhadfpeu?singleDoc#rOMZt ​ - 整合包教程: https://www.yuque.com/baicaigongchang1145haoyuangong/ib3g1e/xyyqrfwiu3e2bgyk?singleDoc#eNDB5","link":"/2024/03/29/AI%20Project/"}],"tags":[{"name":"Computer science","slug":"Computer-science","link":"/tags/Computer-science/"},{"name":"Reading","slug":"Reading","link":"/tags/Reading/"},{"name":"Life experience","slug":"Life-experience","link":"/tags/Life-experience/"},{"name":"Reading notes","slug":"Reading-notes","link":"/tags/Reading-notes/"},{"name":"Knowledge","slug":"Knowledge","link":"/tags/Knowledge/"},{"name":"Kaoyan","slug":"Kaoyan","link":"/tags/Kaoyan/"},{"name":"AI","slug":"AI","link":"/tags/AI/"}],"categories":[{"name":"Computer science","slug":"Computer-science","link":"/categories/Computer-science/"},{"name":"Reading","slug":"Reading","link":"/categories/Reading/"},{"name":"Life experience","slug":"Life-experience","link":"/categories/Life-experience/"},{"name":"Reading notes","slug":"Reading-notes","link":"/categories/Reading-notes/"},{"name":"Knowledge","slug":"Knowledge","link":"/categories/Knowledge/"},{"name":"Diary","slug":"Diary","link":"/categories/Diary/"},{"name":"AI","slug":"AI","link":"/categories/AI/"}]}